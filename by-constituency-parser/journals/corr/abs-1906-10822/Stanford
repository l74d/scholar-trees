(S (NP (NP (JJ Large) (HYPH -) (NN batch)) (NP (JJ stochastic) (NN gradient) (NN descent)) (PRN (-LRB- -LRB-) (NP (NNP SGD)) (-RRB- -RRB-))) (VP (VBZ is) (ADVP (RB widely)) (VP (VBN used) (PP (IN for) (NP (NP (NN training)) (PP (IN in) (NP (VBN distributed) (JJ deep) (NN learning))))) (SBAR (IN because) (S (PP (IN of) (NP (NP (PRP$ its) (NML (NN training) (HYPH -) (NN time)) (NN efficiency)) (, ,) (ADVP (RB however)))) (, ,) (NP (NML (RB extremely) (JJ large) (HYPH -) (NN batch)) (NNP SGD)) (VP (VP (VBZ leads) (PP (IN to) (NP (JJ poor) (NN generalization)))) (CC and) (ADVP (RB easily)) (VP (VBZ converges) (PP (IN to) (NP (NP (JJ sharp) (NN minima)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ prevents) (NP (JJ naive) (NML (JJ large) (HYPH -) (NN scale)) (ADJP (NN data) (HYPH -) (JJ parallel)) (NML (NNP SGD) (PRN (-LRB- -LRB-) (NP (NNP DP) (HYPH -) (NNP SGD)) (-RRB- -RRB-)))) (PP (IN from) (S (VP (VBG converging) (PP (IN to) (NP (JJ good) (NN minima))))))))))))))))) (. .))
(S (S (VP (TO To) (VP (VB overcome) (NP (DT this) (NN difficulty))))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (NN gradient) (NN noise) (NN convolution) (PRN (-LRB- -LRB-) (NP (NN GNC)) (-RRB- -RRB-))) (, ,) (SBAR (WHNP (WDT which)) (S (ADVP (RB effectively)) (VP (VBZ smooths) (NP (NP (JJR sharper) (NN minima)) (PP (IN of) (NP (DT the) (NN loss) (NN function))))))))) (. .))
(S (PP (IN For) (NP (NNP DP) (HYPH -) (NNP SGD))) (, ,) (NP (NNP GNC)) (VP (VBZ utilizes) (NP (NP (ADJP (RB so) (HYPH -) (VBN called)) (NN gradient) (NN noise)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ is) (VP (VP (VBN induced) (PP (IN by) (NP (JJ stochastic) (NN gradient) (NN variation)))) (CC and) (VP (VBN convolved) (PP (IN to) (NP (DT the) (NN loss) (NN function))) (PP (IN as) (NP (DT a) (VBG smoothing) (NN effect)))))))))) (. .))
(S (NP (NNP GNC) (NN computation)) (VP (VP (MD can) (VP (VB be) (VP (VBN performed) (PP (IN by) (S (ADVP (RB simply)) (VP (VP (VBG computing) (NP (DT the) (JJ stochastic) (NN gradient)) (PP (IN on) (NP (DT each) (JJ parallel) (NN worker)))) (CC and) (VP (VBG merging) (NP (PRP them))))))))) (, ,) (CC and) (VP (VBZ is) (ADVP (RB therefore)) (ADJP (RB extremely) (JJ easy) (S (VP (TO to) (VP (VB implement))))))) (. .))
(S (PP (IN Due) (PP (IN to) (S (VP (VBG convolving) (PP (IN with) (NP (NP (DT the) (NN gradient) (NN noise)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ tends) (S (VP (TO to) (VP (VB spread) (PP (IN along) (NP (NP (DT a) (JJR sharper) (NN direction)) (PP (IN of) (NP (DT the) (NN loss) (NN function))))))))))))))))) (, ,) (NP (NNP GNC)) (VP (MD can) (ADVP (RB effectively)) (VP (VP (VB smooth) (NP (JJ sharp) (NN minima))) (CC and) (VP (VB achieve) (NP (JJR better) (NN generalization)) (, ,) (SBAR (IN whereas) (S (NP (JJ isotropic) (JJ random) (NN noise)) (VP (MD can) (RB not))))))) (. .))
(S (NP (PRP We)) (ADVP (RB empirically)) (VP (VP (VBP show) (NP (DT this) (NN effect)) (PP (IN by) (S (VP (VBG comparing) (NP (NNP GNC)) (PP (IN with) (NP (JJ isotropic) (JJ random) (NN noise))))))) (, ,) (CC and) (VP (VBP show) (SBAR (IN that) (S (NP (PRP it)) (VP (VBZ achieves) (NP (NP (NML (NML (NN state)) (HYPH -) (PP (IN of) (HYPH -) (NP (DT the) (HYPH -) (NN art)))) (NN generalization) (NN performance)) (PP (IN for) (NP (NML (JJ large) (HYPH -) (NN scale)) (NML (JJ deep) (JJ neural) (NN network)) (NN optimization))))))))) (. .))
