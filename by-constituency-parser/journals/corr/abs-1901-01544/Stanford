(S (NP (PRP It)) (VP (VBZ is) (ADJP (JJ inevitable) (S (VP (TO to) (VP (VB train) (NP (JJ large) (NML (JJ deep) (NN learning)) (NNS models)) (PP (IN on) (NP (NP (DT a) (NML (JJ large) (HYPH -) (NN scale)) (NN cluster)) (VP (VBN equipped) (PP (IN with) (NP (NNS accelerators) (NN system))))))))))) (. .))
(UCP (S (NP (JJ Deep) (NN gradient) (NN compression)) (VP (MD would) (ADVP (RB highly)) (VP (VP (VB increase) (NP (DT the) (NN bandwidth) (NN utilization))) (CC and) (VP (VB speed) (PRT (RP up)) (NP (DT the) (NN training) (NN process)))))) (CC but) (FRAG (ADJP (JJ hard) (S (VP (TO to) (VP (VB implement) (PP (IN on) (NP (NN ring) (NN structure)))))))) (. .))
(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (NP (PRP we)) (VP (VBP find) (SBAR (IN that) (S (NP (JJ redundant) (NML (NN gradient) (CC and) (NN gradient)) (NN staleness)) (VP (VBZ has) (NP (NP (JJ negative) (NN effect)) (PP (IN on) (NP (NN training)))))))) (. .))
(S (NP (PRP We)) (VP (VBP have) (VP (VBN observed) (SBAR (IN that) (S (PP (IN in) (NP (JJ different) (ADJP (NN epoch) (CC and) (JJ different)) (NNS steps))) (, ,) (NP (DT the) (JJ neural) (NNS networks)) (VP (VBP focus) (PP (IN on) (S (VP (VBG updating) (NP (NP (JJ different) (NNS layers)) (CC and) (NP (JJ different) (NNS parameters))))))))))) (. .))
(S (PP (IN In) (NP (NN order) (S (VP (TO to) (VP (VP (VB save) (NP (JJR more) (NN communication) (NN bandwidth))) (CC and) (VP (VB preserve) (NP (DT the) (NN accuracy)) (PP (IN on) (NP (NP (NN ring) (NN structure)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBP break) (NP (DT the) (NML (S (VP (VB restrict) (PP (IN as) (NP (DT the) (NN node)))))) (NN increase))))))))))))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (DT a) (JJ new) (NN algorithm)) (S (VP (TO to) (VP (VB measure) (NP (NP (DT the) (NN importance)) (PP (IN of) (NP (NP (NNS gradients)) (PP (IN on) (NP (NML (JJ large) (HYPH -) (NN scale)) (NN cluster)))))) (S (VP (VBG implementing) (NP (NN ring)) (VP (RB all) (HYPH -) (VB reduce) (PP (VBN based) (PP (IN on) (NP (NP (DT the) (NN size)) (PP (IN of) (NP (NP (DT the) (NN ratio)) (PP (IN of) (NP (NP (NN parameter) (NN calculation) (NN gradient)) (PP (IN to) (NP (NN parameter) (NN value))))))))))))))))) (. .))
(S (NP (NP (PRP$ Our) (NN importance)) (NP (JJ weighted) (NN pruning) (NN approach))) (VP (VBD achieved) (NP (NP (NN 64X)) (CC and) (NP (NP (CD 58.8) (NN X)) (PP (IN of) (NP (NML (NN gradient) (NN compression)) (NN ratio))))) (PP (IN on) (NP (NNP AlexNet) (CC and) (NNP ResNet50))) (PP (IN on) (NP (NNP ImageNet)))) (. .))
(S (ADVP (RB Meanwhile)) (, ,) (PP (IN in) (NP (NN order) (S (VP (TO to) (VP (VB maintain) (NP (NP (DT the) (NN sparseness)) (PP (IN of) (NP (DT the) (NN gradient) (NN propagation))))))))) (, ,) (NP (PRP we)) (ADVP (RB randomly)) (VP (VBD broadcast) (NP (NP (DT the) (NN index)) (PP (IN of) (NP (JJ important) (NNS gradients)))) (PP (IN on) (NP (DT each) (NN node)))) (. .))
(S (SBAR (IN While) (S (NP (DT the) (VBG remaining) (NNS nodes)) (VP (VP (VBP are) (ADJP (JJ ready) (PP (IN for) (NP (DT the) (NN index) (NN gradient))))) (CC and) (VP (VB perform))))) (VP (RB all) (HYPH -) (VB reduce) (NP (NN update))) (. .))
(S (NP (DT This)) (VP (MD would) (VP (VP (VB speed) (PRT (RP up)) (NP (NP (DT the) (NN convergence)) (PP (IN of) (NP (DT the) (NN model))))) (CC and) (VP (VB preserve) (NP (DT the) (NN training) (NN accuracy))))) (. .))
