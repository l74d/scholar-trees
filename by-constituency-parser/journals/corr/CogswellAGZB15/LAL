(S (NP (NP (CD One) (JJ major) (NN challenge)) (PP (IN in) (S (VP (VBG training) (NP (JJ Deep) (NNP Neural) (NNP Networks)))))) (VP (VBZ is) (VP (VBG preventing) (NP (VBG overfitting)))) (. .))
(S (NP (NP (JJ Many) (NNS techniques)) (PP (JJ such) (IN as) (NP (NP (NNS data) (NN augmentation)) (CC and) (NP (NP (JJ novel) (NNS regularizers)) (PP (JJ such) (IN as) (NP (NNP Dropout))))))) (VP (VBP have) (VP (VBN been) (VP (VBN proposed) (S (VP (TO to) (VP (VB prevent) (NP (VBG overfitting)) (PP (IN without) (S (VP (VBG requiring) (NP (NP (DT a) (JJ massive) (NN amount)) (PP (IN of) (NP (NN training) (NNS data))))))))))))) (. .))
(S (PP (IN In) (NP (DT this) (NN work))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (DT a) (JJ new) (NN regularizer)) (VP (VBN called) (S (NP (NNP DeCov)))) (SBAR (WHNP (WDT which)) (S (VP (VBZ leads) (PP (TO to) (NP (NP (NP (ADJP (RB significantly) (VBN reduced)) (NN overfitting)) (PRN (-LRB- -LRB-) (SBAR (IN as) (S (VP (VBN indicated) (PP (IN by) (NP (NP (DT the) (NN difference)) (PP (IN between) (NP (NN train) (CC and) (JJ val) (NN performance)))))))) (-RRB- -RRB-))) (, ,) (CC and) (NP (JJR better) (NN generalization))))))))) (. .))
(S (NP (PRP$ Our) (NN regularizer)) (VP (VBZ encourages) (NP (NP (ADJP (JJ diverse) (CC or) (JJ non-redundant)) (NNS representations)) (PP (IN in) (NP (NNP Deep) (NNP Neural) (NNP Networks)))) (PP (IN by) (S (VP (VBG minimizing) (NP (NP (DT the) (NN cross-covariance)) (PP (IN of) (NP (JJ hidden) (NNS activations)))))))) (. .))
(S (NP (DT This) (JJ simple) (NN intuition)) (VP (VP (VBZ has) (VP (VBN been) (VP (VBN explored) (PP (IN in) (NP (NP (DT a) (NN number)) (PP (IN of) (NP (JJ past) (NNS works)))))))) (CC but) (VP (ADVP (RB surprisingly)) (VBZ has) (ADVP (RB never)) (VP (VBN been) (VP (VBN applied) (PP (IN as) (NP (DT a) (NN regularizer))) (PP (IN in) (NP (JJ supervised) (NN learning))))))) (. .))
(S (NP (NP (NNS Experiments)) (PP (IN across) (NP (NP (DT a) (NN range)) (PP (IN of) (NP (NP (NNS datasets)) (CC and) (NP (NN network) (NNS architectures))))))) (VP (VBP show) (SBAR (IN that) (S (NP (DT this) (NN loss)) (VP (ADVP (RB always)) (VBZ reduces) (NP (VBG overfitting)) (SBAR (IN while) (S (VP (VP (ADVP (RB almost) (RB always)) (VBG maintaining) (CC or) (VBG increasing) (NP (NN generalization) (NN performance))) (CC and) (VP (ADVP (RB often)) (JJ improving) (NP (NP (NN performance)) (PP (IN over) (NP (NNP Dropout)))))))))))) (. .))
