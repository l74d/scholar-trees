(S (S (NP (NN Dropout)) (VP (VBZ is) (NP (NP (DT a) (ADJP (JJ simple) (CC but) (JJ efficient)) (NN regularization) (NN technique)) (PP (IN for) (S (VP (VBG achieving) (NP (NP (JJR better) (NN generalization)) (PP (IN of) (NP (NP (JJ deep) (JJ neural) (NNS networks)) (PRN (-LRB- -LRB-) (NP (NNP DNNs)) (-RRB- -RRB-))))))))))) (: ;) (S (ADVP (NN hence)) (NP (PRP it)) (VP (VBZ is) (VP (ADVP (RB widely)) (VBN used) (PP (IN in) (NP (NP (NNS tasks)) (VP (VBN based) (PP (IN on) (NP (NNP DNNs))))))))) (. .))
(S (PP (IN During) (NP (NN training))) (, ,) (NP (NN dropout)) (VP (ADVP (RB randomly)) (VBZ discards) (NP (NP (DT a) (NN portion)) (PP (IN of) (NP (DT the) (NNS neurons)))) (S (VP (TO to) (VP (VB avoid) (NP (NN overfitting)))))) (. .))
(S (NP (DT This) (NN paper)) (VP (VBZ presents) (NP (NP (DT an) (JJ enhanced) (NN dropout) (NN technique)) (, ,) (SBAR (WHNP (WDT which)) (S (NP (PRP we)) (VP (VBP call) (S (NP (JJ multi-sample) (NN dropout)))))) (, ,) (PP (IN for) (S (VP (DT both) (VP (VBG accelerating) (NP (NN training))) (CC and) (VP (VBG improving) (NP (NP (NN generalization)) (PP (IN over) (NP (DT the) (JJ original) (NN dropout)))))))))) (. .))
(S (NP (DT The) (JJ original) (NN dropout)) (VP (VBZ creates) (NP (NP (DT a) (ADJP (RB randomly) (VBN selected)) (NN subset)) (PRN (-LRB- -LRB-) (VP (VBN called) (S (NP (DT a) (NN dropout) (NN sample)))) (-RRB- -RRB-))) (PP (IN from) (NP (DT the) (NN input))) (PP (IN in) (NP (DT each) (NN training) (NN iteration))) (SBAR (IN while) (S (NP (DT the) (JJ multi-sample) (NN dropout)) (VP (VBZ creates) (NP (JJ multiple) (NN dropout) (NNS samples)))))) (. .))
(S (S (NP (DT The) (NN loss)) (VP (VBZ is) (VP (VBN calculated) (PP (IN for) (NP (DT each) (NN sample)))))) (, ,) (CC and) (ADVP (RB then)) (S (NP (DT the) (NN sample) (NNS losses)) (VP (VBP are) (VP (VBN averaged) (S (VP (TO to) (VP (VB obtain) (NP (DT the) (JJ final) (NN loss)))))))) (. .))
(S (NP (DT This) (NN technique)) (VP (MD can) (VP (VB be) (VP (ADVP (RB easily)) (VBN implemented) (PP (IN without) (S (VP (VBG implementing) (NP (DT a) (JJ new) (NN operator))))) (PP (IN by) (S (VP (VBG duplicating) (NP (NP (DT a) (NN part)) (PP (IN of) (NP (DT the) (NN network))) (PP (IN after) (NP (DT the) (NN dropout) (NN layer)))) (SBAR (IN while) (S (VP (VBG sharing) (NP (DT the) (NNS weights)) (PP (IN among) (NP (DT the) (VBN duplicated) (ADJP (RB fully) (VBN connected)) (NNS layers)))))))))))) (. .))
(S (NP (JJ Experimental) (NNS results)) (VP (VBD showed) (SBAR (IN that) (S (NP (JJ multi-sample) (NN dropout)) (VP (ADVP (RB significantly)) (VBZ accelerates) (NP (NN training)) (PP (IN by) (S (VP (VBG reducing) (NP (NP (DT the) (NN number)) (PP (IN of) (NP (NNS iterations))) (PP (IN until) (NP (NN convergence))) (PP (IN for) (NP (NP (NN image) (NN classification) (NNS tasks)) (VP (VBG using) (NP (DT the) (NNP ImageNet) (, ,) (NNP CIFAR-10) (, ,) (NNP CIFAR-100) (, ,) (CC and) (NNP SVHN) (NNS datasets))))))))))))) (. .))
(S (NP (JJ Multi-sample) (NN dropout)) (VP (VBZ does) (RB not) (VP (ADVP (RB significantly)) (VB increase) (NP (NP (NN computation) (NN cost)) (PP (IN per) (NP (NN iteration)))) (SBAR (IN because) (S (NP (NP (JJS most)) (PP (IN of) (NP (DT the) (NN computation) (NN time)))) (VP (VBZ is) (VP (VBN consumed) (PP (IN in) (NP (NP (DT the) (NN convolution) (NNS layers)) (PP (IN before) (NP (DT the) (NN dropout) (NN layer))) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBP are) (RB not) (VP (VBN duplicated))))))))))))) (. .))
(S (NP (NNS Experiments)) (ADVP (RB also)) (VP (VBD showed) (SBAR (IN that) (S (NP (NP (NNS networks)) (VP (VBD trained) (S (VP (VBG using) (NP (JJ multi-sample) (NN dropout)))))) (VP (VBD achieved) (NP (JJR lower) (NN error) (NNS rates) (CC and) (NNS losses)) (PP (IN for) (NP (DT both) (DT the) (NX (NN training) (NN set)) (CC and) (NX (NN validation) (NN set)))))))) (. .))
