(S (NP (NN Today)) (NP (NP (DT a) (JJ canonical) (NN approach)) (SBAR (S (VP (TO to) (VP (VB reduce) (NP (NP (DT the) (NN computation) (NN cost)) (PP (IN of) (NP (NP (NNP Deep) (NNP Neural) (NNP Networks)) (PRN (-LRB- -LRB-) (NP (NNP DNNs)) (-RRB- -RRB-)))))))))) (VP (VBZ is) (S (VP (TO to) (VP (VP (VB pre-define) (NP (DT an) (JJ over-parameterized) (NN model)) (PP (IN before) (NP (VBG training))) (S (VP (TO to) (VP (VB guarantee) (NP (DT the) (NN learning) (NN capacity)))))) (, ,) (CC and) (ADVP (RB then)) (VP (VB prune) (NP (NP (JJ unimportant) (VBG learning) (NNS units)) (PRN (-LRB- -LRB-) (NP (NNS filters) (CC and) (NNS neurons)) (-RRB- -RRB-))) (PP (IN during) (NP (NN training))) (S (VP (TO to) (VP (VB improve) (NP (NN model) (NN compactness)))))))))) (. .))
(S (NP (PRP We)) (VP (VBP argue) (SBAR (S (NP (NP (PRP it))) (VP (VBZ is) (ADJP (JJ unnecessary)) (S (VP (TO to) (VP (VP (VB introduce) (NP (NN redundancy)) (PP (IN at) (NP (NP (DT the) (NN beginning)) (PP (IN of) (NP (DT the) (NN training)))))) (CC but) (VP (ADVP (RB then)) (VB reduce) (NP (NN redundancy)) (PP (IN for) (NP (DT the) (JJ ultimate) (NN inference) (NN model))))))))))) (. .))
(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (DT a) (NX (JJ Continuous) (NNP Growth) (CC and) (NNP Pruning)) (PRN (-LRB- -LRB-) (NP (NNP CGaP)) (-RRB- -RRB-)) (NN scheme)) (SBAR (S (VP (TO to) (VP (VB minimize) (NP (DT the) (NN redundancy)) (PP (IN from) (NP (DT the) (NN beginning))))))))) (. .))
(S (NP (NNP CGaP)) (VP (VP (VBZ starts) (NP (DT the) (NN training)) (PP (IN from) (NP (DT a) (JJ small) (NN network) (NN seed)))) (, ,) (ADVP (RB then)) (VP (VBZ expands) (NP (DT the) (NN model)) (ADVP (RB continuously)) (PP (IN by) (S (VP (VBG reinforcing) (NP (JJ important) (VBG learning) (NNS units)))))) (, ,) (CC and) (VP (ADVP (RB finally)) (VBZ prunes) (NP (DT the) (NN network)) (S (VP (TO to) (VP (VB obtain) (NP (DT a) (ADJP (NN compact) (CC and) (JJ accurate)) (NN model))))))) (. .))
(S (SBAR (IN As) (S (NP (DT the) (NN growth) (NN phase)) (VP (NNS favors) (NP (JJ important) (VBG learning) (NNS units))))) (, ,) (NP (NNP CGaP)) (VP (VBZ provides) (NP (DT a) (JJ clear) (VBG learning) (NN purpose)) (PP (TO to) (NP (DT the) (NN pruning) (NN phase)))) (. .))
(S (NP (NP (JJ Experimental) (NNS results)) (PP (IN on) (NP (NP (JJ representative) (NNS datasets)) (CC and) (NP (NNP DNN) (NNS architectures))))) (VP (VBP demonstrate) (SBAR (IN that) (S (NP (NNP CGaP)) (VP (VBZ outperforms) (NP (NP (JJ previous) (JJ pruning-only) (NNS approaches)) (SBAR (WHNP (WDT that)) (S (VP (VBP deal) (PP (IN with) (NP (JJ pre-defined) (NNS structures))))))))))) (. .))
(S (S (PP (IN For) (NP (NP (NP (NNP VGG-19)) (PP (IN on) (NP (NNP CIFAR-100)))) (CC and) (NP (NNP SVHN) (NNS datasets)))) (, ,) (NP (NNP CGaP)) (VP (VP (VBZ reduces) (NP (NP (DT the) (NN number)) (PP (IN of) (NP (NNS parameters)))) (PP (IN by) (NP (NP (CD 78.9) (NN %)) (CC and) (NP (CD 85.8) (NN %))))) (, ,) (S (NP (NNP FLOPs)) (PP (IN by) (NP (NP (CD 53.2) (NN %)) (CC and) (NP (CD 74.2) (NN %)) (, ,) (ADVP (RB respectively))))))) (: ;) (S (PP (IN For) (NP (NP (NNP ResNet-110)) (PP (IN On) (NP (NNP CIFAR-10))))) (, ,) (NP (NNP CGaP)) (VP (VBZ reduces) (NP (NP (NP (ADJP (CD 64.0) (NN %)) (NN number)) (PP (IN of) (NP (NNS parameters)))) (CC and) (NP (ADJP (CD 63.3) (NN %)) (NNP FLOPs))))) (. .))
