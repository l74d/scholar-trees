(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (NP (PRP we)) (VP (VBP present) (NP (DT an) (JJ online) (NN reinforcement)) (S (VP (VBG learning) (NP (NP (NN algorithm)) (, ,) (VP (VBN called) (NP (NP (NNP Renewal) (NNP Monte) (NNP Carlo)) (-LRB- -LRB-) (NP (NNP RMC)) (-RRB- -RRB-))) (, ,) (PP (IN for) (NP (JJ infinite) (NN horizon))) (NP (NP (NNP Markov) (NN decision) (NNS processes)) (PP (IN with) (NP (DT a) (VBN designated) (NN start) (NN state)))))))) (. .))
(S (NP (NNP RMC)) (VP (VP (VBZ is) (NP (DT a) (NML (NNP Monte) (NNP Carlo)) (NN algorithm))) (CC and) (VP (VBZ retains) (NP (NP (DT the) (NNS advantages)) (PP (IN of) (NP (NML (NNP Monte) (NNP Carlo)) (NNS methods))) (PP (VBG including) (NP (NP (ADVP (JJ low)) (NP (NP (NN bias) (, ,) (NN simplicity) (, ,) (CC and) (NN ease)) (PP (IN of) (NP (NN implementation)))) (SBAR (IN while) (S (, ,) (PP (IN at) (NP (DT the) (JJ same) (NN time))) (, ,) (VP (VBZ circumvents) (NP (NP (PRP$ their) (JJ key) (NNS drawbacks)) (PP (IN of) (NP (JJ high) (NN variance)))))))) (CC and) (NP (ADJP (VBN delayed) (PRN (-LRB- -LRB-) (NP (NP (NN end)) (PP (IN of) (NP (NN episode)))) (-RRB- -RRB-))) (NNS updates))))))) (. .))
(S (NP (NP (DT The) (JJ key) (NNS ideas)) (PP (IN behind) (NP (NNP RMC)))) (VP (VBP are) (SBAR (IN as) (S (VP (VBZ follows))))) (. .))
(S (ADVP (RB First)) (, ,) (PP (IN under) (NP (DT any) (JJ reasonable) (NN policy))) (, ,) (NP (DT the) (NN reward) (NN process)) (VP (VBZ is) (ADJP (JJ ergodic))) (. .))
(S (ADVP (RB So)) (, ,) (PP (IN by) (NP (NN renewal) (NN theory))) (, ,) (NP (NP (DT the) (NN performance)) (PP (IN of) (NP (DT a) (NN policy)))) (VP (VBZ is) (ADJP (JJ equal) (PP (IN to) (NP (NP (DT the) (NN ratio)) (PP (IN of) (NP (NP (VBN expected)) (VP (VBN discounted) (NP (NN reward)) (PP (IN to) (NP (NP (DT the) (VBN expected) (VBN discounted) (NN time)) (PP (IN over) (NP (DT a) (JJ regenerative) (NN cycle)))))))))))) (. .))
(S (ADVP (RB Second)) (, ,) (PP (IN by) (S (ADVP (RB carefully)) (VP (VBG examining) (NP (NP (DT the) (NN expression)) (PP (IN for) (NP (NN performance) (NN gradient))))))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (DT a) (JJ stochastic) (NN approximation) (NN algorithm)) (SBAR (WHNP (WDT that)) (S (ADVP (RB only)) (VP (VBZ requires) (NP (NP (NNS estimates)) (PP (IN of) (NP (NP (DT the) (VBN expected) (VBN discounted) (NN reward)) (CC and) (NP (NP (VBN discounted) (NN time)) (PP (IN over) (NP (NP (DT a) (JJ regenerative) (NN cycle)) (CC and) (NP (PRP$ their) (NNS gradients))))))))))))) (. .))
(S (NP (PRP We)) (VP (VP (VBP propose) (NP (CD two) (JJ unbiased) (NNS estimators)) (PP (IN for) (S (VP (VBG evaluating) (NP (NP (NN performance) (NNS gradients)) (, ---) (NP (NP (DT a) (ADJP (NP (NN likelihood) (NN ratio)) (VBN based)) (NN estimator)) (CC and) (NP (DT a) (ADJP (NP (JJ simultaneous) (NN perturbation)) (VBN based)) (NN estimator))) (, ---)))))) (CC and) (VP (VBP show) (SBAR (IN that) (S (PP (IN for) (NP (DT both) (NNS estimators))) (, ,) (NP (NNP RMC)) (VP (VBZ converges) (PP (IN to) (NP (DT a) (ADJP (RB locally) (JJ optimal)) (NN policy)))))))) (. .))
(S (NP (PRP We)) (VP (VP (VBP generalize) (NP (DT the) (NNP RMC) (NN algorithm)) (PP (IN to) (NP (JJ post-decision) (NN state) (NNS models)))) (CC and) (ADVP (RB also)) (VP (VB present) (NP (NP (DT a) (NN variant)) (SBAR (WHNP (WDT that)) (S (VP (VBZ converges) (ADJP (ADJP (JJR faster)) (PP (IN to) (NP (DT an) (ADJP (RB approximately) (JJ optimal)) (NN policy)))))))))) (. .))
(S (NP (PRP We)) (VP (VBP conclude) (PP (IN by) (S (VP (VBG presenting) (NP (JJ numerical) (NNS experiments)) (PP (IN on) (NP (NP (DT a) (ADJP (RB randomly) (VBN generated)) (NNP MDP)) (, ,) (NP (ADJP (NN event) (HYPH -) (VBN triggered)) (NN communication)) (, ,) (CC and) (NP (NN inventory) (NN management)))))))) (. .))
