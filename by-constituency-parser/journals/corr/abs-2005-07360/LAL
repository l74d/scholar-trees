(S (S (NP (VBG Learning) (NN rate) (NN schedule)) (VP (MD can) (VP (ADVP (RB significantly)) (VB affect) (NP (NP (NN generalization) (NN performance)) (PP (IN in) (NP (JJ modern) (JJ neural) (NNS networks))))))) (, ,) (CC but) (S (NP (NP (DT the) (NNS reasons)) (PP (IN for) (NP (DT this)))) (VP (VBP are) (RB not) (ADVP (RB yet)) (VP (JJ understood)))) (. .))
(S (NP (NP (NNP Li-Wei-Ma)) (PRN (-LRB- -LRB-) (NP (CD 2019)) (-RRB- -RRB-))) (ADVP (RB recently)) (VP (VBD proved) (SBAR (S (NP (DT this) (NN behavior)) (VP (MD can) (VP (VB exist) (PP (IN in) (NP (DT a) (JJ simplified) (JJ non-convex) (JJ neural-network) (NN setting)))))))) (. .))
(S (PP (IN In) (NP (DT this) (NN note))) (, ,) (NP (PRP we)) (VP (VBP show) (SBAR (IN that) (S (NP (DT this) (NN phenomenon)) (VP (MD can) (VP (VB exist) (PP (ADVP (RB even)) (IN for) (NP (NP (NN convex) (NN learning) (NNS problems)) (: â€”) (FRAG (PP (IN in) (NP (JJ particular))) (, ,) (NP (NP (JJ linear) (NN regression)) (PP (IN in) (NP (CD 2) (NNS dimensions)))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP give) (NP (NP (DT a) (NN toy) (NN convex) (NN problem)) (SBAR (WHADVP (WRB where)) (S (NP (NP (VBG learning) (NN rate) (NN annealing)) (PRN (-LRB- -LRB-) (NP (NP (JJ large) (JJ initial) (NN learning) (NN rate)) (, ,) (VP (VBN followed) (PP (IN by) (NP (JJ small) (VBG learning) (NN rate))))) (-RRB- -RRB-))) (VP (MD can) (VP (VB lead) (NP (JJ gradient) (NN descent)) (PP (TO to) (NP (VB minima))) (PP (IN with) (NP (NP (ADJP (RB provably) (JJR better)) (NN generalization)) (PP (IN than) (S (VP (VBG using) (NP (DT a) (JJ small) (NN learning) (NN rate)) (ADVP (IN throughout))))))))))))) (. .))
(S (PP (IN In) (NP (PRP$ our) (NN case))) (, ,) (NP (DT this)) (VP (VBZ occurs) (PP (JJ due) (TO to) (NP (NP (DT a) (NN combination)) (PP (IN of) (NP (NP (NP (DT the) (NN mismatch)) (PP (IN between) (NP (DT the) (NN test) (CC and) (NN train) (NN loss) (NNS landscapes)))) (, ,) (CC and) (NP (NN early-stopping))))))) (. .))
