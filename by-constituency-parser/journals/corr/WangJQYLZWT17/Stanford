(S (PP (IN In) (NP (DT this) (NN work))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (`` ") (JJ Residual) (NN Attention) (NN Network) ('' ")) (, ,) (NP (NP (DT a) (JJ convolutional) (JJ neural) (NN network)) (VP (VBG using) (NP (NP (NN attention) (NN mechanism)) (SBAR (WHNP (WDT which)) (S (VP (MD can) (VP (VB incorporate) (PP (IN with) (NP (NP (NML (NML (NN state)) (HYPH -) (PP (IN of) (HYPH -) (NP (NN art) (NN feed)))) (JJ forward) (NN network) (NN architecture)) (PP (IN in) (NP (DT an) (NML (NML (NN end)) (HYPH -) (PP (IN to) (HYPH -) (NP (NN end) (NN training)))) (NN fashion)))))))))))))) (. .))
(S (NP (PRP$ Our) (JJ Residual) (NN Attention) (NN Network)) (VP (VBZ is) (VP (VBN built) (PP (IN by) (S (VP (VBG stacking) (NP (NP (NN Attention) (NNS Modules)) (SBAR (WHNP (WDT which)) (S (VP (VBP generate) (NP (ADJP (NN attention) (HYPH -) (JJ aware)) (NNS features))))))))))) (. .))
(S (NP (NP (DT The) (ADJP (NN attention) (HYPH -) (JJ aware)) (NNS features)) (PP (IN from) (NP (JJ different) (NNS modules)))) (VP (VBP change) (ADVP (RB adaptively)) (PP (IN as) (NP (NP (NNS layers)) (VP (VBG going) (ADVP (RBR deeper)))))) (. .))
(S (PP (IN Inside) (NP (DT each) (NN Attention))) (NP (NP (NNP Module)) (, ,) (NP (ADJP (JJ bottom) (HYPH -) (JJ up)) (NML (NN top) (HYPH -) (NN down)) (NN feedforward) (NN structure))) (VP (VBZ is) (VP (VBN used) (S (VP (TO to) (VP (VB unfold) (NP (NP (DT the) (NN feedforward)) (CC and) (NP (NN feedback) (NN attention) (NN process))) (PP (IN into) (NP (DT a) (JJ single) (NN feedforward) (NN process)))))))) (. .))
(S (ADVP (RB Importantly)) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NN attention) (JJ residual) (NN learning)) (S (VP (TO to) (VP (VB train) (NP (NP (ADJP (RB very) (JJ deep)) (JJ Residual) (NN Attention) (NNS Networks)) (SBAR (WHNP (WDT which)) (S (VP (MD can) (VP (VB be) (ADVP (RB easily)) (VP (VBN scaled) (PRT (RP up)) (PP (IN to) (NP (NP (NNS hundreds)) (PP (IN of) (NP (NNS layers))))))))))))))) (. .))
(S (NP (JJ Extensive) (NNS analyses)) (VP (VBP are) (VP (VBN conducted) (PP (IN on) (NP (NML (NML (NN CIFAR) (HYPH -) (CD 10)) (CC and) (NML (NN CIFAR) (HYPH -) (CD 100))) (NNS datasets))) (S (VP (TO to) (VP (VB verify) (NP (NP (DT the) (NN effectiveness)) (PP (IN of) (NP (NP (DT every) (NN module)) (VP (VBN mentioned) (ADVP (RB above))))))))))) (. .))
(S (NP (PRP$ Our) (JJ Residual) (NN Attention) (NN Network)) (VP (VBZ achieves) (NP (NML (NML (NN state)) (HYPH -) (PP (IN of) (HYPH -) (NP (DT the) (HYPH -) (NN art)))) (NN object) (NN recognition) (NN performance)) (PP (IN on) (NP (NP (CD three) (NN benchmark) (NNS datasets)) (PP (VBG including) (NP (NP (NN CIFAR) (HYPH -) (CD 10) (PRN (-LRB- -LRB-) (NP (QP (CD 3.90) (NN %)) (NN error)) (-RRB- -RRB-))) (, ,) (NP (NN CIFAR) (HYPH -) (CD 100) (PRN (-LRB- -LRB-) (NP (NML (CD 20.45) (NN %)) (NN error)) (-RRB- -RRB-))) (CC and) (NP (NP (NNP ImageNet)) (-LRB- -LRB-) (NP (NP (CD 4.8) (NN %)) (NP (NML (NML (JJ single) (NN model)) (CC and) (NML (JJ single) (NN crop))) (, ,) (NML (JJ top) (HYPH -) (CD 5)) (NN error))) (-RRB- -RRB-))))))) (. .))
(S (VP (VB Note) (SBAR (IN that) (, ,) (S (NP (PRP$ our) (NN method)) (VP (VBZ achieves) (NP (NML (CD 0.6) (NN %)) (NML (JJ top) (HYPH -) (CD 1)) (NN accuracy) (NN improvement)) (PP (IN with) (NP (NP (NML (CD 46) (NN %)) (NN trunk) (NN depth)) (CC and) (NP (NP (NML (CD 69) (NN %)) (JJ forward) (NNS FLOPs)) (VP (VBG comparing) (PP (IN to) (NP (NNP ResNet) (HYPH -) (CD 200))))))))))) (. .))
(S (NP (DT The) (NN experiment)) (ADVP (RB also)) (VP (VBZ demonstrates) (SBAR (IN that) (S (NP (PRP$ our) (NN network)) (VP (VBZ is) (ADJP (JJ robust) (PP (IN against) (NP (JJ noisy) (NNS labels)))))))) (. .))
