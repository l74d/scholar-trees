(S (NP (PRP We)) (VP (VBP propose) (NP (NP (DT a) (JJ novel) (NN training) (NN algorithm)) (PP (IN for) (NP (NN reinforcement) (NN learning))) (SBAR (WHNP (WDT which)) (S (VP (VBZ combines) (NP (NP (DT the) (NN strength)) (PP (IN of) (NP (JJ deep) (NN Q-learning)))) (PP (IN with) (NP (DT a) (JJ constrained) (NN optimization) (NN approach))) (S (VP (TO to) (VP (VP (VB tighten) (NP (NN optimality))) (CC and) (VP (VB encourage) (NP (JJR faster) (NN reward) (NN propagation))))))))))) (. .))
(S (NP (PRP$ Our) (NN novel) (NN technique)) (VP (VBZ makes) (S (NP (JJ deep) (NN reinforcement) (VBG learning)) (ADJP (JJR more) (JJ practical))) (PP (IN by) (S (VP (ADVP (RB drastically)) (VBG reducing) (NP (DT the) (NN training) (NN time)))))) (. .))
(S (NP (PRP We)) (VP (VP (VBP evaluate) (NP (NP (DT the) (NN performance)) (PP (IN of) (NP (PRP$ our) (NN approach))) (PP (IN on) (NP (NP (DT the) (CD 49) (NNS games)) (PP (IN of) (NP (DT the) (VBG challenging) (NNP Arcade) (NNP Learning) (NNP Environment))))))) (, ,) (CC and) (VP (NN report) (NP (NP (JJ significant) (NNS improvements)) (PP (IN in) (NP (DT both) (NP (NN training) (NN time)) (CC and) (NP (NN accuracy))))))) (. .))
