(S (NP (NP (NP (NP (JJ Stochastic) (NN gradient) (NN descent)) (-LRB- -LRB-) (NP (NNP SGD)) (-RRB- -RRB-)) (PP (IN with) (NP (JJ constant) (NN momentum)))) (CC and) (NP (NP (PRP$ its) (NNS variants)) (PP (JJ such) (IN as) (NP (NNP Adam))))) (VP (VBP are) (NP (NP (DT the) (NN optimization) (NNS algorithms)) (PP (IN of) (NP (NP (NP (NN choice)) (PP (IN for) (NP (NN training) (JJ deep) (JJ neural) (NNS networks)))) (-LRB- -LRB-) (NP (NNS DNNs)) (-RRB- -RRB-))))) (. .))
(S (SBAR (IN Since) (S (NP (NN DNN) (NN training)) (VP (VBZ is) (ADJP (RB incredibly) (RB computationally) (JJ expensive))))) (, ,) (NP (EX there)) (VP (VBZ is) (NP (NP (JJ great) (NN interest)) (PP (IN in) (S (VP (VBG speeding) (PRT (RP up)) (NP (DT the) (NN convergence))))))) (. .))
(S (NP (NNP Nesterov)) (VP (VBD accelerated) (SBAR (S (NP (NN gradient) (-LRB- -LRB-) (NN NAG) (-RRB- -RRB-)) (VP (VBZ improves) (NP (NP (DT the) (NN convergence) (NN rate)) (PP (IN of) (NP (NN gradient) (NN descent) (PRN (-LRB- -LRB-) (NP (NN GD)) (-RRB- -RRB-))))) (PP (IN for) (NP (NN convex) (NN optimization))) (S (VP (VP (VBG using) (NP (NP (DT a) (ADJP (RB specially) (VBN designed)) (NN momentum)) (: ;) (S (ADVP (RB however)) (, ,) (NP (PRP it)) (VP (VBZ accumulates) (NP (NP (NP (NN error)) (SBAR (WHADVP (WRB when)) (S (NP (DT an) (JJ inexact) (NN gradient)) (VP (VBZ is) (VP (VBN used)))))) (PP (-LRB- -LRB-) (JJ such) (IN as) (PP (IN in) (NP (NNP SGD))) (-RRB- -RRB-))))))) (, ,) (VP (VBG slowing) (NP (NN convergence)) (ADVP (IN at) (JJS best))) (CC and) (VP (VBG diverging) (ADVP (IN at) (JJS worst))))))))) (. .))
(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (NP (PRP we)) (VP (VBP propose) (S (NP (JJ Scheduled)) (VP (VB Restart) (NP (NP (NP (NNP SGD)) (-LRB- -LRB-) (NP (NNP SRSGD)) (-RRB- -RRB-)) (, ,) (NP (NP (DT a) (JJ new) (NML (NN NAG) (HYPH -) (NN style)) (NN scheme)) (PP (IN for) (NP (NN training) (NNS DNNs)))))))) (. .))
(S (NP (NNP SRSGD)) (VP (VP (VBZ replaces) (NP (DT the) (JJ constant) (NN momentum)) (PP (IN in) (NP (NNP SGD))) (PP (IN by) (NP (NP (DT the) (VBG increasing) (NN momentum)) (PP (IN in) (NP (NN NAG)))))) (CC but) (VP (VBZ stabilizes) (NP (DT the) (NNS iterations)) (PP (IN by) (S (VP (VBG resetting) (NP (DT the) (NN momentum)) (PP (IN to) (NP (CD zero))) (PP (VBG according) (PP (IN to) (NP (DT a) (NN schedule))))))))) (. .))
(S (S (S (VP (VBG Using) (NP (NP (DT a) (NN variety)) (PP (IN of) (NP (NNS models) (CC and) (NNS benchmarks)))) (PP (IN for) (NP (NN image) (NN classification))))) (, ,) (NP (PRP we)) (VP (VBP demonstrate) (SBAR (IN that) (, ,) (S (PP (IN in) (NP (NN training) (NNS DNNs))) (, ,) (NP (NNP SRSGD)) (ADVP (RB significantly)) (VP (VBZ improves) (NP (NN convergence) (CC and) (NN generalization))))))) (: ;) (S (PP (IN for) (NP (NP (NN instance)) (PP (IN in) (NP (NP (NN training) (NN ResNet200)) (PP (IN for) (NP (NNP ImageNet) (NN classification))))))) (, ,) (NP (NNP SRSGD)) (VP (VBZ achieves) (NP (NP (DT an) (NN error) (NN rate)) (PP (IN of) (NP (NP (CD 20.93) (NN %)) (PP (IN vs.) (NP (NP (DT the) (NN benchmark)) (PP (IN of) (NP (CD 22.13) (NN %)))))))))) (. .))
(S (NP (DT These) (NNS improvements)) (VP (VBP become) (ADJP (ADJP (RBR more) (JJ significant)) (SBAR (IN as) (S (NP (DT the) (NN network)) (VP (VBZ grows) (ADJP (JJR deeper))))))) (. .))
(S (ADVP (RB Furthermore)) (, ,) (PP (IN on) (NP (DT both) (NNP CIFAR) (CC and) (NNP ImageNet))) (, ,) (NP (NNP SRSGD)) (VP (VBZ reaches) (NP (ADJP (ADJP (JJ similar)) (CC or) (ADJP (RB even) (JJR better))) (NN error) (NNS rates)) (PP (IN with) (NP (ADJP (RB significantly) (JJR fewer)) (NN training) (NNS epochs))) (PP (VBN compared) (PP (IN to) (NP (DT the) (NNP SGD) (NN baseline))))) (. .))
