(S (S (VP (VBG Learning) (NP (NP (JJ compressed) (NNS representations)) (PP (IN of) (NP (NP (NN multivariate) (NN time) (NN series)) (PRN (-LRB- -LRB-) (NP (NNP MTS)) (-RRB- -RRB-))))))) (VP (VBZ facilitates) (NP (JJ data) (NN analysis)) (PP (PP (IN in) (NP (NP (DT the) (NN presence)) (PP (IN of) (NP (NP (NN noise)) (CC and) (NP (JJ redundant) (NN information)))))) (, ,) (CC and) (PP (IN for) (NP (NP (DT a) (JJ large) (NN number)) (PP (IN of) (NP (NX (NNS variates)) (CC and) (NX (NN time) (NNS steps)))))))) (. .))
(S (ADVP (RB However)) (, ,) (NP (JJ classical) (NN dimensionality) (NN reduction) (NNS approaches)) (VP (VP (VBP are) (VP (VBN designed) (PP (IN for) (NP (JJ vectorial) (NNS data))))) (CC and) (VP (MD can) (RB not) (VP (VB deal) (ADVP (RB explicitly)) (PP (IN with) (NP (VBG missing) (NNS values)))))) (. .))
(S (PP (IN In) (NP (DT this) (NN work))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (DT a) (JJ novel) (NN autoencoder) (NN architecture)) (VP (VBN based) (PP (IN on) (NP (JJ recurrent) (JJ neural) (NNS networks)))) (SBAR (S (VP (TO to) (VP (VB generate) (NP (NP (JJ compressed) (NNS representations)) (PP (IN of) (NP (NNP MTS)))))))))) (. .))
(S (S (NP (DT The) (VBN proposed) (NN model)) (VP (MD can) (VP (VB process) (NP (NP (NNS inputs)) (VP (VBN characterized) (PP (IN by) (NP (JJ variable) (NNS lengths)))))))) (CC and) (S (NP (PRP it)) (VP (VBZ is) (ADVP (RB specifically)) (VP (VBN designed) (S (VP (TO to) (VP (VB handle) (NP (VBG missing) (NNS data)))))))) (. .))
(S (NP (PRP$ Our) (NN autoencoder)) (VP (VBZ learns) (NP (NP (JJ fixed-length) (JJ vectorial) (NNS representations)) (, ,) (SBAR (WHNP (WP$ whose) (NN pairwise) (NNS similarities)) (S (VP (VBP are) (VP (VBN aligned) (PP (TO to) (NP (NP (DT a) (NN kernel) (NN function)) (SBAR (SBAR (WHNP (WDT that)) (S (VP (VBZ operates) (PP (IN in) (NP (NN input) (NN space)))))) (CC and) (SBAR (WHNP (IN that)) (S (VP (VBZ handles) (NP (VBG missing) (NNS values)))))))))))))) (. .))
(S (NP (DT This)) (VP (VBZ allows) (S (VP (TO to) (VP (VB learn) (NP (JJ good) (NNS representations)) (, ,) (PP (ADVP (RB even)) (IN in) (NP (NP (DT the) (NN presence)) (PP (IN of) (NP (NP (DT a) (JJ significant) (NN amount)) (PP (IN of) (NP (VBG missing) (NNS data))))))))))) (. .))
(S (S (VP (TO To) (VP (VB show) (NP (NP (DT the) (NN effectiveness)) (PP (IN of) (NP (DT the) (VBN proposed) (NN approach))))))) (, ,) (S (NP (PRP we)) (VP (VBP evaluate) (NP (NP (DT the) (NN quality)) (PP (IN of) (NP (DT the) (JJ learned) (NNS representations))) (PP (IN in) (NP (NP (JJ several) (NN classification) (NNS tasks)) (, ,) (PP (VBG including) (NP (NP (DT those)) (VP (VBG involving) (NP (JJ medical) (NNS data)))))))))) (, ,) (CC and) (S (NP (PRP we)) (VP (VBP compare) (PP (TO to) (NP (NP (JJ other) (NNS methods)) (PP (IN for) (NP (NN dimensionality) (NN reduction))))))) (. .))
(S (ADVP (RB Successively)) (, ,) (NP (PRP we)) (VP (VBP design) (NP (NP (NP (CD two) (NNS frameworks)) (VP (VBN based) (PP (IN on) (NP (DT the) (VBN proposed) (NN architecture))))) (: :) (NP (NP (NP (CD one)) (PP (IN for) (S (VP (VBG imputing) (NP (VBG missing) (NNS data)))))) (CC and) (NP (NP (DT another)) (PP (IN for) (NP (JJ one-class) (NN classification))))))) (. .))
(S (ADVP (RB Finally)) (, ,) (NP (PRP we)) (VP (VBP analyze) (SBAR (WHPP (IN under) (WHNP (WP what) (NNS circumstances))) (S (NP (NP (DT an) (NN autoencoder)) (PP (IN with) (NP (JJ recurrent) (NNS layers)))) (VP (MD can) (VP (VB learn) (NP (NP (JJR better) (JJ compressed) (NNS representations)) (PP (IN of) (NP (NNP MTS))) (PP (IN than) (NP (JJ feed-forward) (NNS architectures))))))))) (. .))
