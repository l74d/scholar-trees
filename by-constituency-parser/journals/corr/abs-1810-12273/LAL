(S (NP (PRP We)) (VP (VBP introduce) (NP (NP (NNP Kalman) (NNP Gradient) (NNP Descent)) (, ,) (NP (NP (DT a) (JJ stochastic) (NN optimization) (NN algorithm)) (SBAR (WHNP (WDT that)) (S (VP (VBZ uses) (NP (NNP Kalman) (VBG filtering)) (S (VP (TO to) (VP (ADVP (RB adaptively)) (VB reduce) (NP (NP (NN gradient) (NN variance)) (PP (IN in) (NP (JJ stochastic) (NN gradient) (NN descent)))) (PP (IN by) (S (VP (VBG filtering) (NP (DT the) (NN gradient) (NNS estimates)))))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP present) (NP (DT both) (NP (NP (DT a) (JJ theoretical) (NN analysis)) (PP (IN of) (NP (NP (NN convergence)) (PP (IN in) (NP (DT a) (JJ non-convex) (NN setting)))))) (CC and) (NP (NP (JJ experimental) (NNS results)) (SBAR (WHNP (WDT which)) (S (VP (VBP demonstrate) (NP (NP (VBN improved) (NN performance)) (PP (IN on) (NP (NP (DT a) (NN variety)) (PP (IN of) (NP (NP (NN machine) (VBG learning) (NNS areas)) (PP (VBG including) (NP (NP (JJ neural) (NNS networks)) (CC and) (NP (JJ black) (NN box) (JJ variational) (NN inference))))))))))))))) (. .))
(S (S (NP (PRP We)) (ADVP (RB also)) (VP (VBD present) (NP (NP (DT a) (JJ distributed) (NN version)) (PP (IN of) (NP (PRP$ our) (NN algorithm))) (SBAR (WHNP (IN that)) (S (VP (VBZ enables) (NP (JJ large-dimensional) (NN optimization)))))))) (, ,) (CC and) (S (NP (PRP we)) (VP (VBP extend) (NP (PRP$ our) (NN algorithm)) (PP (TO to) (NP (NNP SGD))) (PP (IN with) (NP (NP (NN momentum)) (CC and) (NP (NNP RMSProp)))))) (. .))
