(S (NP (NNP Analog) (NNS arrays)) (VP (VBP are) (NP (NP (DT a) (JJ promising) (JJ upcoming) (NN hardware) (NN technology)) (PP (IN with) (NP (DT the) (NN potential) (S (VP (TO to) (ADVP (RB drastically)) (VP (VB speed) (PRT (RP up)) (NP (JJ deep) (NN learning))))))))) (. .))
(S (NP (PRP$ Their) (JJ main) (NN advantage)) (VP (VBZ is) (SBAR (IN that) (S (NP (PRP they)) (VP (VBP compute) (NP (NML (NN matrix) (HYPH -) (NN vector)) (NNS products)) (PP (IN in) (NP (JJ constant) (NN time))) (, ,) (ADVP (RB irrespective) (PP (IN of) (NP (NP (DT the) (NN size)) (PP (IN of) (NP (DT the) (NN matrix)))))))))) (. .))
(S (ADVP (RB However)) (, ,) (NP (NP (JJ early) (NN convolution) (NNS layers)) (PP (IN in) (NP (NNP ConvNets)))) (VP (VBP map) (ADJP (RB very) (PP (ADVP (RB unfavorably)) (IN onto) (NP (NN analog) (NNS arrays)))) (, ,) (SBAR (IN because) (S (S (NP (NN kernel) (NNS matrices)) (VP (VBP are) (ADVP (RB typically)) (ADJP (JJ small)))) (CC and) (S (NP (DT the) (JJ constant) (NN time) (NN operation)) (VP (VBZ needs) (S (VP (TO to) (VP (VB be) (ADVP (RB sequentially)) (VP (VBN iterated) (NP (NP (DT a) (JJ large) (NN number)) (PP (IN of) (NP (NNS times)))) (, ,) (S (VP (VBG reducing) (NP (NP (DT the) (NML (NN speed) (RP up)) (NN advantage)) (PP (IN for) (NP (NNPS ConvNets))))))))))))))) (. .))
(S (ADVP (RB Here)) (, ,) (NP (PRP we)) (VP (VP (VBP propose) (PP (IN to) (NP (NP (NN replicate)) (NP (NP (DT the) (NN kernel) (NN matrix)) (PP (IN of) (NP (NP (DT a) (NN convolution) (NN layer)) (PP (IN on) (NP (JJ distinct) (NN analog) (NNS arrays))))))))) (, ,) (CC and) (VP (ADVP (RB randomly)) (VBP divide) (NP (NP (NNS parts)) (PP (IN of) (NP (DT the) (NML (S (VP (VB compute) (PP (IN among) (NP (PRP them))))))))) (, ,) (SBAR (IN so) (IN that) (S (NP (JJ multiple) (NN kernel) (NNS matrices)) (VP (VBP are) (VP (VBN trained) (PP (IN in) (NP (NN parallel))))))))) (. .))
(S (PP (IN With) (NP (DT this) (NN modification))) (, ,) (NP (NN analog) (NNS arrays)) (VP (VB execute) (NP (NNPS ConvNets)) (PP (IN with) (NP (NP (DT an) (NN acceleration) (NN factor)) (SBAR (WHNP (WDT that)) (S (VP (VBZ is) (ADJP (JJ proportional) (PP (IN to) (NP (NP (DT the) (NN number)) (PP (IN of) (NP (NP (NN kernel) (NNS matrices)) (VP (VBN used) (PP (IN per) (NP (NN layer)))) (PRN (-LRB- -LRB-) (ADVP (RB here)) (VP (VBN tested) (NP-TMP (CD 16) (HYPH -) (CD 128))) (-RRB- -RRB-))))))))))))) (. .))
(S (PP (IN Despite) (S (VP (VBG having) (NP (JJR more) (JJ free) (NNS parameters))))) (, ,) (NP (PRP we)) (VP (VBP show) (ADVP (RB analytically)) (VP (CC and) (PP (IN in) (NP (JJ numerical) (NNS experiments))) (SBAR (IN that) (S (NP (DT this) (NN convolution) (NN architecture)) (VP (VP (VBZ is) (VP (NN self) (HYPH -) (VBG regularizing))) (CC and) (ADVP (RB implicitly)) (VP (VBZ learns) (NP (JJ similar) (NNS filters)) (PP (IN across) (NP (NNS arrays))))))))) (. .))
(S (NP (PRP We)) (ADVP (RB also)) (VP (VP (VBP report) (NP (JJ superior) (NN performance)) (PP (IN on) (NP (NP (DT a) (NN number)) (PP (IN of) (NP (NNS datasets)))))) (CC and) (VP (VBD increased) (NP (NN robustness)) (PP (IN to) (NP (JJ adversarial) (NNS attacks))))) (. .))
(S (NP (PRP$ Our) (NN investigation)) (VP (VBZ suggests) (S (VP (TO to) (VP (VB revise) (NP (DT the) (NN notion)) (SBAR (IN that) (S (NP (JJ mixed) (NN analog) (HYPH -) (JJ digital) (NN hardware)) (VP (VBZ is) (RB not) (ADJP (JJ suitable) (PP (IN for) (NP (NNPS ConvNets))))))))))) (. .))
