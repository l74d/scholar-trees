(S (NP (NP (NN Regularization)) (PP (IN in) (NP (NP (DT the) (NN optimization)) (PP (IN of) (NP (JJ deep) (JJ neural) (NNS networks)))))) (VP (VBZ is) (ADVP (RB often)) (ADJP (JJ critical) (S (VP (TO to) (VP (VB avoid) (NP (NP (JJ undesirable) (JJ over-fitting)) (VP (VBG leading) (PP (TO to) (NP (NP (JJR better) (NN generalization)) (PP (IN of) (NP (NN model)))))))))))) (. .))
(S (S (NP (NP (CD One)) (PP (IN of) (NP (DT the) (ADJP (RBS most) (JJ popular)) (NN regularization) (NN algorithms)))) (VP (VBZ is) (S (VP (TO to) (VP (VB impose) (NP (NP (NNP L-2) (NN penalty))) (PP (IN on) (NP (DT the) (NN model) (NNS parameters))) (VP (VBG resulting) (PP (IN in) (NP (NP (DT the) (NN decay)) (PP (IN of) (NP (NNS parameters))) (, ,) (VP (VBD called) (S (NP (NN weight-decay)))))))))))) (, ,) (CC and) (S (NP (DT the) (NN decay) (NN rate)) (VP (VBZ is) (ADVP (RB generally)) (ADJP (JJ constant) (PP (TO to) (NP (NP (PDT all) (DT the) (NN model) (NNS parameters)) (PP (IN in) (NP (NP (DT the) (NN course)) (PP (IN of) (NP (NN optimization)))))))))) (. .))
(S (PP (IN In) (NP (NP (NN contrast)) (PP (TO to) (NP (NP (DT the) (JJ previous) (NN approach)) (VP (VBN based) (PP (IN on) (NP (NP (DT the) (JJ constant) (NN rate)) (PP (IN of) (NP (NN weight-decay)))))))))) (, ,) (NP (PRP we)) (VP (VBP propose) (S (VP (TO to) (VP (VB consider) (NP (NP (DT the) (NN residual)) (SBAR (WHNP (WDT that)) (S (VP (VBZ measures) (NP (NP (NN dissimilarity)) (PP (IN between) (NP (NP (NP (DT the) (JJ current) (NN state)) (PP (IN of) (NP (NN model)))) (CC and) (NP (NNS observations))))) (PP (IN in) (NP (NP (DT the) (NN determination)) (PP (IN of) (NP (NP (DT the) (NN weight-decay)) (PP (IN for) (NP (DT each) (NN parameter))))))) (PP (IN in) (NP (NP (DT an) (JJ adaptive) (NN way)) (, ,) (VP (VBD called) (S (NP (NP (JJ adaptive) (NN weight-decay)) (PRN (-LRB- -LRB-) (NP (NNP AdaDecay)) (-RRB- -RRB-))))) (SBAR (WHADVP (WRB where)) (S (S (NP (DT the) (NN gradient) (NNS norms)) (VP (VBP are) (VP (VBN normalized) (PP (IN within) (NP (DT each) (NN layer)))))) (CC and) (S (NP (NP (DT the) (NN degree)) (PP (IN of) (NP (NN regularization))) (PP (IN for) (NP (DT each) (NN parameter)))) (VP (VBZ is) (VP (VBN determined) (PP (IN in) (ADJP (JJ proportional) (PP (TO to) (NP (NP (DT the) (NN magnitude)) (PP (IN of) (NP (PRP$ its) (NN gradient))))))) (S (VP (VBG using) (NP (DT the) (JJ sigmoid) (NN function))))))))))))))))))) (. .))
(S (NP (PRP We)) (VP (ADVP (RB empirically)) (VBP demonstrate) (NP (NP (DT the) (NN effectiveness)) (PP (IN of) (NP (NNP AdaDecay)))) (PP (IN in) (NP (NP (NN comparison)) (PP (TO to) (NP (DT the) (JJ state-of-the-art) (NN optimization) (IN algorithms))))) (S (VP (VBG using) (NP (NP (JJ popular) (NN benchmark) (NNS datasets)) (: :) (NP (NP (NP (NNP MNIST)) (, ,) (NP (NNP Fashion-MNIST)) (, ,) (CC and) (NP (NNP CIFAR-10))) (PP (IN with) (NP (NP (JJ conventional) (JJ neural) (NN network) (NNS models)) (VP (VBG ranging) (PP (PP (IN from) (ADJP (NN shallow))) (PP (TO to) (ADJP (VB deep)))))))))))) (. .))
(S (NP (NP (DT The) (JJ quantitative) (NN evaluation)) (PP (IN of) (NP (PRP$ our) (VBN proposed) (NN algorithm)))) (VP (VBZ indicates) (SBAR (IN that) (S (NP (NNP AdaDecay)) (VP (VBZ improves) (NP (NP (NN generalization)) (VP (VBG leading) (PP (TO to) (NP (NP (RBR better) (NN accuracy)) (PP (IN across) (NP (PDT all) (DT the) (NNS datasets) (CC and) (NNS models))))))))))) (. .))
