(S (S (VP (VBG Training) (NP (JJ convolutional) (NML (JJ neural) (NN network)) (NNS models)))) (VP (VBZ is) (ADJP (ADJP (NN memory) (JJ intensive)) (PP (IN since) (NP (NP (RB back) (HYPH -) (NN propagation)) (SBAR (S (VP (VBZ requires) (S (VP (VBG storing) (NP (NP (NNS activations)) (PP (IN of) (NP (DT all) (JJ intermediate) (NNS layers))))))))))))) (. .))
(S (NP (DT This)) (VP (VBZ presents) (NP (DT a) (JJ practical) (NN concern)) (SBAR (SBAR (WHADVP (WRB when)) (S (VP (VBG seeking) (S (VP (TO to) (VP (VB deploy) (NP (ADJP (RB very) (JJ deep)) (NNS architectures)) (PP (IN in) (NP (NN production))))))))) (, ,) (RB especially) (SBAR (WHADVP (WRB when)) (S (NP (NNS models)) (VP (VBP need) (S (VP (TO to) (VP (VB be) (ADVP (RB frequently)) (VP (VBN re-trained) (PP (IN on) (NP (VBN updated) (NNS datasets)))))))))))) (. .))
(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (DT a) (JJ new) (NN implementation)) (PP (IN for) (NP (NP (RB back) (HYPH -) (NN propagation)) (SBAR (WHNP (WDT that)) (S (ADVP (RB significantly)) (VP (VBZ reduces) (NP (NN memory) (NN usage)))))))) (, ,) (PP (IN by) (S (VP (VBG enabling) (NP (NP (DT the) (NN use)) (PP (IN of) (NP (NP (NNS approximations)) (PP (IN with) (NP (NP (JJ negligible) (JJ computational) (NN cost)) (CC and) (NP (JJ minimal) (NN effect))))))) (PP (IN on) (NP (NN training) (NN performance))))))) (. .))
(S (NP (DT The) (NN algorithm)) (VP (VBZ reuses) (NP (JJ common) (NNS buffers)) (S (VP (TO to) (ADVP (RB temporarily)) (VP (VP (VB store) (NP (JJ full) (NNS activations))) (CC and) (VP (VB compute) (NP (DT the) (JJ forward) (NN pass)) (ADVP (RB exactly))))))) (. .))
(S (NP (PRP It)) (ADVP (RB also)) (VP (VBZ stores) (NP (NP (JJ approximate) (NML (IN per) (HYPH -) (NN layer)) (NNS copies)) (PP (IN of) (NP (NNS activations)))) (, ,) (PP (IN at) (NP (NP (JJ significant) (NN memory) (NNS savings)) (, ,) (SBAR (WHNP (WDT that)) (S (VP (VBP are) (VP (VBN used) (PP (IN in) (NP (DT the) (JJ backward) (NN pass)))))))))) (. .))
(PP (PP (VBN Compared) (PP (IN to) (S (ADVP (RB simply)) (VP (VBG approximating) (NP (NNS activations)) (PP (IN within) (NP (NP (ADJP (JJ standard) (NP-TMP (NP (RB back) (HYPH -) (NN propagation)) (, ,) (NP (PRP$ our) (NN method) (NNS limits)))) (NN accumulation)) (PP (IN of) (NP (NP (NNS errors)) (PP (IN across) (NP (NNS layers))))))))))) (. .))
(S (NP (DT This)) (VP (VBZ allows) (NP (NP (DT the) (NN use)) (PP (IN of) (NP (NP (RB much) (JJR lower)) (HYPH -) (NP (NN precision) (NNS approximations))))) (PP (IN without) (S (VP (VBG affecting) (NP (NN training) (NN accuracy)))))) (. .))
(S (NP (NP (NNS Experiments)) (PP (IN on) (NP (NP (NN CIFAR) (HYPH -) (CD 10)) (, ,) (NP (NN CIFAR) (HYPH -) (CD 100)) (, ,) (CC and) (NP (NNP ImageNet))))) (VP (VBP show) (SBAR (IN that) (S (NP (PRP$ our) (NN method) (NNS yields)) (VP (VBP performance) (ADJP (JJ close) (PP (IN to) (NP (JJ exact) (NN training)))) (, ,) (SBAR (IN while) (S (VP (VBG storing) (NP (NNS activations)) (ADVP (RB compactly)) (PP (IN with) (NP (QP (RB as) (JJ low) (IN as) (CD 4) (HYPH -) (NN bit)) (NN precision)))))))))) (. .))
