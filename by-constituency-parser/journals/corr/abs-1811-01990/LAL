(S (NP (PRP We)) (VP (VBP propose) (CC and) (VBP compare) (NP (NP (NNS methods)) (PP (IN for) (NP (NP (JJ gradient-based) (NN domain) (NN adaptation)) (PP (IN of) (NP (JJ self-attentive) (JJ neural) (NN machine) (NN translation) (NNS models))))))) (. .))
(S (NP (PRP We)) (VP (VBP demonstrate) (SBAR (IN that) (S (NP (NP (DT a) (JJ large) (NN proportion)) (PP (IN of) (NP (NN model) (NNS parameters)))) (VP (MD can) (VP (VB be) (VP (JJ frozen) (PP (IN during) (NP (NN adaptation))) (PP (IN with) (NP (NP (ADJP (JJ minimal) (CC or) (DT no)) (NN reduction)) (PP (IN in) (NP (NN translation) (NN quality))))) (PP (IN by) (S (VP (VBG encouraging) (NP (NP (VBN structured) (NN sparsity)) (PP (IN in) (NP (NP (DT the) (NN set)) (PP (IN of) (NP (JJ offset) (NNS tensors)))))) (PP (IN during) (NP (VBG learning))) (PP (IN via) (NP (NN group) (JJ lasso) (NN regularization)))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP evaluate) (NP (DT this) (NN technique)) (PP (IN for) (NP (UCP (DT both) (NN batch) (CC and) (JJ incremental)) (NN adaptation))) (PP (IN across) (NP (JJ multiple) (NX (NX (NNS data) (NNS sets)) (CC and) (NX (NN language) (NNS pairs)))))) (. .))
(S (NP (NP (PRP$ Our) (NN system) (NN architecture)) (: -) (VP (VBG combining) (NP (DT a) (JJ state-of-the-art) (JJ self-attentive) (NN model)) (PP (IN with) (NP (JJ compact) (NN domain) (NN adaptation)))) (: -)) (VP (VBZ provides) (NP (NP (ADJP (JJ high) (NN quality)) (VBN personalized) (NN machine) (NN translation)) (SBAR (WHNP (WDT that)) (S (VP (VBZ is) (ADJP (DT both) (NN space) (CC and) (NN time) (NN efficient))))))) (. .))
