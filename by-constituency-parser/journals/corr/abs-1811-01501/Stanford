(S (NP (PRP We)) (VP (VBP propose) (NP (NP (DT a) (JJ new) (NN optimization) (NN method)) (PP (IN for) (NP (ADJP (NP (NN training) (NN feed)) (HYPH -) (JJ forward)) (JJ neural) (NNS networks))))) (. .))
(S (S (PP (IN By) (S (VP (VBG rewriting) (NP (DT the) (NN activation) (NN function)) (PP (IN as) (NP (DT an) (ADJP (JJ equivalent) (JJ proximal)) (NN operator)))))) (, ,) (NP (PRP we)) (VP (VBP approximate) (NP (DT a) (NML (NN feed) (HYPH -) (JJ forward)) (JJ neural) (NN network)) (PP (IN by) (S (VP (VBG adding) (NP (DT the) (JJ proximal) (NNS operators)) (PP (IN to) (NP (NP (DT the) (JJ objective) (NN function)) (PP (IN as) (NP (NNS penalties)))))))))) (, ,) (S (ADVP (RB hence)) (NP (PRP we)) (VP (VBP call) (NP (DT the) (VBN lifted) (JJ proximal) (NN operator) (NN machine) (PRN (-LRB- -LRB-) (NP (NN LPOM)) (-RRB- -RRB-))))) (. .))
(S (NP (NNP LPOM)) (VP (VBZ is) (NP (NP (NN block) (NN multi-convex)) (PP (IN in) (NP (DT all) (JJ layer-wise) (NNS weights) (CC and) (NNS activations))))) (. .))
(S (NP (DT This)) (VP (VBZ allows) (S (NP (PRP us)) (VP (TO to) (VP (VB use) (NP (NN block) (NN coordinate) (NN descent)) (S (VP (TO to) (VP (VB update) (NP (NP (DT the) (JJ layer-wise) (NNS weights) (CC and) (NNS activations)) (PP (IN in) (NP (NN parallel))))))))))) (. .))
(S (ADVP (RBS Most) (RB notably)) (, ,) (NP (PRP we)) (ADVP (RB only)) (VP (VBP use) (NP (NP (DT the) (NN mapping)) (PP (IN of) (NP (NP (NP (DT the) (NN activation) (NN function)) (NP (PRP itself))) (, ,) (CONJP (RB rather) (IN than)) (NP (PRP$ its) (NNS derivatives))))) (, ,) (S (ADVP (RB thus)) (VP (VBG avoiding) (NP (NP (DT the) (NN gradient) (VBG vanishing)) (CC or) (NP (NP (NML (S (VP (VB blow) (HYPH -) (PRT (RP up))))) (NNS issues)) (PP (IN in) (NP (NP (NN gradient)) (VP (VBN based) (NP (NN training) (NNS methods)))))))))) (. .))
(S (ADVP (RB So)) (NP (PRP$ our) (NN method)) (VP (VBZ is) (ADJP (JJ applicable) (PP (IN to) (NP (NP (JJ various)) (VP (VBG non-decreasing) (NP (NP (NNP Lipschitz) (NML (JJ continuous) (NN activation)) (NNS functions)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (MD can) (VP (VB be) (ADJP (ADJP (VBG saturating)) (CC and) (ADJP (JJ non-differentiable))))))))))))) (. .))
(S (NP (NNP LPOM)) (VP (VBZ does) (RB not) (VP (VB require) (NP (NP (JJR more) (JJ auxiliary) (NNS variables)) (PP (IN than) (NP (DT the) (JJ layer-wise) (NNS activations)))) (, ,) (S (ADVP (RB thus)) (VP (VBG using) (NP (NP (RB roughly) (DT the) (JJ same) (NN amount)) (PP (IN of) (NP (NN memory)))))) (SBAR (IN as) (S (NP (JJ stochastic) (NN gradient) (NN descent) (PRN (-LRB- -LRB-) (NP (NNP SGD)) (-RRB- -RRB-))) (VP (VBZ does)))))) (. .))
(S (NP (PRP We)) (ADVP (RB further)) (VP (VB prove) (NP (NP (DT the) (NN convergence)) (PP (IN of) (S (VP (VBG updating) (NP (DT the) (JJ layer-wise) (NNS weights) (CC and) (NNS activations))))))) (. .))
(S (NP (NP (NNS Experiments)) (PP (IN on) (NP (NML (NML (NN MNIST)) (CC and) (NML (NN CIFAR) (HYPH -) (CD 10))) (NNS datasets)))) (VP (VB testify) (PP (IN to) (NP (NP (DT the) (NNS advantages)) (PP (IN of) (NP (NNP LPOM)))))) (. .))
