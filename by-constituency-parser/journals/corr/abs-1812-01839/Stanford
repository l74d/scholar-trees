(S (NP (NP (JJ Deep) (NML (JJ neural) (NN network) (NN compression)) (NNS techniques)) (PP (JJ such) (IN as) (NP (NP (NN pruning)) (CC and) (NP (NN weight) (NN tensor) (NN decomposition))))) (ADVP (RB usually)) (VP (VBP require) (NP (JJ fine) (HYPH -) (NN tuning)) (S (VP (TO to) (VP (VB recover) (NP (DT the) (NN prediction) (NN accuracy)) (SBAR (WHADVP (WRB when)) (S (NP (DT the) (NN compression) (NN ratio)) (VP (VBZ is) (ADJP (JJ high))))))))) (. .))
(S (ADVP (RB However)) (, ,) (NP (JJ conventional) (NML (JJ fine) (HYPH -) (NN tuning))) (VP (VBZ suffers) (PP (IN from) (NP (NP (DT the) (NN requirement)) (PP (IN of) (NP (NP (DT a) (JJ large) (NN training) (NN set)) (CC and) (NP (DT the) (ADJP (NN time) (HYPH -) (VBG consuming)) (NN training) (NN procedure))))))) (. .))
(S (NP (DT This) (NN paper)) (VP (VBZ proposes) (NP (NP (DT a) (JJ novel) (NN solution)) (PP (IN for) (NP (NN knowledge) (NN distillation)))) (PP (IN from) (NP (ADJP (NP (NN label)) (HYPH -) (JJ free)) (JJ few) (NNS samples))) (S (VP (TO to) (VP (VB realize) (NP (DT both) (NML (NML (NNS data) (NN efficiency)) (CC and) (NML (NN training) (HYPH /) (NN processing))) (NN efficiency)))))) (. .))
(S (NP (PRP We)) (VP (VBP treat) (NP (NP (NP (DT the) (JJ original) (NN network)) (PP (IN as) (`` ") (NP (NN teacher) (HYPH -) (NN net)) ('' "))) (CC and) (NP (NP (DT the) (ADJP (JJ compressed)) (NN network)) (PP (IN as) (`` ") (NP (NN student) (HYPH -) (NN net)) ('' "))))) (. .))
(S (S (NP (DT A) (NN 1x1) (NN convolution) (NN layer)) (VP (VBZ is) (VP (VBN added) (PP (IN at) (NP (NP (DT the) (NN end)) (PP (IN of) (NP (NP (DT each) (NN layer) (NN block)) (PP (IN of) (NP (DT the) (NN student) (HYPH -) (NN net)))))))))) (, ,) (CC and) (S (NP (PRP we)) (VP (VBP fit) (NP (NP (DT the) (NML (NN block) (HYPH -) (NN level)) (NNS outputs)) (PP (IN of) (NP (NP (DT the) (NN student) (HYPH -) (NN net)) (PP (IN to) (NP (DT the) (NN teacher) (HYPH -) (NN net)))))) (PP (IN by) (S (VP (VBG estimating) (NP (NP (DT the) (NNS parameters)) (PP (IN of) (NP (DT the) (VBN added) (NNS layers))))))))) (. .))
(S (NP (PRP We)) (VP (VBP prove) (SBAR (IN that) (S (NP (DT the) (VBN added) (NN layer)) (VP (MD can) (VP (VB be) (VP (VBN merged) (PP (IN without) (S (VP (VBG adding) (NP (NP (JJ extra) (NNS parameters)) (CC and) (NP (NN computation) (NN cost))) (PP (IN during) (NP (NN inference)))))))))))) (. .))
(S (NP (NP (NNS Experiments)) (PP (IN on) (NP (NP (JJ multiple) (NNS datasets)) (CC and) (NP (NN network) (NNS architectures))))) (VP (VBP verify) (NP (NP (DT the) (NN method) (POS 's)) (NN effectiveness)) (PP (IN on) (NP (NP (NN student) (HYPH -) (NNS nets)) (VP (VBN obtained) (PP (IN by) (NP (NP (JJ various) (NN network) (NN pruning)) (CC and) (NP (NN weight) (NN decomposition) (NNS methods)))))))) (. .))
(S (NP (PRP$ Our) (NN method)) (VP (MD can) (VP (VB recover) (NP (NP (NN student) (HYPH -) (NN net) (POS 's)) (NN accuracy)) (PP (IN to) (NP (NP (DT the) (JJ same) (NN level)) (PP (IN as) (NP (NP (JJ conventional) (NML (JJ fine) (HYPH -) (NN tuning)) (NNS methods)) (PP (IN in) (NP (NNS minutes))))))) (PP (IN while) (S (VP (VBG using) (NP (NP (RB only) (ADJP (NP (NML (CD 1) (NN %)) (NN label)) (HYPH -) (JJ free)) (NNS data)) (PP (IN of) (NP (DT the) (JJ full) (NN training) (NNS data))))))))) (. .))
