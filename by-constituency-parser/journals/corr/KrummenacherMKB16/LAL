(S (NP (NP (NNP Adaptive) (JJ stochastic) (NN gradient) (NNS methods)) (PP (JJ such) (IN as) (NP (NNP AdaGrad)))) (VP (VBP have) (VP (VBN gained) (NP (NN popularity)) (PP (IN in) (ADJP (JJ particular))) (PP (IN for) (S (VP (VBG training) (NP (JJ deep) (JJ neural) (NNS networks))))))) (. .))
(S (NP (DT The) (ADJP (ADVP (RBS most) (RB commonly)) (VBN used) (CC and) (VBN studied)) (NN variant)) (VP (VBZ maintains) (NP (NP (DT a) (JJ diagonal) (NN matrix) (NN approximation)) (PP (TO to) (NP (JJ second) (NN order) (NN information)))) (PP (IN by) (S (VP (VBG accumulating) (NP (NP (NN past) (NNS gradients)) (SBAR (WHNP (WDT which)) (S (VP (VBP are) (VP (VBN used) (S (VP (TO to) (VP (VB tune) (NP (DT the) (NN step) (NN size)) (ADVP (RB adaptively)))))))))))))) (. .))
(S (S (PP (IN In) (NP (JJ certain) (NNS situations))) (NP (NP (DT the) (JJ full-matrix) (NN variant)) (PP (IN of) (NP (NNP AdaGrad)))) (VP (VBZ is) (VP (VBN expected) (S (VP (TO to) (VP (VB attain) (NP (JJR better) (NN performance)))))))) (, ,) (S (ADVP (RB however)) (PP (IN in) (NP (JJ high) (NNS dimensions))) (NP (PRP it)) (VP (VBZ is) (ADJP (RB computationally) (JJ impractical)))) (. .))
(S (NP (PRP We)) (VP (JJ present) (NP (NP (NP (NNP Ada-LR)) (CC and) (NP (NNP RadaGrad))) (NP (NP (CD two) (ADJP (RB computationally) (JJ efficient)) (NNS approximations)) (PP (TO to) (NP (JJ full-matrix) (NNP AdaGrad))) (VP (VBN based) (PP (IN on) (NP (JJ randomized) (NN dimensionality) (NN reduction))))))) (. .))
(S (NP (PRP They)) (VP (VBP are) (ADJP (JJ able) (S (VP (TO to) (VP (VP (VB capture) (NP (NP (NNS dependencies)) (PP (IN between) (NP (NNS features))))) (CC and) (VP (VB achieve) (NP (NP (JJ similar) (NN performance)) (PP (TO to) (NP (JJ full-matrix) (NNP AdaGrad)))) (CC but) (PP (IN at) (NP (DT a) (ADJP (RB much) (JJR smaller)) (JJ computational) (NN cost))))))))) (. .))
(S (NP (PRP We)) (VP (VBP show) (SBAR (IN that) (S (NP (NP (DT the) (NN regret)) (PP (IN of) (NP (NNP Ada-LR)))) (VP (VBZ is) (ADJP (RB close) (PP (TO to) (NP (NP (DT the) (NN regret)) (PP (IN of) (NP (NP (JJ full-matrix) (NNP AdaGrad)) (SBAR (WHNP (WDT which)) (S (VP (MD can) (VP (VB have) (NP (NP (DT an) (ADJP (JJ up-to) (RB exponentially) (JJR smaller)) (NN dependence)) (PP (IN on) (NP (DT the) (NN dimension))) (PP (IN than) (NP (DT the) (JJ diagonal) (NN variant))))))))))))))))) (. .))
(S (ADVP (RB Empirically)) (, ,) (NP (PRP we)) (VP (VBP show) (SBAR (IN that) (S (NP (NNP Ada-LR) (CC and) (NNP RadaGrad)) (VP (VBP perform) (ADVP (RB similarly) (PP (TO to) (NP (JJ full-matrix) (NNP AdaGrad)))))))) (. .))
(S (PP (IN On) (NP (NP (DT the) (NN task)) (PP (IN of) (S (VP (VBG training) (NP (NP (JJ convolutional) (JJ neural) (NNS networks)) (CONJP (RB as) (RB well) (IN as)) (NP (JJ recurrent) (JJ neural) (NNS networks)))))))) (, ,) (NP (NNP RadaGrad)) (VP (VBZ achieves) (NP (NP (JJR faster) (NN convergence)) (PP (IN than) (NP (JJ diagonal) (NNP AdaGrad))))) (. .))
