(S (NP (NP (JJ Residual) (NN Network)) (-LRB- -LRB-) (NP (NNP ResNet)) (-RRB- -RRB-)) (VP (VBZ is) (NP (NP (DT the) (NML (NML (NN state)) (HYPH -) (PP (IN of) (HYPH -) (NP (DT the) (HYPH -) (NN art)))) (NN architecture)) (SBAR (WHNP (WDT that)) (S (VP (VBZ realizes) (NP (NP (JJ successful) (NN training)) (PP (IN of) (NP (ADJP (RB really) (JJ deep)) (JJ neural) (NN network))))))))) (. .))
(S (NP (PRP It)) (VP (VBZ is) (NP (ADJP (ADJP (RB also) (VBN known) (SBAR (IN that) (S (NP (NP (JJ good) (NN weight) (NN initialization)) (PP (IN of) (NP (JJ neural) (NN network)))) (VP (VBZ avoids) (NP (NP (NN problem)) (PP (IN of) (NP (VBG vanishing)))))))) (HYPH /) (ADJP (VBG exploding))) (NNS gradients))) (. .))
(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (NP (NP (VBN simplified) (NNS models)) (PP (IN of) (NP (NNS ResNets)))) (VP (VBP are) (VP (VBN analyzed))) (. .))
(S (NP (PRP We)) (VP (VBP argue) (SBAR (IN that) (S (NP (NP (NN goodness)) (PP (IN of) (NP (NNP ResNet)))) (VP (VBZ is) (VP (VBN correlated) (PP (IN with) (NP (DT the) (NN fact))) (SBAR (IN that) (S (NP (NNPS ResNets)) (VP (VBP are) (ADJP (RB relatively) (JJ insensitive) (PP (IN to) (NP (NP (NN choice)) (PP (IN of) (NP (JJ initial) (NNS weights)))))))))))))) (. .))
(S (NP (PRP We)) (ADVP (RB also)) (VP (VBP demonstrate) (SBAR (WHADVP (WRB how)) (S (NP (NN batch) (NN normalization)) (VP (VBZ improves) (NP (NP (NN backpropagation)) (PP (IN of) (NP (JJ deep) (NNPS ResNets)))) (PP (IN without) (S (VP (VBG tuning) (NP (NP (JJ initial) (NNS values)) (PP (IN of) (NP (NNS weights))))))))))) (. .))
