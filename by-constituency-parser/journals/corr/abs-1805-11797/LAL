(S (NP (NP (RB Long) (JJ short-term) (NN memory)) (PRN (-LRB- -LRB-) (NP (NNP LSTM)) (-RRB- -RRB-))) (VP (VBZ has) (VP (VBN been) (VP (ADVP (RB widely)) (VBN used) (PP (IN for) (NP (JJ sequential) (NNS data) (NN modeling)))))) (. .))
(S (NP (NNS Researchers)) (VP (VBP have) (VP (VBN increased) (NP (NNP LSTM) (NN depth)) (PP (IN by) (S (VP (VBG stacking) (NP (NNP LSTM) (NNS cells)) (S (VP (TO to) (VP (VB improve) (NP (NN performance)))))))))) (. .))
(S (NP (DT This)) (VP (VP (VBZ incurs) (NP (NN model) (NN redundancy))) (, ,) (VP (VBZ increases) (NP (JJ run-time) (NN delay))) (, ,) (CC and) (VP (VBZ makes) (S (NP (DT the) (NNP LSTMs)) (ADJP (RBR more) (NN prone) (PP (TO to) (NP (NN overfitting))))))) (. .))
(S (S (VP (TO To) (VP (VB address) (NP (DT these) (NNS problems))))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (DT a) (JJ hidden-layer) (NNP LSTM)) (PRN (-LRB- -LRB-) (NP (NNP H-LSTM)) (-RRB- -RRB-)) (SBAR (WHNP (IN that)) (S (VP (VBZ adds) (NP (JJ hidden) (NNS layers)) (PP (TO to) (NP (NP (NNP LSTM) (POS 's)) (JJ original) (ADJP (CD one) (NN level)) (JJ non-linear) (NN control) (NNS gates)))))))) (. .))
(S (NP (JJ H-LSTM)) (VP (NNS increases) (NP (NN accuracy)) (SBAR (IN while) (S (VP (VBG employing) (NP (JJR fewer) (JJ external) (NN stacked) (NNS layers))))) (, ,) (S (ADVP (RB thus)) (VP (VBG reducing) (NP (NP (NP (DT the) (NN number)) (PP (IN of) (NP (NNS parameters)))) (CC and) (NP (JJ run-time) (NN latency))) (ADVP (RB significantly))))) (. .))
(S (NP (PRP We)) (VP (VBP employ) (NP (JJ grow-and-prune) (PRN (-LRB- -LRB-) (NNP GP) (-RRB- -RRB-)) (NN training)) (S (VP (TO to) (VP (ADVP (RB iteratively)) (VB adjust) (NP (DT the) (JJ hidden) (NNS layers)) (PP (IN through) (NP (NP (JJ gradient-based) (NN growth)) (CC and) (NP (NP (JJ magnitude-based) (NN pruning)) (PP (IN of) (NP (NNS connections)))))))))) (. .))
(S (NP (DT This)) (VP (VBZ learns) (NP (NP (CC both) (NP (DT the) (NNS weights)) (CC and) (NP (DT the) (JJ compact) (NN architecture))) (PP (IN of) (NP (NNP H-LSTM) (NN control) (NNS gates))))) (. .))
(S (NP (PRP We)) (VP (VBP have) (VP (VBN GP-trained) (NP (NNP H-LSTMs)) (PP (IN for) (NP (NP (NN image) (NN captioning)) (CC and) (NP (NN speech) (NN recognition) (NNS applications)))))) (. .))
(S (PP (IN For) (NP (NP (DT the) (NNP NeuralTalk) (NN architecture)) (PP (IN on) (NP (DT the) (NNP MSCOCO) (NN dataset))))) (, ,) (NP (PRP$ our) (CD three) (NNS models)) (VP (VP (VP (VB reduce) (NP (NP (DT the) (NN number)) (PP (IN of) (NP (NNS parameters)))) (PP (IN by) (NP (CD 38.7x)))) (PRN (JJ -LSB-) (NP (NP (JJ floating-point) (NNS operations)) (PRN (-LRB- -LRB-) (NP (NNP FLOPs)) (-RRB- -RRB-))) (PP (IN by) (NP (CD 45.5x))) (NNS -RSB-))) (, ,) (VP (NP (JJ run-time) (NN latency)) (PP (IN by) (NP (CD 4.5x)))) (, ,) (CC and) (VP (VB improve) (NP (DT the) (NNP CIDEr) (NN score)) (PP (IN by) (NP (CD 2.6))))) (. .))
(S (PP (IN For) (NP (NP (DT the) (NNP DeepSpeech2) (NN architecture)) (PP (IN on) (NP (DT the) (NNP AN4) (NN dataset))))) (, ,) (NP (PRP$ our) (CD two) (NNS models)) (VP (VP (VB reduce) (NP (NP (DT the) (NN number)) (PP (IN of) (NP (NNS parameters)))) (PP (IN by) (NP (CD 19.4x))) (PRN (-LRB- -LRB-) (NP (NNP FLOPs)) (PP (IN by) (NP (CD 23.5x))) (-RRB- -RRB-))) (, ,) (VP (NP (JJ run-time) (NN latency)) (PP (IN by) (NP (CD 15.7) (NN %)))) (, ,) (CC and) (VP (NP (DT the) (NN word) (NN error) (NN rate)) (PP (PP (IN from) (NP (CD 12.9) (NN %))) (PP (TO to) (NP (CD 8.7) (NN %)))))) (. .))
(S (ADVP (RB Thus)) (, ,) (NP (JJ GP-trained) (NNP H-LSTMs)) (VP (MD can) (VP (VB be) (VP (VBN seen) (S (VP (TO to) (VP (VB be) (ADJP (JJ compact) (, ,) (RB fast) (, ,) (CC and) (NN accurate)))))))) (. .))
