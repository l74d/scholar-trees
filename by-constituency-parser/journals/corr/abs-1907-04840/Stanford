(S (NP (PRP We)) (VP (VBP demonstrate) (NP (NP (DT the) (NN possibility)) (PP (IN of) (SBAR (SBAR (WHNP (WP what)) (S (NP (PRP we)) (VP (VBP call) (NP (JJ sparse) (NN learning))))) (: :) (S (VP (VBD accelerated) (NP (NP (NN training)) (PP (IN of) (NP (NP (JJ deep) (JJ neural) (NNS networks)) (SBAR (WHNP (WDT that)) (S (VP (VBP maintain) (NP (JJ sparse) (NNS weights)) (PP (IN throughout) (NP (NN training))) (PP (IN while) (S (VP (VBG achieving) (NP (JJ dense) (NN performance) (NNS levels))))))))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP accomplish) (NP (DT this)) (PP (IN by) (NP (NP (VBG developing) (JJ sparse) (NN momentum)) (, ,) (NP (NP (DT an) (NN algorithm)) (SBAR (WHNP (WDT which)) (S (VP (VBZ uses) (ADVP (RB exponentially)) (VP (VBN smoothed) (NP (NP (NNS gradients)) (-LRB- -LRB-) (NP (NN momentum)) (-RRB- -RRB-)) (S (VP (TO to) (VP (VB identify) (NP (NP (NNS layers) (CC and) (NNS weights)) (SBAR (WHNP (WDT which)) (S (VP (VBP reduce) (NP (DT the) (NN error)) (ADVP (RB efficiently))))))))))))))))) (. .))
(S (NP (JJ Sparse) (NN momentum)) (VP (VBZ redistributes) (VP (VBN pruned) (NP (NP (NNS weights)) (PP (IN across) (NP (NNS layers)))) (PP (VBG according) (PP (IN to) (NP (NP (DT the) (JJ mean) (NN momentum) (NN magnitude)) (PP (IN of) (NP (DT each) (NN layer)))))))) (. .))
(S (PP (IN Within) (NP (DT a) (NN layer))) (, ,) (NP (JJ sparse) (NN momentum)) (VP (VBZ grows) (NP (NNS weights)) (PP (VBG according) (PP (IN to) (NP (NP (DT the) (NN momentum) (NN magnitude)) (PP (IN of) (NP (ADJP (NP (CD zero)) (HYPH -) (VBN valued)) (NNS weights))))))) (. .))
(S (NP (PRP We)) (VP (VBP demonstrate) (NP (NP (NP (ADJP (NN state) (HYPH -) (IN of) (HYPH -) (DT the) (HYPH -) (NN art)) (JJ sparse) (NN performance)) (PP (IN on) (NP (NNP MNIST) (, ,) (NNP CIFAR) (HYPH -) (CD 10) (, ,)))) (CC and) (NP (NNP ImageNet))) (, ,) (S (VP (VBG decreasing) (NP (DT the) (JJ mean) (NN error)) (PP (IN by) (NP (NP (DT a) (JJ relative) (CD 8) (NN %)) (, ,) (NP (CD 15) (NN %)) (, ,) (CC and) (NP (CD 6) (NN %)))) (PP (VBN compared) (PP (IN to) (NP (JJ other) (JJ sparse) (NNS algorithms))))))) (. .))
(S (ADVP (RB Furthermore)) (, ,) (NP (PRP we)) (VP (VBP show) (SBAR (IN that) (S (NP (JJ sparse) (NN momentum)) (ADVP (RB reliably)) (VP (VBZ reproduces) (NP (JJ dense) (NN performance) (NNS levels)) (SBAR (IN while) (S (VP (VBG providing) (PRT (RP up)) (PP (IN to) (NP (QP (CD 5.61) (SYM x))))) (NP-TMP (ADVP (RBR faster)) (NP (NN training))))))))) (. .))
(S (PP (IN In) (NP (PRP$ our) (NN analysis))) (, ,) (NP (NNS ablations)) (VP (VBP show) (SBAR (IN that) (S (NP (NP (DT the) (NNS benefits)) (PP (IN of) (NP (NP (NN momentum) (NN redistribution)) (CC and) (NP (NN growth) (NN increase))))) (PP (IN with) (NP (NP (DT the) (NN depth) (CC and) (NN size)) (PP (IN of) (NP (DT the) (NN network)))))))) (. .))
(S (ADVP (RB Additionally)) (, ,) (NP (PRP we)) (VP (VBP find) (SBAR (IN that) (S (NP (JJ sparse) (NN momentum)) (VP (VBZ is) (ADJP (JJ insensitive) (PP (IN to) (NP (NP (DT the) (NN choice)) (PP (IN of) (NP (PRP$ its) (NNS hyperparameters)))))) (S (VP (VBG suggesting) (SBAR (IN that) (S (NP (JJ sparse) (NN momentum)) (VP (VBZ is) (ADJP (JJ robust) (CC and) (JJ easy)) (S (VP (TO to) (VP (VB use))))))))))))) (. .))
