(S (NP (PRP We)) (VP (VBP demonstrate) (NP (NP (DT the) (NN possibility)) (PP (IN of) (NP (SBAR (WHNP (WP what)) (S (NP (PRP we)) (VP (VBP call) (S (NP (JJ sparse) (NN learning)))))) (: :) (NP (NP (JJ accelerated) (NN training)) (PP (IN of) (NP (NP (JJ deep) (JJ neural) (NNS networks)) (SBAR (WHNP (WDT that)) (S (VP (VBP maintain) (NP (JJ sparse) (NNS weights)) (PP (IN throughout) (NP (VBG training))) (SBAR (IN while) (S (VP (VBG achieving) (NP (JJ dense) (NN performance) (NNS levels))))))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP accomplish) (NP (DT this)) (PP (IN by) (S (VP (VBG developing) (NP (NP (JJR sparse) (NN momentum)) (, ,) (NP (NP (DT an) (NN algorithm)) (SBAR (WHNP (WDT which)) (S (VP (VBZ uses) (NP (NP (ADJP (RB exponentially) (VBN smoothed)) (NNS gradients)) (PRN (-LRB- -LRB-) (NP (NN momentum)) (-RRB- -RRB-))) (S (VP (TO to) (VP (VB identify) (NP (NP (NNS layers) (CC and) (NNS weights)) (SBAR (WHNP (WDT which)) (S (VP (VB reduce) (NP (DT the) (NN error)) (ADVP (RB efficiently)))))))))))))))))) (. .))
(S (NP (NNP Sparse) (NN momentum)) (VP (NNS redistributes) (NP (VBD pruned) (NNS weights)) (PP (IN across) (NP (NNS layers))) (PP (VBG according) (PP (TO to) (NP (NP (DT the) (JJ mean) (NN momentum) (NN magnitude)) (PP (IN of) (NP (DT each) (NN layer))))))) (. .))
(S (PP (IN Within) (NP (DT a) (NN layer))) (, ,) (NP (JJ sparse) (NN momentum)) (VP (VBZ grows) (NP (NNS weights)) (PP (VBG according) (PP (TO to) (NP (NP (DT the) (NN momentum) (NN magnitude)) (PP (IN of) (NP (JJ zero-valued) (NNS weights))))))) (. .))
(S (NP (PRP We)) (VP (VBP demonstrate) (NP (JJ state-of-the-art) (JJ sparse) (NN performance)) (PP (IN on) (NP (NP (NNP MNIST)) (, ,) (NP (NNP CIFAR-10)) (, ,) (CC and) (NP (NNP ImageNet)))) (, ,) (S (VP (VBG decreasing) (NP (DT the) (JJ mean) (NN error)) (PP (IN by) (NP (DT a) (JJ relative) (NX (NX (CD 8) (NN %)) (, ,) (NX (CD 15) (NN %)) (, ,) (CC and) (CD 6) (NN %)))) (PP (VBN compared) (PP (TO to) (NP (JJ other) (JJ sparse) (NN algorithms))))))) (. .))
(S (ADVP (RB Furthermore)) (, ,) (NP (PRP we)) (VP (VBP show) (SBAR (IN that) (S (NP (JJR sparse) (NN momentum)) (VP (ADVP (RB reliably)) (VBZ reproduces) (NP (JJ dense) (NN performance) (NNS levels)) (SBAR (IN while) (S (VP (VBG providing) (NP (ADJP (QP (RP up) (TO to) (CD 5.61x)) (RBR faster)) (NN training))))))))) (. .))
(S (PP (IN In) (NP (PRP$ our) (NN analysis))) (, ,) (NP (NNS ablations)) (VP (VBP show) (SBAR (IN that) (S (NP (NP (DT the) (NNS benefits)) (PP (IN of) (NP (NP (NN momentum) (NN redistribution)) (CC and) (NP (NN growth))))) (VP (NN increase) (PP (IN with) (NP (NP (DT the) (NN depth) (CC and) (NN size)) (PP (IN of) (NP (DT the) (NN network))))))))) (. .))
(S (ADVP (RB Additionally)) (, ,) (NP (PRP we)) (VP (VBP find) (SBAR (IN that) (S (NP (JJR sparse) (NN momentum)) (VP (VBZ is) (ADJP (JJ insensitive) (PP (TO to) (NP (NP (DT the) (NN choice)) (PP (IN of) (NP (PRP$ its) (NNS hyperparameters))))))))) (S (VP (VBG suggesting) (SBAR (IN that) (S (NP (JJ sparse) (NN momentum)) (VP (VBZ is) (ADJP (ADJP (JJ robust)) (CC and) (ADJP (JJ easy) (SBAR (S (VP (TO to) (VP (VB use))))))))))))) (. .))
