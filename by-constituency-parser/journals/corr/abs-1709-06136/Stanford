(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (NP (PRP we)) (VP (VBP present) (NP (NP (DT a) (NML (NML (JJ deep) (NN reinforcement) (NN learning)) (-LRB- -LRB-) (NML (NN RL)) (-RRB- -RRB-)) (NN framework)) (PP (IN for) (NP (JJ iterative) (NN dialog) (NN policy) (NN optimization)))) (PP (IN in) (NP (ADJP (NP (NML (NML (NN end)) (HYPH -) (PP (IN to) (HYPH -) (NP (NN end)))) (NN task)) (HYPH -) (VBN oriented)) (NN dialog) (NNS systems)))) (. .))
(S (NP (NP (JJ Popular) (NNS approaches)) (PP (IN in) (S (VP (VBG learning) (NP (NN dialog) (NN policy)) (PP (IN with) (NP (NN RL))))))) (VP (VBP include) (S (VP (VBG letting) (NP (DT a) (NN dialog) (NN agent)) (S (VP (TO to) (VP (VB learn) (PP (IN against) (NP (DT a) (NN user) (NN simulator))))))))) (. .))
(S (S (VP (VBG Building) (NP (DT a) (JJ reliable) (NN user) (NN simulator)))) (, ,) (ADVP (RB however)) (, ,) (VP (VBZ is) (RB not) (ADJP (JJ trivial)) (, ,) (PP (ADVP (RB often)) (IN as) (NP (ADJP (JJ difficult) (PP (IN as) (S (VP (VBG building) (NP (DT a) (JJ good) (NN dialog)))))) (NN agent)))) (. .))
(S (NP (PRP We)) (VP (VBP address) (NP (DT this) (NN challenge)) (PP (IN by) (S (ADVP (RB jointly)) (VP (VBG optimizing) (NP (NP (DT the) (NN dialog) (NN agent)) (CC and) (NP (DT the) (NN user) (NN simulator))) (PP (IN with) (NP (JJ deep) (NN RL))) (PP (IN by) (S (VP (VBG simulating) (NP (NP (NNS dialogs)) (PP (IN between) (NP (DT the) (CD two) (NNS agents))))))))))) (. .))
(FRAG (NP (PRP We)) (NP (JJ first) (NN bootstrap)) (NP (NP (DT a) (JJ basic) (NN dialog) (NN agent)) (CC and) (NP (NP (DT a) (JJ basic) (NN user) (NN simulator)) (PP (IN by) (S (VP (VBG learning) (ADVP (RB directly)) (PP (IN from) (NP (NP (NN dialog) (NNS corpora)) (PP (IN with) (NP (JJ supervised) (NN training)))))))))) (. .))
(S (NP (PRP We)) (ADVP (RB then)) (VP (VB improve) (NP (PRP them)) (ADVP (RB further)) (PP (IN by) (S (VP (VP (VBG letting) (NP (DT the) (CD two) (NNS agents)) (S (VP (TO to) (VP (VB conduct) (NP (ADJP (NN task) (HYPH -) (VBN oriented)) (NNS dialogs)))))) (CC and) (ADVP (RB iteratively)) (VP (VBG optimizing) (NP (PRP$ their) (NNS policies)) (PP (IN with) (NP (JJ deep) (NN RL)))))))) (. .))
(S (NP (CC Both) (NP (DT the) (NN dialog) (NN agent)) (CC and) (NP (DT the) (NN user) (NN simulator))) (VP (VBP are) (VP (VBN designed) (PP (IN with) (NP (NP (JJ neural) (NN network) (NNS models)) (SBAR (WHNP (WDT that)) (S (VP (MD can) (VP (VB be) (NP (VBN trained) (NML (NML (NN end)) (HYPH -) (PP (IN to) (HYPH -) (NP (NN end))))))))))))) (. .))
(S (NP (PRP$ Our) (NN experiment) (NNS results)) (VP (VBP show) (SBAR (IN that) (S (NP (DT the) (JJ proposed) (NN method)) (VP (VBZ leads) (PP (IN to) (NP (NP (NP (JJ promising) (NNS improvements)) (PP (IN on) (NP (NN task) (NN success) (NN rate)))) (CC and) (NP (NP (JJ total) (NN task) (NN reward)) (VP (VBG comparing) (PP (IN to) (NP (NP (JJ supervised) (NN training)) (CC and) (NP (NML (JJ single) (HYPH -) (NN agent)) (NN RL) (NN training) (NN baseline) (NNS models)))))))))))) (. .))
