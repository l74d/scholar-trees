(S (NP (NP (JJ Much)) (PP (IN of) (NP (NP (DT the) (NN success)) (PP (IN of) (NP (NP (NML (JJ single) (NN agent)) (JJ deep) (NN reinforcement) (NN learning) (PRN (-LRB- -LRB-) (NP (NN DRL)) (-RRB- -RRB-))) (PP (IN in) (NP (JJ recent) (NNS years)))))))) (VP (MD can) (VP (VB be) (VP (VBN attributed) (PP (IN to) (NP (NP (DT the) (NN use)) (PP (IN of) (NP (NP (NN experience) (NN replay) (NNS memories) (PRN (-LRB- -LRB-) (NP (NN ERM)) (-RRB- -RRB-))) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBP allow) (NP (NP (NNP Deep) (NNP Q) (HYPH -) (NNP Networks)) (-LRB- -LRB-) (NP (NNS DQNs)) (-RRB- -RRB-)) (S (VP (TO to) (VP (VB be) (VP (VBN trained) (ADVP (RB efficiently)) (PP (IN through) (NP (NP (NN sampling)) (VP (VBN stored) (NP (NN state) (NNS transitions))))))))))))))))))) (. .))
(S (ADVP (RB However)) (, ,) (NP (NN care)) (VP (VBZ is) (VP (VBN required) (SBAR (WHADVP (WRB when)) (S (VP (VBG using) (NP (NNS ERMs)) (PP (IN for) (NP (JJ multi-agent) (JJ deep) (NN reinforcement) (NN learning)))))) (PRN (-LRB- -LRB-) (NP (NN MA) (HYPH -) (NN DRL)) (-RRB- -RRB-)) (, ,) (SBAR (IN as) (S (S (VP (VBN stored) (NP (NNS transitions)))) (VP (MD can) (VP (VB become) (ADJP (JJ outdated)) (SBAR (IN because) (S (NP (NNS agents)) (VP (VBP update) (NP (PRP$ their) (NNS policies)) (PP (IN in) (NP (NN parallel)))))))))) (PRN (-LRB- [) (NP (CD 11)) (-RRB- ])))) (. .))
(S (PP (IN In) (NP (DT this) (NN work))) (NP (PRP we)) (VP (VBP apply) (NP (NP (NN leniency) (PRN (-LRB- [) (NP (CD 23)) (-RRB- ]))) (PP (IN to) (NP (NN MA) (HYPH -) (NN DRL))))) (. .))
(S (NP (JJ Lenient) (NNS agents)) (VP (VBP map) (NP (NML (NN state) (HYPH -) (NN action)) (NNS pairs)) (PP (IN to) (NP (NP (JJ decaying) (NN temperature) (NNS values)) (SBAR (WHNP (WDT that)) (S (VP (VBP control) (SBAR (S (NP (NP (DT the) (NN amount)) (PP (IN of) (NP (NN leniency)))) (VP (VBD applied) (PP (IN towards) (NP (NP (JJ negative) (NN policy) (NNS updates)) (SBAR (WHNP (WDT that)) (S (VP (VBP are) (VP (VBN sampled) (PP (IN from) (NP (DT the) (NNP ERM)))))))))))))))))) (. .))
(S (NP (DT This)) (VP (VP (VBZ introduces) (NP (NN optimism)) (PP (IN in) (NP (DT the) (NML (NN value) (HYPH -) (NN function)) (NN update)))) (, ,) (CC and) (VP (VBZ has) (VP (VBN been) (VP (VBN shown) (S (VP (TO to) (VP (VB facilitate) (NP (NN cooperation)) (PP (IN in) (NP (JJ tabular) (ADJP (RB fully) (HYPH -) (JJ cooperative)) (JJ multi-agent) (NML (NN reinforcement) (NN learning)) (NNS problems)))))))))) (. .))
(S (S (NP (PRP We)) (VP (VBP evaluate) (NP (NP (PRP$ our) (JJ Lenient) (HYPH -) (NN DQN)) (-LRB- -LRB-) (NP (NN LDQN)) (-RRB- -RRB-)) (ADVP (RB empirically)) (PP (IN against) (NP (DT the) (NML (NML (ADJP (JJ related) (JJ Hysteretic)) (HYPH -) (NN DQN)) (-LRB- -LRB-) (NML (NN HDQN)) (-RRB- -RRB-)) (NN algorithm))) (PRN (-LRB- [) (NP (CD 22)) (-RRB- ])))) (CONJP (RB as) (RB well) (IN as)) (FRAG (NP (NP (DT a) (VBN modified) (NN version)) (SBAR (S (NP (PRP we)) (VP (VBP call) (SBAR (S (NP (NP (VBN scheduled) (HYPH -) (NN HDQN)) (, ,) (NP (DT that))) (VP (VBZ uses) (NP (NP (JJ average) (NN reward) (NN learning)) (PP (IN near) (NP (JJ terminal) (NNS states)))))))))))) (. .))
(S (NP (NNS Evaluations)) (VP (VBP take) (NP (NN place)) (PP (IN in) (NP (NP (JJ extended) (NNS variations)) (PP (IN of) (NP (NP (NP (DT the) (VBN Coordinated) (JJ Multi-Agent) (NN Object) (NN Transportation) (NN Problem)) (-LRB- -LRB-) (NP (NN CMOTP)) (-RRB- -RRB-) (PRN (-LRB- [) (NP (CD 8)) (-RRB- ]))) (SBAR (WHNP (WDT which)) (S (VP (VBP include) (NP (NP (ADJP (RB fully) (HYPH -) (JJ cooperative)) (NNS sub-tasks)) (CC and) (NP (JJ stochastic) (NNS rewards))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP find) (SBAR (IN that) (S (NP (NNP LDQN) (NNS agents)) (VP (VBP are) (ADJP (RBR more) (JJ likely) (S (VP (TO to) (VP (VB converge) (PP (IN to) (NP (NP (DT the) (JJ optimal) (NN policy)) (PP (IN in) (NP (DT a) (JJ stochastic) (NN reward) (NN CMOTP))))) (PP (VBN compared) (PP (IN to) (NP (NP (JJ standard)) (CC and) (NP (ADJP (NP (JJ scheduled)) (HYPH -) (JJ HDQN)) (NNS agents))))))))))))) (. .))
