(S (PP (IN As) (NP (NP (DT an) (ADJP (JJ adaptive) (, ,) (JJ interpretable) (, ,) (JJ robust) (, ,) (CC and) (JJ accurate)) (NN meta-algorithm)) (PP (IN for) (NP (JJ arbitrary) (JJ differentiable) (NN loss) (NNS functions))))) (, ,) (NP (NN gradient) (NN tree) (NN boosting)) (VP (VBZ is) (NP (NP (CD one)) (PP (IN of) (NP (DT the) (ADJP (RBS most) (JJ popular)) (NN machine) (VBG learning) (NNS techniques)))) (, ,) (SBAR (IN though) (S (NP (DT the) (JJ computational) (NN expensiveness)) (VP (ADVP (RB severely)) (VBZ limits) (NP (PRP$ its) (NN usage)))))) (. .))
(S (S (NP (JJ Stochastic) (NN gradient) (NN boosting)) (VP (MD could) (VP (VB be) (VP (VBN adopted) (S (VP (TO to) (VP (VB accelerates) (NP (JJ gradient) (NN boosting)) (PP (IN by) (S (VP (ADVP (JJ uniformly)) (VBG sampling) (NP (NN training) (NNS instances)))))))))))) (, ,) (CC but) (S (NP (PRP$ its) (NN estimator)) (VP (MD could) (VP (VB introduce) (NP (DT a) (JJ high) (NN variance))))) (. .))
(S (NP (DT This) (NN situation)) (VP (VBZ arises) (NP (NN motivation) (SBAR (IN for) (S (NP (PRP us)) (VP (TO to) (VP (VB optimize) (NP (JJ gradient) (NN tree) (NN boosting)))))))) (. .))
(S (NP (PRP We)) (VP (VBP combine) (NP (JJ gradient) (NN tree) (VBG boosting)) (PP (IN with) (NP (NP (NN importance) (NN sampling)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ achieves) (NP (JJR better) (NN performance)) (PP (IN by) (S (VP (VBG reducing) (NP (DT the) (JJ stochastic) (NN variance))))))))))) (. .))
(S (ADVP (RB Furthermore)) (, ,) (NP (PRP we)) (VP (VBP use) (NP (DT a) (NN regularizer)) (S (VP (TO to) (VP (VB improve) (NP (NP (DT the) (JJ diagonal) (NN approximation)) (PP (IN in) (NP (NP (DT the) (NNP Newton) (NN step)) (PP (IN of) (NP (NN gradient) (NN boosting)))))))))) (. .))
(S (NP (DT The) (JJ theoretical) (NN analysis)) (VP (VBZ supports) (SBAR (IN that) (S (NP (PRP$ our) (NNS strategies)) (VP (VBP achieve) (NP (NP (DT a) (JJ linear) (NN convergence) (NN rate)) (PP (IN on) (NP (JJ logistic) (NN loss)))))))) (. .))
(S (NP (JJ Empirical) (NNS results)) (VP (VBP show) (SBAR (IN that) (S (NP (PRP$ our) (NN algorithm)) (VP (VBZ achieves) (NP (DT a) (ADJP (QP (CD 2.5x) (: â€”) (CD 18x))) (NN acceleration)) (PP (IN on) (NP (NP (CD two) (JJ different) (NN gradient) (VBG boosting) (NN algorithms)) (PRN (-LRB- -LRB-) (NP (NP (NNP LogitBoost)) (CC and) (NP (NNP LambdaMART))) (-RRB- -RRB-)))) (PP (IN without) (NP (JJ appreciable) (NN performance) (NN loss))))))) (. .))
