(S (SBAR (IN In) (NN order) (S (VP (TO to) (VP (VB extract) (NP (DT the) (ADJP (JJS best) (JJ possible)) (NN performance)) (PP (IN from) (NP (JJ asynchronous) (JJ stochastic) (NN gradient) (NN descent))))))) (NP (CD one)) (VP (MD must) (VP (VP (VB increase) (NP (DT the) (JJ mini-batch) (NN size))) (CC and) (VP (VB scale) (NP (DT the) (NN learning) (NN rate)) (ADVP (RB accordingly))))) (. .))
(S (SBAR (IN In) (NN order) (S (VP (TO to) (VP (VB achieve) (NP (JJ further) (NN speedup)))))) (NP (PRP we)) (VP (VBP introduce) (NP (NP (DT a) (NN technique)) (SBAR (WHNP (WDT that)) (S (VP (VBZ delays) (NP (NN gradient) (NNS updates)) (S (VP (ADVP (RB effectively)) (VBG increasing) (NP (DT the) (NN mini-batch) (NN size))))))))) (. .))
(S (ADVP (RB Unfortunately)) (PP (IN with) (NP (NP (DT the) (NN increase)) (PP (IN of) (NP (JJ mini-batch) (NN size))))) (NP (PRP we)) (VP (VBP worsen) (NP (NP (DT the) (JJ stale) (NN gradient) (NN problem)) (PP (IN in) (NP (NP (JJ asynchronous) (JJ stochastic) (NN gradient) (NN descent)) (PRN (-LRB- -LRB-) (NP (NNP SGD)) (-RRB- -RRB-)))) (SBAR (WHNP (WDT which)) (S (VP (VBZ makes) (S (NP (DT the) (NN model) (NN convergence)) (ADJP (JJ poor)))))))) (. .))
(S (S (NP (PRP We)) (VP (VBP introduce) (NP (NP (JJ local) (NNS optimizers)) (SBAR (WHNP (WDT which)) (S (VP (VBP mitigate) (NP (DT the) (JJ stale) (NN gradient) (NN problem)))))))) (CC and) (S (ADVP (RB together) (PP (IN with) (S (VP (JJ fine) (VBG tuning) (NP (PRP$ our) (NN momentum)))))) (NP (PRP we)) (VP (VBP are) (ADJP (JJ able) (S (VP (TO to) (VP (VB train) (NP (DT a) (JJ shallow) (NN machine) (NN translation) (NN system)) (ADVP (ADVP (NP (CD 27) (NN %)) (JJR faster)) (PP (IN than) (NP (DT an) (JJ optimized) (NN baseline)))) (PP (IN with) (NP (NP (JJ negligible) (NN penalty)) (PP (IN in) (NP (NNP BLEU))))))))))) (. .))
