(S (NP (JJ Gradient-based) (NN hyperparameter) (NN optimization)) (VP (VBZ is) (NP (NP (DT an) (JJ attractive) (NN way)) (SBAR (S (VP (TO to) (VP (VP (VB perform) (NP (JJ meta-learning)) (PP (IN across) (NP (NP (DT a) (NN distribution)) (PP (IN of) (NP (NNS tasks)))))) (, ,) (CC or) (VP (VB improve) (NP (NP (DT the) (NN performance)) (PP (IN of) (NP (DT an) (NN optimizer))) (PP (IN on) (NP (DT a) (JJ single) (NN task))))))))))) (. .))
(S (ADVP (RB However)) (, ,) (NP (DT this) (NN approach)) (VP (VBZ has) (VP (VBN been) (ADJP (JJ unpopular)) (PP (IN for) (NP (NP (NNS tasks)) (VP (VBG requiring) (NP (NP (JJ long) (NNS horizons)) (PRN (-LRB- -LRB-) (NP (JJ many) (JJ gradient) (NNS steps)) (-RRB- -RRB-)))))) (, ,) (PP (JJ due) (TO to) (NP (NP (VB memory) (NN scaling)) (CC and) (NP (JJ gradient) (NN degradation) (NNS issues)))))) (. .))
(S (NP (DT A) (JJ common) (NN workaround)) (VP (VBZ is) (S (VP (TO to) (VP (VP (VB learn) (NP (NNS hyperparameters)) (ADVP (VBP online))) (CC or) (VP (VBP split) (NP (DT the) (NN horizon)) (PP (IN into) (NP (JJR smaller) (NNS chunks)))))))) (. .))
(S (ADVP (RB However)) (, ,) (NP (DT this)) (VP (NNS introduces) (NP (NP (NN greediness)) (SBAR (WHNP (WDT which)) (S (VP (VBZ comes) (PP (IN with) (NP (DT a) (JJ large) (NN performance) (NN drop))) (, ,) (SBAR (IN since) (S (NP (DT the) (JJS best) (JJ local) (NNS hyperparameters)) (VP (MD can) (VP (VB make) (PP (IN for) (NP (JJ poor) (JJ global) (NNS solutions)))))))))))) (. .))
(S (PP (IN In) (NP (DT this) (NN work))) (, ,) (NP (PRP we)) (VP (VBP enable) (NP (NP (JJ non-greediness)) (PP (IN over) (NP (JJ long) (NNS horizons)))) (PP (IN with) (NP (DT a) (JJ two-fold) (NN solution)))) (. .))
(S (ADVP (RB First)) (, ,) (NP (PRP we)) (VP (VP (NN share) (NP (NP (NNS hyperparameters)) (SBAR (WHNP (WDT that)) (S (VP (VBP are) (ADJP (JJ contiguous)) (PP (IN in) (NP (NN time)))))))) (, ,) (CC and) (VP (VBP show) (SBAR (IN that) (S (NP (DT this)) (VP (ADVP (RB drastically)) (VBZ mitigates) (NP (JJ gradient) (NN degradation) (NNS issues))))))) (. .))
(S (ADVP (RB Then)) (, ,) (NP (PRP we)) (VP (VBP derive) (NP (NP (DT a) (JJ forward-mode) (NN differentiation) (NN algorithm)) (PP (IN for) (NP (DT the) (JJ popular) (JJ momentum-based) (NNP SGD) (NN optimizer))) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ allows) (PP (IN for) (NP (NP (DT a) (NN memory) (NN cost)) (SBAR (WHNP (WDT that)) (S (VP (VBZ is) (ADJP (JJ constant) (PP (IN with) (NP (NN horizon) (NN size)))))))))))))) (. .))
(S (SBAR (WHADVP (WRB When)) (S (VP (VB put) (ADVP (RB together))))) (, ,) (NP (DT these) (NNS solutions)) (VP (VBP allow) (S (NP (PRP us)) (VP (TO to) (VP (VB learn) (NP (NNS hyperparameters)) (PP (IN without) (NP (DT any) (JJ prior) (NN knowledge))))))) (. .))
(S (PP (VBN Compared) (PP (TO to) (NP (NP (DT the) (NN baseline)) (PP (IN of) (NP (JJ hand-tuned) (JJ off-the-shelf) (NNS hyperparameters)))))) (, ,) (NP (PRP$ our) (NN method)) (VP (VBZ compares) (ADVP (RB favorably)) (PP (IN on) (NP (NP (JJ simple) (NNS datasets)) (PP (IN like) (NP (NNP SVHN)))))) (. .))
(S (PP (IN On) (NP (NNP CIFAR-10))) (NP (PRP we)) (VP (VP (VBP match) (NP (DT the) (NN baseline) (NN performance))) (, ,) (CC and) (VP (NN demonstrate) (PP (IN for) (NP (DT the) (JJ first) (NN time))) (SBAR (IN that) (S (S (VP (VBG learning) (NP (NN rate) (, ,) (NN momentum) (CC and) (NN weight) (NN decay) (NNS schedules)))) (VP (MD can) (VP (VB be) (VP (VBN learned) (PP (IN with) (NP (NNS gradients))) (PP (IN on) (NP (NP (DT a) (NN dataset)) (PP (IN of) (NP (DT this) (NN size)))))))))))) (. .))
(S (NP (NNP Code)) (VP (VBZ is) (ADJP (JJ available)) (PP (IN at) (NP (DT this) (NN https) (NNP URL)))))
