(S (S (PP (IN In) (NP (NP (ADJP (NN data) (HYPH -) (JJ parallel)) (JJ synchronous) (NN training)) (PP (IN of) (NP (JJ deep) (JJ neural) (NNS networks))))) (, ,) (NP (NP (JJ different) (NNS devices)) (-LRB- -LRB-) (NP (NNS replicas)) (-RRB- -RRB-)) (VP (VBP run) (NP (DT the) (JJ same) (NN program)) (PP (IN with) (NP (NP (JJ different) (NNS partitions)) (PP (IN of) (NP (DT the) (NN training) (NN batch))))))) (, ,) (CC but) (S (NP (NN weight) (NN update) (NN computation)) (VP (VBZ is) (VP (VBN repeated) (PP (IN on) (NP (DT all) (NNS replicas))) (, ,) (SBAR (IN because) (S (NP (DT the) (NNS weights)) (VP (VBP do) (RB not) (VP (VB have) (NP (DT a) (NN batch) (NN dimension)) (PP (IN to) (NP (NN partition)))))))))) (. .))
(S (NP (DT This)) (VP (MD can) (VP (VB be) (NP (NP (NP (DT a) (NN bottleneck)) (PP (IN for) (NP (NP (NN performance) (CC and) (NN scalability)) (PP (IN in) (NP (NP (NP (JJ typical) (NN language) (NNS models)) (PP (IN with) (NP (JJ large) (NNS weights)))) (, ,) (CC and) (NP (NP (NNS models)) (PP (IN with) (NP (JJ small) (NML (PP (IN per) (HYPH -) (NP (NN replica) (NN batch)))) (NN size))))))))) (SBAR (WHNP (WDT which)) (S (VP (VBZ is) (ADJP (JJ typical) (PP (IN in) (NP (NML (JJ large) (HYPH -) (NN scale)) (NN training)))))))))) (. .))
(S (NP (DT This) (NN paper)) (VP (VBZ presents) (NP (DT an) (NN approach)) (PP (IN to) (S (S (ADVP (RB automatically)) (VP (VB shard) (NP (DT the) (NN weight) (NN update) (NN computation)) (PP (IN across) (NP (NP (NNS replicas)) (PP (IN with) (NP (JJ efficient) (NN communication) (NNS primitives) (CC and) (NNS data))))))) (VP (VBG formatting) (, ,) (VBG using) (NP (NN static) (NN analysis) (CC and) (NNS transformations)) (PP (IN on) (NP (DT the) (NN training) (NN computation) (NN graph))))))) (. .))
(S (NP (PRP We)) (VP (VBP show) (SBAR (S (NP (DT this) (NN technique)) (VP (VBZ achieves) (NP (JJ substantial) (NNS speedups)) (PP (IN on) (NP (NP (JJ typical) (NML (NN image) (CC and) (NN language)) (NNS models)) (PP (IN on) (NP (NNP Cloud) (NNP TPUs))))) (, ,) (S (VP (VBG requiring) (NP (DT no) (NN change)) (PP (IN to) (NP (NN model) (NN code))))))))) (. .))
(S (NP (DT This) (NN technique)) (VP (VBZ helps) (VP (VB close) (NP (NP (DT the) (NN gap)) (PP (IN between) (NP (NP (ADJP (RB traditionally) (JJ expensive)) (-LRB- -LRB-) (NP (NN ADAM)) (-RRB- -RRB-)) (CC and) (NP (ADJP (JJ cheap) (PRN (-LRB- -LRB-) (NP (NNP SGD)) (-RRB- -RRB-))) (NNS optimizers))))) (, ,) (SBAR (IN as) (S (NP (PRP they)) (VP (VP (MD will) (ADVP (RB only)) (VP (VB take) (NP (NP (DT a) (JJ small) (NN part)) (PP (IN of) (NP (NN training) (NN step) (NN time)))))) (CC and) (VP (VBP have) (NP (JJ similar) (NML (NN peak) (NN memory)) (NN usage)))))))) (. .))
(S (NP (PRP It)) (VP (VBD helped) (NP (PRP us)) (S (VP (TO to) (VP (VB achieve) (NP (NP (NML (NML (NN state)) (HYPH -) (PP (IN of) (HYPH -) (NP (DT the) (HYPH -) (NN art)))) (NN training) (NN performance)) (PP (IN in) (NP (NP (NNP Google) (POS 's)) (NML (NNP MLPerf) (CD 0.6)) (NN submission)))))))) (. .))
