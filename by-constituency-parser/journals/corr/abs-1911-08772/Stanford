(S (S (VP (VBN Distributed) (NP (NP (JJ stochastic) (NN gradient) (NN descent)) (-LRB- -LRB-) (NP (NNP SGD)) (-RRB- -RRB-)))) (NP (NNS algorithms)) (VP (VBP are) (ADVP (RB widely)) (VP (VBN deployed) (PP (IN in) (S (VP (VBG training) (NP (NML (JJ large) (HYPH -) (NN scale)) (NML (JJ deep) (NN learning)) (NNS models))))) (, ,) (SBAR (IN while) (S (NP (NP (DT the) (NN communication) (NN overhead)) (PP (IN among) (NP (NNS workers)))) (VP (VBZ becomes) (NP (DT the) (JJ new) (NN system) (NN bottleneck))))))) (. .))
(S (ADVP (RB Recently)) (NP (NP (NP (VBN proposed) (NML (NN gradient) (NN sparsification)) (NNS techniques)) (, ,) (ADJP (RB especially) (JJ Top))) (: -) (NP (NP ($ $) (CD k)) (NP (NP ($ $) (CD sparsification)) (PP (IN with) (NP (NN error) (NN compensation)))) (PRN (-LRB- -LRB-) (NP (NN TopK) (HYPH -) (NN SGD)) (-RRB- -RRB-))) (, ,)) (VP (MD can) (ADVP (RB significantly)) (VP (VB reduce) (NP (DT the) (NN communication) (NN traffic)) (PP (IN without) (NP (NP (DT an) (JJ obvious) (NN impact)) (PP (IN on) (NP (DT the) (NN model) (NN accuracy))))))) (. .))
(S (NP (DT Some) (JJ theoretical) (NNS studies)) (VP (VBP have) (VP (VBN been) (VP (VBN carried) (PRT (RP out)) (S (VP (TO to) (VP (VB analyze) (NP (NP (DT the) (NN convergence) (NN property)) (PP (IN of) (NP (NN TopK) (HYPH -) (NN SGD)))))))))) (. .))
(S (ADVP (RB However)) (, ,) (S (NP (VBG existing) (NNS studies)) (VP (VBP do) (RB not) (VP (VP (VB dive) (PP (IN into) (NP (NP (DT the) (NNS details)) (PP (IN of) (NP (ADJP (JJ Top) (HYPH -) (NP ($ $) (CD k))) (NML (NML ($ $)) (NN operator)))))) (PP (IN in) (NP (NN gradient) (NN sparsification)))) (CC and) (VP (VB use) (NP (NP (NP (JJ relaxed) (NNS bounds)) (-LRB- -LRB-) (NP (ADVP (FW e.g.)) (, ,) (NP (NP (JJ exact)) (VP (VBN bound) (PP (IN of) (NP (NNP Random))))) (: -) (NP (NP ($ $) (CD k)) (NP ($ $)))) (-RRB- -RRB-)) (PP (IN for) (NP (NN analysis)))))))) (: ;) (S (NP (RB hence) (DT the) (VBN derived) (NNS results)) (VP (MD can) (RB not) (ADVP (RB well)) (VP (VB describe) (NP (NP (DT the) (JJ real) (NN convergence) (NN performance)) (PP (IN of) (NP (NN TopK) (HYPH -) (NN SGD))))))) (. .))
(S (PP (IN To) (NP (DT this) (NN end))) (, ,) (NP (PRP we)) (ADVP (RB first)) (VP (VB study) (NP (NP (DT the) (NN gradient) (NNS distributions)) (PP (IN of) (NP (NN TopK) (HYPH -) (NN SGD)))) (PP (IN during) (NP (DT the) (NN training) (NN process))) (PP (IN through) (NP (JJ extensive) (NNS experiments)))) (. .))
(FRAG (S (NP (PRP We)) (ADVP (RB then)) (VP (ADVP (RB theoretically)) (VBP derive) (NP (NP (DT a) (JJR tighter)) (VP (VBN bound) (PP (IN for) (NP (DT the) (JJ Top))))))) (: -) (FRAG (NP ($ $) (CD k)) (NP ($ $)) (NP (NN operator))) (. .))
(S (ADVP (RB Finally)) (, ,) (NP (PRP we)) (VP (VBP exploit) (NP (NP (DT the) (NN property)) (PP (IN of) (NP (NN gradient) (NN distribution)))) (S (VP (TO to) (VP (VB propose) (NP (NP (DT an) (JJ approximate) (NN top)) (: -) (NP (NP (NP (QP ($ $) (CD k)) (NML (NML ($ $)) (NN selection)) (NN algorithm)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ is) (VP (VBG computing)))))) (HYPH -) (NP (ADJP (JJ efficient) (PP (IN for))) (NNS GPUs))))))) (, ,) (S (VP (TO to) (VP (VB improve) (NP (NP (DT the) (NN scaling) (NN efficiency)) (PP (IN of) (NP (NN TopK) (HYPH -) (NN SGD)))) (PP (IN by) (S (ADVP (RB significantly)) (VP (VBG reducing) (NP (DT the) (NN computing) (NN overhead))))))))) (. .))
