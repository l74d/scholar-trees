(S (NP (DT This) (NN paper)) (VP (VBZ proposes) (NP (NP (DT a) (NN framework)) (PP (IN for) (NP (NP (VBN distributed) (, ,) (JJ in-storage) (NN training)) (PP (IN of) (NP (JJ neural) (NNS networks))) (PP (IN on) (NP (NP (NNS clusters)) (PP (IN of) (NP (JJ computational) (NN storage) (NNS devices))))))))) (. .))
(S (NP (JJ Such) (NNS devices)) (VP (CONJP (RB not) (RB only)) (VP (VB contain) (NP (JJ hardware) (NNS accelerators))) (CONJP (CC but) (RB also)) (VP (VB eliminate) (NP (NP (NNS data) (NN movement)) (PP (IN between) (NP (DT the) (NN host) (CC and) (NN storage)))) (, ,) (S (VP (VBG resulting) (PP (IN in) (NP (DT both) (NP (JJ improved) (NN performance)) (CC and) (NP (NN power) (NNS savings)))))))) (. .))
(S (ADVP (RBR More) (RB importantly)) (, ,) (NP (NP (DT this) (JJ in-storage) (NN processing) (NN style)) (PP (IN of) (NP (NN training)))) (VP (NNS ensures) (SBAR (WDT that) (S (NP (JJ private) (NNS data)) (ADVP (RB never)) (VP (VBZ leaves) (NP (DT the) (NN storage))))) (SBAR (IN while) (S (VP (ADVP (RB fully)) (VBG controlling) (NP (NP (DT the) (NN sharing)) (PP (IN of) (NP (JJ public) (NNS data)))))))) (. .))
(S (NP (JJ Experimental) (NNS results)) (VP (VBP show) (NP (NP (QP (RB up) (TO to) (CD 2.7x)) (NN speedup)) (CC and) (NP (NP (ADJP (CD 69) (NN %)) (NN reduction)) (PP (IN in) (NP (NN energy) (NN consumption)))) (CC and) (NP (NP (DT no) (JJ significant) (NN loss)) (PP (IN in) (NP (NN accuracy)))))) (. .))
