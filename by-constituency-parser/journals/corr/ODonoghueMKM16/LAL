(S (NP (NN Policy) (NN gradient)) (VP (VBZ is) (NP (NP (DT an) (JJ efficient) (NN technique)) (PP (IN for) (S (VP (VBG improving) (NP (DT a) (NN policy)) (PP (IN in) (NP (DT a) (NN reinforcement) (VBG learning) (NN setting)))))))) (. .))
(S (ADVP (RB However)) (, ,) (NP (FW vanilla) (JJ online) (NNS variants)) (VP (VBP are) (ADJP (ADJP (JJ on-policy) (RB only)) (CC and) (ADJP (RB not) (JJ able) (S (VP (TO to) (VP (VB take) (NP (NN advantage)) (PP (IN of) (NP (NN off-policy) (NNS data))))))))) (. .))
(S (PP (IN In) (NP (DT this) (NN paper))) (NP (PRP we)) (VP (VBP describe) (NP (NP (DT a) (JJ new) (NN technique)) (SBAR (WHNP (WDT that)) (S (VP (VBZ combines) (NP (NN policy) (NN gradient)) (PP (IN with) (NP (JJ off-policy) (NNP Q-learning))) (, ,) (S (VP (VBG drawing) (NP (NN experience)) (PP (IN from) (NP (DT a) (NN replay) (NN buffer)))))))))) (. .))
(S (NP (DT This)) (VP (VBZ is) (VP (VBN motivated) (PP (IN by) (S (VP (VBG making) (NP (NP (DT a) (NN connection)) (PP (IN between) (NP (NP (NP (DT the) (JJ fixed) (NNS points)) (PP (IN of) (NP (DT the) (JJ regularized) (NN policy) (NN gradient) (NN algorithm)))) (CC and) (NP (DT the) (NNP Q-values)))))))))) (. .))
(S (NP (DT This) (NN connection)) (VP (VBZ allows) (S (NP (PRP us)) (VP (TO to) (VP (VB estimate) (NP (NP (DT the) (NNS Q-values)) (PP (IN from) (NP (NP (DT the) (NN action) (NNS preferences)) (PP (IN of) (NP (NP (DT the) (NN policy)) (, ,) (SBAR (WHPP (TO to) (WHNP (WDT which))) (S (NP (PRP we)) (VP (VBP apply) (NP (JJ Q-learning) (NNS updates)))))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP refer) (PP (TO to) (NP (DT the) (JJ new) (NN technique))) (PP (IN as) (NP (NNP 'PGQL) (POS â€º) (, ,) (PP (IN for) (NP (NP (NN policy) (NN gradient)) (CC and) (NP (NN Q-learning))))))) (. .))
(S (NP (PRP We)) (ADVP (RB also)) (VP (VB establish) (NP (NP (DT an) (NN equivalency)) (PP (IN between) (NP (NP (JJ action-value) (NN fitting) (NNS techniques)) (CC and) (NP (JJ actor-critic) (NN algorithms))))) (, ,) (S (VP (VBG showing) (SBAR (IN that) (S (NP (JJ regularized) (NN policy) (NN gradient) (NNS techniques)) (VP (MD can) (VP (VB be) (VP (VBN interpreted) (PP (IN as) (NP (NN advantage) (NN function) (VBG learning) (NNS algorithms))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP conclude) (PP (IN with) (NP (NP (DT some) (JJ numerical) (NNS examples)) (SBAR (WHNP (WDT that)) (S (VP (VBP demonstrate) (NP (NP (VBN improved) (NX (NX (NNS data) (NN efficiency)) (CC and) (NX (NN stability)))) (PP (IN of) (NP (NNP PGQL)))))))))) (. .))
(S (PP (IN In) (NP (JJ particular))) (, ,) (NP (PRP we)) (VP (VP (VBD tested) (NP (NNP PGQL)) (PP (IN on) (NP (NP (DT the) (JJ full) (NN suite)) (PP (IN of) (NP (NNP Atari) (NNS games)))))) (CC and) (VP (VBD achieved) (NP (NP (NN performance)) (VP (VBG exceeding) (NP (NP (IN that)) (PP (IN of) (NP (DT both) (NP (NP (JJ asynchronous) (NN advantage) (JJ actor-critic)) (PRN (-LRB- -LRB-) (NP (NNP A3C)) (-RRB- -RRB-))) (CC and) (NP (NNP Q-learning))))))))) (. .))
