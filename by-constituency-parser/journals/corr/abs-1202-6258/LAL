(S (NP (PRP We)) (VP (VBP propose) (NP (NP (DT a) (JJ new) (JJ stochastic) (NN gradient) (NN method)) (PP (IN for) (S (VP (VBG optimizing) (NP (NP (DT the) (NN sum)) (PP (IN of) (NP (NP (DT a) (JJ finite) (NN set)) (PP (IN of) (NP (JJ smooth) (NNS functions))) (, ,) (SBAR (WHADVP (WRB where)) (S (NP (DT the) (NN sum)) (VP (VBZ is) (ADJP (RB strongly) (JJ convex))))))))))))) (. .))
(S (SBAR (IN While) (S (NP (JJ standard) (JJ stochastic) (NN gradient) (NNS methods)) (VP (NN converge) (PP (IN at) (NP (JJ sublinear) (NNS rates))) (PP (IN for) (NP (DT this) (NN problem)))))) (, ,) (NP (DT the) (VBN proposed) (NN method)) (VP (VBZ incorporates) (NP (NP (DT a) (NN memory)) (PP (IN of) (NP (JJ previous) (NN gradient) (NNS values)))) (SBAR (IN in) (NN order) (S (VP (TO to) (VP (VB achieve) (NP (DT a) (JJ linear) (NN convergence) (NN rate))))))) (. .))
(S (PP (IN In) (NP (DT a) (NN machine) (VBG learning) (NN context))) (, ,) (NP (JJ numerical) (NNS experiments)) (VP (VBP indicate) (SBAR (IN that) (S (NP (DT the) (JJ new) (NN algorithm)) (VP (MD can) (VP (ADVP (RB dramatically)) (VB outperform) (NP (JJ standard) (NN algorithms)) (, ,) (PP (DT both) (IN in) (NP (NP (NNS terms)) (PP (IN of) (S (VP (VP (VBG optimizing) (NP (DT the) (NN training) (NN error))) (CC and) (VP (VBG reducing) (NP (DT the) (NN test) (NN error)) (ADVP (RB quickly))))))))))))) (. .))
