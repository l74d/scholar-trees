(S (NP (NN Transformer) (NNS models)) (VP (VBP have) (VP (VBN been) (VP (VBN introduced) (PP (IN into) (NP (NML (NML (NN end)) (HYPH -) (PP (IN to) (HYPH -) (NP (NN end) (NN speech)))) (NN recognition))) (PP (IN with) (NP (NML (NML (NN state)) (HYPH -) (PP (IN of) (HYPH -) (NP (DT the) (HYPH -) (NN art)))) (NN performance))) (PP (IN on) (NP (JJ various) (NNS tasks))) (PP (VBG owing) (PP (IN to) (NP (NP (PRP$ their) (NN superiority)) (PP (IN in) (NP (NML (NML (NN modeling)) (RB long) (HYPH -) (NN term)) (NNS dependencies))))))))) (. .))
(S (ADVP (RB However)) (, ,) (NP (JJ such) (NNS improvements)) (VP (VBP are) (ADVP (RB usually)) (VP (VBN obtained) (PP (IN through) (NP (NP (DT the) (NN use)) (PP (IN of) (NP (ADJP (RB very) (JJ large)) (JJ neural) (NNS networks))))))) (. .))
(S (NP (NN Transformer) (NNS models)) (ADVP (RB mainly)) (VP (VBP include) (NP (NP (ADJP (NP (CD two) (NNS submodules)) (HYPH -) (JJ position-wise)) (JJ feedforward) (NNS layers)) (CC and) (NP (NN self) (HYPH -) (NN attention) (PRN (-LRB- -LRB-) (NP (NNP SAN)) (-RRB- -RRB-))) (NP (NNS layers)))) (. .))
(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (S (VP (TO to) (VP (VB reduce) (NP (DT the) (NN model) (NN complexity)) (PP (IN while) (S (VP (VBG maintaining) (NP (JJ good) (NN performance)))))))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (NP (DT a) (VBN simplified) (NN self) (HYPH -) (NN attention)) (-LRB- -LRB-) (NP (NN SSAN)) (-RRB- -RRB-)) (NN layer) (SBAR (WHNP (WDT which)) (S (VP (VBZ employs) (NP (NP (NNP FSMN) (NN memory) (NN block)) (PP (RB instead) (IN of) (NP (NN projection) (NNS layers)))))))) (S (VP (TO to) (VP (VB form) (NP (NP (NN query) (CC and) (NN key) (NNS vectors)) (PP (IN for) (NP (ADJP (NN transformer) (HYPH -) (VBN based)) (NML (NML (NN end)) (HYPH -) (PP (IN to) (HYPH -) (NP (NN end) (NN speech)))) (NN recognition)))))))) (. .))
(S (NP (PRP We)) (VP (VBP evaluate) (NP (NP (ADJP (ADJP (NP (DT the) (NN SSAN)) (HYPH -) (VBN based)) (CC and) (ADJP (NP (DT the) (JJ conventional) (NN SAN)) (HYPH -) (VBN based))) (NNS transformers)) (PP (IN on) (NP (NP (DT the) (JJ public) (NML (NN AISHELL) (HYPH -) (CD 1))) (, ,) (NP (JJ internal) (NML (CD 1000) (HYPH -) (NN hour))) (CC and) (NP (NML (CD 20,000) (HYPH -) (NN hour)) (NML (JJ large) (HYPH -) (NN scale)) (NNP Mandarin) (NNS tasks)))))) (. .))
(S (NP (NNS Results)) (VP (VBP show) (SBAR (IN that) (S (NP (PRP$ our) (ADJP (ADJP (VBN proposed)) (ADJP (NP (NN SSAN)) (HYPH -) (VBN based))) (NN transformer) (NN model)) (VP (MD can) (VP (VB achieve) (PP (IN over) (NP (NML (CD 20) (NN %)) (JJ relative) (NN reduction))) (PP (IN in) (NP (NP (NN model) (NNS parameters)) (CC and) (NP (NP (NML (CD 6.7) (NN %)) (JJ relative) (NN CER) (NN reduction)) (PP (IN on) (NP (DT the) (NML (NNP AISHELL) (HYPH -) (NNP 1)) (NN task))))))))))) (. .))
(S (PP (IN With) (NP (ADVP (RB impressively) (NP (CD 20) (NN %))) (NN parameter) (NN reduction))) (, ,) (NP (PRP$ our) (NN model)) (VP (VBZ shows) (NP (NP (DT no) (NN loss)) (PP (IN of) (NP (NN recognition) (NN performance)))) (PP (IN on) (NP (DT the) (NML (CD 20,000) (HYPH -) (NN hour)) (NML (JJ large) (HYPH -) (NN scale)) (NN task)))) (. .))
