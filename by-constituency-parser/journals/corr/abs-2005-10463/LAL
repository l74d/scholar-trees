(S (NP (NNP Transformer) (NNS models)) (VP (VBP have) (VP (VBN been) (VP (VBN introduced) (PP (IN into) (NP (JJ end-to-end) (NN speech) (NN recognition))) (PP (IN with) (NP (NP (JJ state-of-the-art) (NN performance)) (PP (IN on) (NP (JJ various) (NNS tasks))))) (PP (VBG owing) (PP (TO to) (NP (NP (PRP$ their) (NN superiority)) (PP (IN in) (S (VP (VBG modeling) (NP (JJ long-term) (NNS dependencies))))))))))) (. .))
(S (ADVP (RB However)) (, ,) (NP (JJ such) (NNS improvements)) (VP (VBP are) (ADVP (RB usually)) (VP (VBN obtained) (PP (IN through) (NP (NP (DT the) (NN use)) (PP (IN of) (NP (ADJP (RB very) (JJ large)) (JJ neural) (NNS networks))))))) (. .))
(S (NP (NNP Transformer) (NNS models)) (ADVP (RB mainly)) (VP (VBP include) (NP (NP (CD two) (NNS submodules)) (: -) (NP (NP (NN position-wise) (NN feedforward) (NNS layers)) (CC and) (NP (NN self-attention) (PRN (-LRB- -LRB-) (NNP SAN) (-RRB- -RRB-)) (NNS layers))))) (. .))
(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (S (VP (TO to) (VP (VB reduce) (NP (DT the) (NN model) (NN complexity)) (SBAR (IN while) (S (VP (VBG maintaining) (NP (JJ good) (NN performance)))))))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (DT a) (JJ simplified) (NN self-attention) (PRN (-LRB- -LRB-) (NNP SSAN) (-RRB- -RRB-)) (NN layer)) (SBAR (WHNP (WDT which)) (S (VP (VBZ employs) (NP (NP (NNP FSMN) (NN memory) (NN block)) (PP (RB instead) (IN of) (NP (NN projection) (NNS layers)))) (S (VP (TO to) (VP (VB form) (NP (NP (NN query) (CC and) (JJ key) (NNS vectors)) (PP (IN for) (NP (JJ transformer-based) (JJ end-to-end) (NN speech) (NN recognition)))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP evaluate) (NP (NP (DT the) (JJ SSAN-based)) (CC and) (NP (NP (DT the) (JJ conventional) (JJ SAN-based)) (NNS transformers))) (PP (IN on) (NP (DT the) (JJ public) (NNP AISHELL-1) (, ,) (ADJP (JJ internal) (CD 1000-hour) (CC and) (JJ 20,000-hour)) (JJ large-scale) (NNP Mandarin) (NNS tasks)))) (. .))
(S (NP (NNS Results)) (VP (VBP show) (SBAR (IN that) (S (NP (PRP$ our) (VBN proposed) (JJ SSAN-based) (NN transformer) (NN model)) (VP (MD can) (VP (VB achieve) (NP (NP (NP (ADJP (QP (IN over) (CD 20)) (NN %)) (JJ relative) (NN reduction)) (PP (IN in) (NP (NN model) (NNS parameters)))) (CC and) (NP (ADJP (CD 6.7) (NN %)) (JJ relative) (NNP CER) (NN reduction))) (PP (IN on) (NP (DT the) (NNP AISHELL-1) (NN task)))))))) (. .))
(S (PP (IN With) (NP (ADJP (QP (RB impressively) (CD 20)) (NN %)) (NN parameter) (NN reduction))) (, ,) (NP (PRP$ our) (NN model)) (VP (VBZ shows) (NP (NP (DT no) (NN loss)) (PP (IN of) (NP (NP (NN recognition) (NN performance)) (PP (IN on) (NP (DT the) (JJ 20,000-hour) (JJ large-scale) (NN task))))))) (. .))
