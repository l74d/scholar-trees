(S (NP (PRP We)) (VP (VBP develop) (NP (NP (DT a) (ADJP (JJ scalable) (CC and) (JJ extendable)) (NN training) (NN framework)) (SBAR (WHNP (WDT that)) (S (VP (MD can) (VP (VP (VB utilize) (NP (NNS GPUs)) (PP (IN across) (NP (NP (NNS nodes)) (PP (IN in) (NP (DT a) (NN cluster)))))) (CC and) (VP (VB accelerate) (NP (NP (DT the) (NN training)) (PP (IN of) (NP (NML (JJ deep) (NN learning)) (NNS models)))) (PP (VBN based) (PP (IN on) (NP (NNS data) (NN parallelism))))))))))) (. .))
(S (NP (DT Both) (ADJP (JJ synchronous) (CC and) (JJ asynchronous)) (NN training)) (VP (VBP are) (VP (VBN implemented) (PP (IN in) (NP (PRP$ our) (NN framework))) (, ,) (SBAR (WHADVP (WRB where)) (S (NP (NP (NN parameter) (NN exchange)) (PP (IN among) (NP (NNS GPUs)))) (VP (VBZ is) (VP (VBN based) (PP (IN on) (NP (ADJP (NP (NNP CUDA)) (HYPH -) (JJ aware)) (NNP MPI))))))))) (. .))
(S (PP (IN In) (NP (DT this) (NN report))) (, ,) (NP (PRP we)) (VP (VBP analyze) (NP (NP (DT the) (NN convergence) (CC and) (NN capability)) (PP (IN of) (NP (DT the) (NN framework)))) (S (VP (TO to) (VP (VB reduce) (NP (NN training) (NN time)) (SBAR (WHADVP (WRB when)) (S (VP (VBG scaling) (NP (NP (DT the) (JJ synchronous) (NN training)) (PP (IN of) (NP (NNP AlexNet) (CC and) (NNP GoogLeNet)))) (PP (IN from) (NP (NP (CD 2) (NNS GPUs)) (TO to) (NP (CD 8) (NNS GPUs))))))))))) (. .))
(S (PP (IN In) (NP (NN addition))) (, ,) (NP (PRP we)) (VP (VBP explore) (NP (JJ novel) (NNS ways)) (S (VP (TO to) (VP (VB reduce) (NP (NP (DT the) (NN communication) (NN overhead)) (VP (VBN caused) (PP (IN by) (S (VP (VBG exchanging) (NP (NNS parameters))))))))))) (. .))
(S (ADVP (RB Finally)) (, ,) (NP (PRP we)) (VP (VBP release) (NP (NP (DT the) (NN framework)) (PP (IN as) (NP (JJ open) (HYPH -) (NN source)))) (PP (IN for) (NP (NP (JJ further) (NN research)) (PP (IN on) (NP (VBN distributed) (JJ deep) (NN learning)))))))
