(S (NP (NNS CNNs)) (VP (VBP outperform) (NP (JJ traditional) (NN machine)) (S (VP (VBG learning) (NP (NNS algorithms)) (PP (IN across) (NP (NP (DT a) (JJ wide) (NN range)) (PP (IN of) (NP (NNS applications)))))))) (. .))
(S (ADVP (RB However)) (, ,) (NP (PRP$ their) (JJ computational) (NN complexity)) (VP (VBZ makes) (S (NP (PRP it)) (ADJP (JJ necessary) (S (VP (TO to) (VP (VB design) (NP (JJ efficient) (NN hardware) (NNS accelerators)))))))) (. .))
(S (NP (JJS Most) (NNP CNN) (NNS accelerators)) (VP (VBP focus) (PP (IN on) (S (VP (VBG exploring) (NP (NP (NN dataflow) (NNS styles)) (SBAR (WHNP (WDT that)) (S (VP (VBP exploit) (NP (JJ computational) (NN parallelism)))))))))) (. .))
(S (ADVP (RB However)) (, ,) (NP (NP (JJ potential) (NN performance) (NN speedup)) (PP (IN from) (NP (NN sparsity)))) (VP (VBZ has) (RB not) (VP (VBN been) (ADVP (RB adequately)) (VP (VBN addressed)))) (. .))
(S (NP (NP (DT The) (NML (NN computation) (CC and) (NN memory)) (NN footprint)) (PP (IN of) (NP (NNS CNNs)))) (VP (MD can) (VP (VB be) (ADVP (RB significantly)) (VP (VBN reduced) (SBAR (IN if) (S (NP (NN sparsity)) (VP (VBZ is) (VP (VBN exploited) (PP (IN in) (NP (NN network) (NNS evaluations)))))))))) (. .))
(S (S (VP (TO To) (VP (VB take) (NP (NP (NN advantage)) (PP (IN of) (NP (NN sparsity))))))) (, ,) (NP (DT some) (NN accelerator) (NNS designs)) (VP (VB explore) (NP (NP (NP (NN sparsity)) (VP (VBG encoding))) (CC and) (NP (NP (NN evaluation)) (PP (IN on) (NP (NNP CNN) (NNS accelerators)))))) (. .))
(S (ADVP (RB However)) (, ,) (NP (NN sparsity) (NN encoding)) (VP (VBZ is) (ADVP (RB just)) (VP (VBN performed) (PP (PP (IN on) (NP (NN activation) (CC or) (NN weight))) (CONJP (CC and) (RB only)) (PP (IN in) (NP (NN inference)))))) (. .))
(S (NP (PRP It)) (VP (VBZ has) (VP (VBN been) (VP (VBN shown) (SBAR (IN that) (S (NP (NN activation) (CC and) (NN weight)) (ADVP (RB also)) (VP (VBP have) (NP (JJ high) (NN sparsity) (NNS levels)) (PP (IN during) (NP (NN training))))))))) (. .))
(S (ADVP (RB Hence)) (, ,) (NP (ADJP (NN sparsity) (HYPH -) (JJ aware)) (NN computation)) (VP (MD should) (ADVP (RB also)) (VP (VB be) (VP (VBN considered) (PP (IN in) (NP (NN training)))))) (. .))
(S (S (VP (TO To) (ADVP (RB further)) (VP (VB improve) (NP (NML (NN performance) (CC and) (NN energy)) (NN efficiency))))) (, ,) (NP (DT some) (NNS accelerators)) (VP (VBP evaluate) (NP (NNS CNNs)) (PP (IN with) (NP (JJ limited) (NN precision)))) (. .))
(S (ADVP (RB However)) (, ,) (NP (DT this)) (VP (VBZ is) (NP (ADJP (VBN limited) (PP (IN to) (NP (NP (DT the) (NN inference)) (PP (IN since) (NP (VBN reduced) (NN precision) (NNS sacrifices)))))) (NN network) (NN accuracy)) (SBAR (IN if) (S (VP (VBN used) (PP (IN in) (NP (NN training))))))) (. .))
(S (PP (IN In) (NP (NN addition))) (, ,) (NP (NNP CNN) (NN evaluation)) (VP (VBZ is) (ADVP (RB usually)) (ADJP (NN memory) (HYPH -) (JJ intensive)) (, ,) (PP (ADVP (RB especially)) (IN in) (NP (NN training)))) (. .))
(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (NNP SPRING)) (, ,) (NP (NP (DT a) (ADJP (NP (NN SParsity)) (HYPH -) (JJ aware)) (VBN Reduced) (NML (PP (HYPH -) (NP (NN precision)))) (JJ Monolithic) (NML (NML (NN 3D)) (NNP CNN)) (NN accelerator)) (PP (IN for) (NP (NN trainING) (CC and) (NN inference)))))) (. .))
(S (NP (NNP SPRING)) (VP (VBZ supports) (NP (NP (DT both) (NNP CNN) (NN training)) (CC and) (NP (NN inference)))) (. .))
(S (NP (PRP It)) (VP (VBZ uses) (NP (DT a) (JJ binary) (NN mask) (NN scheme)) (S (VP (TO to) (VP (VB encode) (NP (NNS sparsities)) (PP (IN in) (NP (NN activation) (CC and) (NN weight))))))) (. .))
(S (NP (PRP It)) (VP (VBZ uses) (NP (DT the) (JJ stochastic) (VBG rounding) (NN algorithm)) (S (VP (TO to) (VP (VB train) (NP (NNS CNNs)) (PP (IN with) (NP (NP (VBN reduced) (NN precision)) (PP (IN without) (NP (NN accuracy) (NN loss))))))))) (. .))
(S (S (VP (TO To) (VP (VB alleviate) (NP (DT the) (NN memory) (NN bottleneck)) (PP (PP (IN in) (NP (NNP CNN) (NN evaluation))) (, ,) (RB especially) (PP (IN in) (NP (NN training))))))) (, ,) (NP (NNP SPRING)) (VP (VBZ uses) (NP (DT an) (JJ efficient) (JJ monolithic) (NML (NN 3D) (NN NVM)) (NN interface)) (S (VP (TO to) (VP (VB increase) (NP (NN memory) (NN bandwidth)))))) (. .))
(S (PP (VBN Compared) (PP (IN to) (NP (NNP GTX) (CD 1080) (NNP Ti)))) (, ,) (NP (NNP SPRING)) (VP (VBZ achieves) (NP (NP (NP (CD 15.6) (NML (NML (NN X)) (, ,) (NML (CD 4.2) (NN X)) (CC and) (NML (CD 66.0) (NN X))) (NNS improvements)) (PP (IN in) (NP (NML (NML (NN performance)) (, ,) (NML (NN power) (NN reduction)) (, ,) (CC and) (NML (NN energy))) (NN efficiency))) (, ,) (ADVP (RB respectively)) (, ,) (PP (IN for) (NP (NNP CNN) (NN training)))) (, ,) (CC and) (NP (NP (NML (NML (CD 15.5) (NN X)) (, ,) (NML (CD 4.5) (NN X)) (CC and) (NML (CD 69.1) (NN X))) (NNS improvements)) (PP (IN for) (NP (NN inference)))))) (. .))
