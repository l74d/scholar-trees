(S (NP (PRP We)) (VP (VBP consider) (NP (NP (JJ off-policy) (NN temporal-difference) (PRN (-LRB- -LRB-) (NNP TD) (-RRB- -RRB-)) (NN learning)) (PP (IN in) (NP (NP (VBN discounted) (NNP Markov) (NN decision) (VBZ processes)) (, ,) (SBAR (WHADVP (WRB where)) (S (NP (DT the) (NN goal)) (VP (VBZ is) (S (VP (TO to) (VP (VB evaluate) (NP (DT a) (NN policy)) (PP (IN in) (NP (DT a) (JJ model-free) (NN way))) (PP (IN by) (S (VP (VBG using) (NP (NP (NNS observations)) (PP (IN of) (NP (DT a) (NN state) (NN process))) (VP (VBD generated) (PP (IN without) (S (VP (VBG executing) (NP (DT the) (NN policy)))))))))))))))))))) (. .))
(S (NP (DT These) (NNS results)) (VP (CONJP (RB not) (RB only)) (VP (VB lead) (ADVP (RB immediately)) (PP (TO to) (NP (NP (DT a) (NN characterization)) (PP (IN of) (NP (NP (DT the) (NN convergence) (NN behavior)) (PP (IN of) (NP (NP (ADJP (NNS least-squares) (VBN based)) (NN implementation)) (PP (IN of) (NP (PRP$ our) (NN scheme)))))))))) (, ,) (CONJP (CC but) (RB also)) (VP (VBP prepare) (NP (DT the) (NN ground)) (PP (IN for) (NP (NP (JJ further) (NN analysis)) (PP (IN of) (NP (JJ gradient-based) (NNS implementations))))))) (. .))
