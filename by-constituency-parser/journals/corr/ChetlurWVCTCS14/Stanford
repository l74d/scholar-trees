(S (NP (PRP We)) (VP (VBP present) (NP (NP (DT a) (NN library)) (PP (IN of) (NP (NP (JJ efficient) (NNS implementations)) (PP (IN of) (NP (NML (JJ deep) (NN learning)) (NNS primitives))))))) (. .))
(S (S (NP (JJ Deep) (NN learning) (NNS workloads)) (VP (VBP are) (ADVP (RB computationally)) (ADJP (JJ intensive)))) (, ,) (CC and) (S (S (VP (VBG optimizing) (NP (PRP$ their) (NNS kernels)))) (VP (VBZ is) (ADJP (ADJP (JJ difficult)) (CC and) (ADJP (NN time) (HYPH -) (VBG consuming))))) (. .))
(S (SBAR (IN As) (S (NP (JJ parallel) (NNS architectures)) (VP (VBP evolve)))) (, ,) (NP (NNS kernels)) (VP (MD must) (VP (VB be) (VP (VBN reoptimized) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ makes) (S (VP (VBG maintaining) (S (NP (NNS codebases)) (ADJP (JJ difficult) (PP (IN over) (NP (NN time))))))))))))) (. .))
(S (NP (JJ Similar) (NNS issues)) (VP (VBP have) (ADVP (RB long)) (VP (VBN been) (VP (VBN addressed) (PP (IN in) (NP (DT the) (NNP HPC) (NN community))) (PP (IN by) (NP (NP (NNS libraries)) (PP (JJ such) (IN as) (NP (DT the) (NNP Basic) (NNP Linear) (NNP Algebra) (NNPS Subroutines))))) (PRN (-LRB- -LRB-) (NP (NNP BLAS)) (-RRB- -RRB-))))) (. .))
(S (ADVP (RB However)) (, ,) (NP (EX there)) (VP (VBZ is) (NP (NP (DT no) (JJ analogous) (NN library)) (PP (IN for) (NP (JJ deep) (NN learning))))) (. .))
(S (S (PP (IN Without) (NP (PDT such) (DT a) (NN library))) (, ,) (NP (NP (NNS researchers)) (VP (VBG implementing) (NP (JJ deep) (NN learning) (NNS workloads)) (PP (IN on) (NP (JJ parallel) (NNS processors))))) (VP (MD must) (VP (VB create) (CC and) (VB optimize) (NP (NP (PRP$ their) (JJ own) (NNS implementations)) (PP (IN of) (NP (DT the) (JJ main) (JJ computational) (NNS kernels))))))) (, ,) (CC and) (S (NP (DT this) (NN work)) (VP (MD must) (VP (VB be) (VP (VBN repeated) (SBAR (IN as) (S (NP (JJ new) (JJ parallel) (NNS processors)) (VP (VBP emerge)))))))) (. .))
(S (S (VP (TO To) (VP (VB address) (NP (DT this) (NN problem))))) (, ,) (NP (PRP we)) (VP (VBP have) (VP (VBN created) (NP (NP (DT a) (NN library)) (ADJP (JJ similar) (PP (IN in) (NP (NN intent))))) (PP (TO to) (NP (NNP BLAS))) (, ,) (PP (IN with) (NP (NP (VBN optimized) (NNS routines)) (PP (IN for) (NP (NML (JJ deep) (NN learning)) (NNS workloads))))))) (. .))
(S (S (NP (PRP$ Our) (NN implementation)) (VP (VBZ contains) (NP (NP (NNS routines)) (PP (IN for) (NP (NNS GPUs)))))) (, ,) (S (SBAR (IN although) (FRAG (ADVP (RB similarly)) (PP (IN to) (NP (DT the) (NNP BLAS) (NN library))))) (, ,) (NP (DT these) (NNS routines)) (VP (MD could) (VP (VB be) (VP (VBN implemented) (PP (IN for) (NP (JJ other) (NNS platforms))))))) (. .))
(S (NP (DT The) (NN library)) (VP (VP (VBZ is) (ADJP (JJ easy) (S (VP (TO to) (VP (VB integrate) (PP (IN into) (NP (VBG existing) (NNS frameworks)))))))) (, ,) (CC and) (VP (VBZ provides) (NP (VBN optimized) (NML (NN performance) (CC and) (NN memory)) (NN usage)))) (. .))
(S (PP (IN For) (NP (NN example))) (, ,) (S (VP (VBG integrating) (NP (NN cuDNN)) (PP (IN into) (NP (NP (NNP Caffe)) (, ,) (NP (NP (DT a) (JJ popular) (NN framework)) (PP (IN for) (NP (JJ convolutional) (NNS networks)))))))) (, ,) (VP (VBZ improves) (NP (NN performance)) (PP (IN by) (NP (NP (CD 36) (NN %)) (PP (IN on) (NP (DT a) (JJ standard) (NN model))))) (SBAR (IN while) (S (ADVP (RB also)) (VP (VBG reducing) (NP (NN memory) (NN consumption)))))) (. .))
