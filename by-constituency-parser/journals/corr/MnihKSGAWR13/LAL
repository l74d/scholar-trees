(S (NP (PRP We)) (VP (VBP present) (NP (NP (DT the) (JJ first) (JJ deep) (NN learning) (NN model)) (SBAR (S (VP (TO to) (VP (ADVP (RB successfully)) (VB learn) (NP (NN control) (NNS policies)) (ADVP (RB directly)) (PP (IN from) (NP (JJ high-dimensional) (NN sensory) (NN input))) (S (VP (VBG using) (NP (JJ reinforcement) (NN learning)))))))))) (. .))
(S (NP (DT The) (NN model)) (VP (VBZ is) (NP (NP (DT a) (JJ convolutional) (JJ neural) (NN network)) (, ,) (VP (VBN trained) (PP (IN with) (NP (NP (DT a) (NN variant)) (PP (IN of) (NP (NNP Q-learning)))))) (, ,) (SBAR (SBAR (WHNP (WP$ whose) (NN input)) (S (VP (VBZ is) (NP (JJ raw) (NNS pixels))))) (CC and) (SBAR (WHNP (WP$ whose) (NN output)) (S (VP (VBZ is) (NP (NP (DT a) (NN value) (NN function)) (VP (VBG estimating) (NP (JJ future) (NNS rewards)))))))))) (. .))
(S (NP (PRP We)) (VP (VBP apply) (NP (PRP$ our) (NN method)) (PP (TO to) (NP (NP (CD seven) (NNP Atari) (CD 2600) (NNS games)) (PP (IN from) (NP (DT the) (NNP Arcade) (NNP Learning) (NNP Environment))))) (, ,) (PP (IN with) (NP (NP (DT no) (NN adjustment)) (PP (IN of) (NP (DT the) (NX (NX (NN architecture)) (CC or) (NX (VBG learning) (NNS algorithm)))))))) (. .))
(S (NP (PRP We)) (VP (VBP find) (SBAR (IN that) (S (NP (PRP it)) (VP (VP (VBZ outperforms) (NP (DT all) (JJ previous) (NNS approaches)) (PP (IN on) (NP (NP (CD six)) (PP (IN of) (NP (DT the) (NNS games)))))) (CC and) (VP (VBZ surpasses) (NP (DT a) (JJ human) (NN expert)) (PP (IN on) (NP (NP (CD three)) (PP (IN of) (NP (PRP them)))))))))) (. .))
