(S (NP (NP (DT The) (ADJP (RBS most) (JJ data-efficient)) (NN algorithms)) (PP (IN for) (NP (NP (NN reinforcement) (NN learning)) (PP (IN in) (NP (NNS robotics)))))) (VP (VBP are) (NP (NP (JJ model-based) (NN policy) (NN search) (NN algorithms)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBP alternate) (PP (IN between) (S (VP (VP (VBG learning) (NP (NP (DT a) (JJ dynamical) (NN model)) (PP (IN of) (NP (DT the) (NN robot))))) (CC and) (VP (VBG optimizing) (NP (DT a) (NN policy)) (S (VP (TO to) (VP (VB maximize) (NP (NP (DT the) (JJ expected) (NN return)) (PP (VBN given) (NP (NP (DT the) (NN model)) (CC and) (NP (PRP$ its) (NNS uncertainties))))))))))))))))) (. .))
(S (S (ADVP (RB However)) (, ,) (NP (DT the) (JJ current) (NN algorithms)) (VP (NN lack) (NP (NP (DT an) (JJ effective) (NN exploration) (NN strategy)) (SBAR (S (VP (TO to) (VP (VB deal) (PP (IN with) (NP (ADJP (NN sparse) (CC or) (VBG misleading)) (JJ reward) (NNS scenarios)))))))))) (: :) (S (SBAR (IN if) (S (NP (PRP they)) (VP (VBP do) (RB not) (VP (VB experience) (NP (NP (DT any) (NN state)) (PP (IN with) (NP (DT a) (JJ positive) (NN reward)))) (PP (IN during) (NP (DT the) (JJ initial) (NN random) (NN exploration))))))) (, ,) (NP (PRP it)) (VP (VBZ is) (ADJP (RB very) (JJ unlikely) (S (VP (TO to) (VP (VB solve) (NP (DT the) (NN problem)))))))) (. .))
(S (ADVP (RB Here)) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (DT a) (JJ novel) (JJ model-based) (NN policy) (NN search) (NN algorithm)) (, ,) (NP (NNP Multi-DEX)) (, ,) (SBAR (WHNP (WDT that)) (S (VP (VBZ leverages) (NP (DT a) (JJ learned) (JJ dynamical) (NN model)) (S (VP (TO to) (VP (VP (ADVP (RB efficiently)) (VB explore) (NP (DT the) (NN task) (NN space))) (CC and) (VP (VB solve) (NP (NP (NNS tasks)) (PP (IN with) (NP (JJ sparse) (NNS rewards)))) (PP (IN in) (NP (DT a) (JJ few) (NNS episodes)))))))))))) (. .))
(S (S (VP (TO To) (VP (VB achieve) (NP (DT this))))) (, ,) (NP (PRP we)) (VP (VBP frame) (NP (DT the) (NN policy) (NN search) (NN problem)) (PP (IN as) (NP (NP (DT a) (JJ multi-objective) (, ,) (JJ model-based) (NN policy) (NN optimization) (NN problem)) (PP (IN with) (NP (NP (CD three) (NNS objectives)) (: :) (S (VP (PRN (-LRB- -LRB-) (CD 1) (-RRB- -RRB-)) (VP (NN generate) (NP (ADJP (RB maximally) (JJ novel)) (NN state) (NNS trajectories)))) (, ,) (VP (PRN (-LRB- -LRB-) (CD 2) (-RRB- -RRB-)) (VP (VB maximize) (NP (DT the) (JJ expected) (NN return)))) (CC and) (VP (PRN (-LRB- -LRB-) (CD 3) (-RRB- -RRB-)) (VP (VB keep) (NP (DT the) (NN system)) (PP (IN in) (NP (NP (JJ state-space) (NNS regions)) (SBAR (WHPP (IN for) (WHNP (WDT which))) (S (NP (DT the) (NN model)) (VP (VBZ is) (ADJP (ADJP (RB as) (JJ accurate)) (PP (IN as) (ADJP (JJ possible))))))))))))))))) (. .))
(S (NP (PRP We)) (ADVP (RB then)) (VP (VB optimize) (NP (DT these) (NNS objectives)) (S (VP (VBG using) (NP (DT a) (JJ Pareto-based) (JJ multi-objective) (NN optimization) (NN algorithm))))) (. .))
(S (NP (DT The) (NNS experiments)) (VP (VBP show) (SBAR (IN that) (S (NP (NNP Multi-DEX)) (VP (VBZ is) (ADJP (JJ able) (S (VP (TO to) (VP (VB solve) (NP (JJ sparse) (NN reward) (NNS scenarios)) (PRN (-LRB- -LRB-) (PP (IN with) (NP (DT a) (JJ simulated) (JJ robotic) (NN arm))) (-RRB- -RRB-)) (PP (IN in) (NP (NP (ADJP (JJ much) (JJR lower)) (NN interaction) (NN time)) (PP (IN than) (NP (NNP VIME) (, ,) (NNP TRPO) (, ,) (NNP GEP-PG) (, ,) (NNP CMA-ES) (CC and) (NNP Black-DROPS))))))))))))) (. .))
