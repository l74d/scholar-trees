(S (NP (JJ Deep) (JJ neural) (NNS networks)) (VP (VBP have) (VP (VBN shown) (NP (NP (JJ remarkable) (NN performance)) (PP (IN across) (NP (ADJP (NP (NP (DT a) (JJ wide) (NN range)) (PP (IN of) (NP (NN vision)))) (HYPH -) (VBN based)) (NNS tasks)))) (, ,) (PP (ADVP (RB particularly)) (IN due) (IN to) (NP (NP (DT the) (NN availability)) (PP (IN of) (NP (NP (NML (JJ large) (HYPH -) (NN scale)) (NNS datasets)) (PP (IN for) (NP (NP (NN training)) (CC and) (NP (JJR better) (NNS architectures)))))))))) (. .))
(S (ADVP (RB However)) (, ,) (NP (NP (NNS data)) (VP (VBN seen) (PP (IN in) (NP (DT the) (JJ real) (NN world))))) (VP (VBP are) (ADVP (RB often)) (VP (VBN affected) (PP (IN by) (NP (NP (NNS distortions)) (SBAR (WHNP (WDT that)) (S (ADVP (RB not)) (VP (VBD accounted) (PP (IN for) (PP (IN by) (NP (DT the) (NN training) (NNS datasets))))))))))) (. .))
(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (NP (PRP we)) (VP (VP (VBP address) (NP (NP (DT the) (NN challenge)) (PP (IN of) (NP (NN robustness) (CC and) (NN stability))) (PP (IN of) (NP (JJ neural) (NNS networks))))) (CC and) (VP (VB propose) (NP (NP (DT a) (JJ general) (NN training) (NN method)) (SBAR (WHNP (WDT that)) (S (VP (MD can) (VP (VB be) (VP (VBN used) (S (VP (TO to) (VP (VB make) (NP (DT the) (VBG existing) (JJ neural) (NN network) (NNS architectures)) (S (ADJP (RBR more) (JJ robust) (CC and) (JJ stable))) (PP (IN to) (NP (NN input) (JJ visual) (NNS perturbations))))))))))))) (PP (IN while) (S (VP (VBG using) (NP (ADJP (RB only) (JJ available)) (NNS datasets)) (PP (IN for) (NP (NN training))))))) (. .))
(S (NP (VBN Proposed) (NN training) (NN method)) (VP (VBZ is) (ADJP (JJ convenient) (S (VP (TO to) (VP (VB use) (SBAR (IN as) (S (NP (PRP it)) (VP (VBZ does) (RB not) (VP (VB require) (NP (NP (NNS data) (NN augmentation)) (CC or) (NP (NNS changes))) (PP (IN in) (NP (DT the) (NN network) (NN architecture)))))))))))) (. .))
(S (NP (PRP We)) (VP (VP (VBP provide) (NP (NP (JJ theoretical) (NN proof)) (CONJP (RB as) (RB well) (IN as)) (NP (JJ empirical) (NN evidence))) (PP (IN for) (NP (NP (DT the) (NN efficiency)) (PP (IN of) (NP (NP (DT the) (VBN proposed) (NN training) (NN method)) (PP (IN by) (S (VP (VBG performing) (NP (NNS experiments)) (PP (IN with) (NP (VBG existing) (NML (JJ neural) (NN network)) (NNS architectures))))))))))) (CC and) (VP (VBP demonstrate) (NP (DT that) (JJ same) (NN architecture)) (SBAR (WHADVP (WRB when)) (S (VP (VBN trained) (PP (IN with) (NP (DT the) (VBN proposed) (NN training) (NN method))))))) (VP (VB perform) (NP (JJR better)) (PP (IN than) (SBAR (WHADVP (WRB when)) (S (VP (VBN trained) (PP (IN with) (NP (NP (JJ conventional) (NN training) (NN approach)) (PP (IN in) (NP (NP (DT the) (NN presence)) (PP (IN of) (NP (JJ noisy) (NNS datasets))))))))))))) (. .))
