(S (NP (NP (JJ Model-free) (NN reinforcement) (VBG learning) (NNS methods)) (PP (JJ such) (IN as) (NP (NP (DT the) (NNP Proximal) (NNP Policy) (NNP Optimization) (NN algorithm)) (PRN (-LRB- -LRB-) (NP (NNP PPO)) (-RRB- -RRB-))))) (VP (VBP have) (VP (ADVP (RB successfully)) (VBN applied) (PP (IN in) (NP (NP (JJ complex) (JJ decision-making) (NNS problems)) (PP (JJ such) (IN as) (NP (NNP Atari) (NNS games))))))) (. .))
(S (ADVP (RB However)) (, ,) (NP (DT these) (NNS methods)) (VP (VBP suffer) (PP (IN from) (NP (NP (JJ high) (NNS variances)) (CC and) (NP (JJ high) (NN sample) (NN complexity))))) (. .))
(S (PP (IN On) (NP (DT the) (JJ other) (NN hand))) (, ,) (S (NP (NP (JJ model-based) (NN reinforcement) (VBG learning) (NNS methods)) (SBAR (WHNP (WDT that)) (S (VP (VBP learn) (NP (DT the) (NN transition) (NNS dynamics)))))) (VP (VBP are) (ADJP (JJR more) (JJ sample) (NN efficient)))) (, ,) (CC but) (S (NP (PRP they)) (ADVP (RB often)) (VP (VBP suffer) (PP (IN from) (NP (NP (DT the) (NN bias)) (PP (IN of) (NP (DT the) (NN transition) (NN estimation))))))) (. .))
(S (SBAR (WHADVP (WRB How)) (S (VP (TO to) (VP (VB make) (NP (NN use)) (PP (IN of) (NP (ADJP (DT both) (JJ model-based) (CC and) (JJ model-free)) (NN learning))))))) (VP (VBZ is) (NP (NP (DT a) (JJ central) (NN problem)) (PP (IN in) (NP (NN reinforcement) (NN learning))))) (. .))
(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (NP (PRP we)) (VP (VBP present) (NP (NP (DT a) (JJ new) (NN technique)) (SBAR (S (VP (TO to) (VP (VB address) (NP (NP (DT the) (NN trade-off)) (PP (IN between) (NP (NN exploration) (CC and) (NN exploitation))) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ regards) (NP (NP (DT the) (NN difference)) (PP (IN between) (NP (ADJP (JJ model-free) (CC and) (JJ model-based)) (NNS estimations)))) (PP (IN as) (NP (NP (DT a) (NN measure)) (PP (IN of) (NP (NN exploration) (NN value))))))))))))))) (. .))
(S (NP (PRP We)) (VP (VP (VBP apply) (NP (DT this) (JJ new) (NN technique)) (PP (TO to) (NP (DT the) (NNP PPO) (NN algorithm)))) (CC and) (VP (NN arrive) (PP (IN at) (NP (NP (DT a) (JJ new) (NN policy) (NN optimization) (NN method)) (, ,) (VP (VBN named) (S (NP (NP (NP (NNP Policy) (NNP Optimization)) (PP (IN with) (NP (JJ Model-based) (NNS Explorations)))) (PRN (-LRB- -LRB-) (NP (NNP POME)) (-RRB- -RRB-))))))))) (. .))
(S (NP (NNP POME)) (VP (VBZ uses) (NP (NP (CD two) (NNS components))) (S (VP (TO to) (VP (VB predict) (NP (NP (DT the) (NNS actions) (POS â€º)) (NN target) (NNS values))))) (: :) (NP (NP (NP (DT a) (JJ model-free) (CD one)) (VP (VBN estimated) (PP (IN by) (NP (NNP Monte-Carlo) (NN sampling))))) (CC and) (NP (NP (DT a) (JJ model-based) (CD one)) (SBAR (WHNP (WDT which)) (S (VP (VP (VBZ learns) (NP (DT a) (NN transition) (NN model))) (CC and) (VP (VBZ predicts) (NP (NP (DT the) (NN value)) (PP (IN of) (NP (DT the) (JJ next) (NN state))))))))))) (. .))
(S (NP (NNP POME)) (VP (VP (VBZ adds) (NP (NP (DT the) (NN error)) (PP (IN of) (NP (DT these) (CD two) (NN target) (NNS estimations)))) (PP (IN as) (NP (NP (DT the) (JJ additional) (NN exploration) (NN value)) (PP (IN for) (NP (DT each) (NN state-action) (NN pair)))))) (, ,) (PRN (VP (NN i.e)) (, ,)) (S (VP (VBZ encourages) (S (NP (DT the) (NN algorithm)) (VP (TO to) (VP (VB explore) (NP (NP (DT the) (NNS states)) (PP (IN with) (NP (NP (JJR larger) (NN target) (NNS errors)) (SBAR (WHNP (WDT which)) (S (VP (VBP are) (ADJP (JJ hard) (S (VP (TO to) (VP (VB estimate))))))))))))))))) (. .))
(S (S (NP (PRP We)) (VP (VBP compare) (NP (NNP POME)) (PP (IN with) (NP (NNP PPO))) (PP (IN on) (NP (NNP Atari) (CD 2600) (NNS games))))) (, ,) (CC and) (S (NP (PRP it)) (VP (VBZ shows) (SBAR (IN that) (S (NP (NNP POME)) (VP (VBZ outperforms) (NP (NNP PPO)) (PP (IN on) (NP (NP (CD 33) (NNS games)) (PP (IN out) (PP (IN of) (NP (CD 49) (NNS games))))))))))) (. .))
