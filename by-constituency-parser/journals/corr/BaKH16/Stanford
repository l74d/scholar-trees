(S (S (VP (VBG Training) (NP (ADJP (NN state) (HYPH -) (IN of) (HYPH -) (DT the) (HYPH -) (NN art))))) (, ,) (NP (JJ deep) (JJ neural) (NNS networks)) (VP (VBZ is) (ADVP (RB computationally)) (ADJP (JJ expensive))) (. .))
(S (NP (NP (CD One) (NN way)) (SBAR (S (VP (TO to) (VP (VB reduce) (NP (DT the) (NN training) (NN time))))))) (VP (VBZ is) (S (VP (TO to) (VP (VB normalize) (NP (NP (DT the) (NNS activities)) (PP (IN of) (NP (DT the) (NNS neurons)))))))) (. .))
(S (NP (NP (DT A) (ADJP (RB recently) (VBN introduced)) (NN technique)) (VP (VBN called) (NP (NN batch) (NN normalization)))) (VP (VBZ uses) (NP (NP (DT the) (NN distribution)) (PP (IN of) (NP (NP (DT the) (VBN summed) (NN input)) (PP (IN to) (NP (DT a) (ADJP (NN neuron) (PP (IN over) (NP (NP (DT a) (NN mini-batch)) (PP (IN of) (NP (NN training) (NNS cases))))) (PP (IN to) (S (VP (VB compute) (NP (NP (DT a) (NN mean) (CC and) (NN variance)) (SBAR (WHNP (WDT which)) (S (VP (VBP are) (ADVP (RB then)) (VP (VBN used) (S (VP (TO to) (VP (VB normalize) (NP (DT the) (VBN summed) (NN input)) (PP (IN to) (NP (NP (DT that) (NN neuron)) (PP (IN on) (NP (DT each) (NN training))))))))))))))))) (NN case))))))) (. .))
(S (NP (DT This)) (ADVP (RB significantly)) (VP (VBZ reduces) (NP (NP (DT the) (NN training) (NN time)) (PP (IN in) (NP (NML (NN feed) (HYPH -) (JJ forward)) (JJ neural) (NNS networks))))) (. .))
(S (S (ADVP (RB However)) (, ,) (NP (NP (DT the) (NN effect)) (PP (IN of) (NP (NN batch) (NN normalization)))) (VP (VBZ is) (ADJP (JJ dependent) (PP (IN on) (NP (DT the) (NN mini-batch) (NN size)))))) (CC and) (S (NP (PRP it)) (VP (VBZ is) (RB not) (ADJP (JJ obvious)) (SBAR (WHADVP (WRB how)) (S (VP (TO to) (VP (VB apply) (NP (PRP it)) (PP (IN to) (NP (ADJP (JJ recurrent)) (JJ neural) (NNS networks))))))))) (. .))
(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (NP (PRP we)) (VP (VBP transpose) (NP (NN batch) (NN normalization)) (PP (IN into) (NP (NN layer) (NN normalization))) (PP (IN by) (S (VP (VBG computing) (NP (NP (DT the) (NN mean) (CC and) (NN variance)) (VP (VBN used) (PP (IN for) (NP (NN normalization))) (PP (IN from) (NP (NP (DT all)) (PP (IN of) (NP (DT the) (VBN summed) (NNS inputs))))) (PP (IN to) (NP (NP (DT the) (NNS neurons)) (PP (IN in) (NP (DT a) (NN layer))))) (PP (IN on) (NP (DT a) (JJ single) (NN training) (NN case))))))))) (. .))
(S (PP (IN Like) (NP (NN batch) (NN normalization))) (, ,) (NP (PRP we)) (ADVP (RB also)) (VP (VBP give) (NP (DT each) (NN neuron)) (NP (NP (PRP$ its) (JJ own) (JJ adaptive) (NN bias) (CC and) (NN gain)) (SBAR (WHNP (WDT which)) (S (VP (VBP are) (VP (VBN applied) (PP (PP (IN after) (NP (DT the) (NN normalization))) (CC but) (PP (IN before) (NP (DT the) (NN non-linearity)))))))))) (. .))
(S (PP (IN Unlike) (NP (NN batch) (NN normalization))) (, ,) (NP (NN layer) (NN normalization)) (VP (VBZ performs) (NP (RB exactly) (DT the) (JJ same) (NN computation)) (PP (IN at) (NP (NML (NN training) (CC and) (NN test)) (NNS times)))) (. .))
(S (NP (PRP It)) (VP (VBZ is) (ADVP (RB also)) (ADJP (JJ straightforward) (S (VP (TO to) (VP (VB apply) (PP (IN to) (NP (ADJP (JJ recurrent)) (JJ neural) (NNS networks))) (PP (IN by) (S (VP (VBG computing) (NP (DT the) (NN normalization) (NNS statistics)) (ADVP (RB separately)) (PP (IN at) (NP (DT each) (NN time) (NN step))))))))))) (. .))
(S (NP (NN Layer) (NN normalization)) (VP (VBZ is) (ADJP (RB very) (JJ effective) (PP (IN at) (S (VP (VBG stabilizing) (NP (DT the) (JJ hidden) (NN state) (NNS dynamics)) (PP (IN in) (NP (ADJP (JJ recurrent)) (NNS networks)))))))) (. .))
(S (ADVP (RB Empirically)) (, ,) (NP (PRP we)) (VP (VBP show) (SBAR (IN that) (S (NP (NN layer) (NN normalization)) (VP (MD can) (ADVP (RB substantially)) (VP (VB reduce) (NP (DT the) (NN training) (NN time)) (PP (VBN compared) (PP (IN with) (NP (ADJP (RB previously) (VBN published)) (NNS techniques))))))))) (. .))
