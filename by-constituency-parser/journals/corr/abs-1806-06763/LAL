(S (NP (NP (JJ Adaptive) (NN gradient) (NNS methods)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBP adopt) (NP (JJ historical) (NN gradient) (NN information)) (S (VP (TO to) (VP (ADVP (RB automatically)) (VB adjust) (NP (DT the) (NN learning) (NN rate))))))))) (, ,) (PP (IN despite) (NP (NP (DT the) (JJ nice) (NN property)) (PP (IN of) (NP (JJ fast) (NN convergence))))) (, ,) (VP (VBP have) (VP (VBN been) (VP (VBN observed) (S (VP (TO to) (VP (VB generalize) (ADVP (ADVP (JJR worse)) (PP (IN than) (NP (NP (JJ stochastic) (JJ gradient) (NN descent)) (PRN (-LRB- -LRB-) (NP (NNP SGD)) (-RRB- -RRB-)) (PP (IN with) (NP (NN momentum)))))) (PP (IN in) (S (VP (VBG training) (NP (JJ deep) (JJ neural) (NNS networks))))))))))) (. .))
(S (NP (DT This)) (VP (VBZ leaves) (SBAR (WHADVP (WRB how)) (S (VP (TO to) (VP (VB close) (NP (NP (DT the) (NN generalization) (NN gap)) (PP (IN of) (NP (JJ adaptive) (NN gradient) (NNS methods)))))))) (NP (DT an) (JJ open) (NN problem))) (. .))
(S (PP (IN In) (NP (DT this) (NN work))) (, ,) (NP (PRP we)) (VP (VBP show) (SBAR (IN that) (S (NP (NP (JJ adaptive) (NN gradient) (NNS methods)) (PP (JJ such) (IN as) (NP (NNP Adam) (, ,) (NNP Amsgrad) (, ,)))) (VP (VBP are) (ADVP (RB sometimes)) (`` ``) (ADJP (ADVP (IN over)) (VBN adapted)) ('' ''))))) (. .))
(S (NP (PRP We)) (VP (VBP design) (NP (NP (DT a) (JJ new) (NN algorithm)) (, ,) (VP (VBN called) (S (NP (ADJP (RB Partially) (JJ adaptive)) (NN momentum) (NN estimation) (NN method)))) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ unifies) (NP (DT the) (NNP Adam/Amsgrad)) (PP (IN with) (NP (NNP SGD))) (PP (IN by) (S (VP (VBG introducing) (NP (NP (DT a) (JJ partial) (JJ adaptive) (NN parameter)) (NP ($ $) (JJ p) ($ $)))))))))) (, ,) (S (VP (TO to) (VP (VB achieve) (NP (NP (DT the) (JJS best)) (PP (IN from) (NP (DT both) (NNS worlds)))))))) (. .))
(S (NP (PRP We)) (ADVP (RB also)) (VP (VBP prove) (NP (NP (DT the) (NN convergence) (NN rate)) (PP (IN of) (NP (PRP$ our) (VBN proposed) (NN algorithm))) (PP (TO to) (NP (NP (DT a) (JJ stationary) (NN point)) (PP (IN in) (NP (DT the) (JJ stochastic) (JJ nonconvex) (NN optimization) (NN setting))))))) (. .))
(S (NP (NP (NNS Experiments)) (PP (IN on) (NP (JJ standard) (NNS benchmarks)))) (VP (VBP show) (SBAR (IN that) (S (NP (PRP$ our) (VBN proposed) (NN algorithm)) (VP (MD can) (VP (VB maintain) (NP (NP (DT a) (JJ fast) (NN convergence) (NN rate)) (PP (IN as) (NP (NNP Adam/Amsgrad)))) (SBAR (IN while) (S (VP (VBG generalizing) (ADVP (RB as) (RB well) (IN as) (NP (NNP SGD))) (PP (IN in) (S (VP (VBG training) (NP (JJ deep) (JJ neural) (NNS networks))))))))))))) (. .))
(S (NP (DT These) (NNS results)) (VP (MD would) (VP (VB suggest) (SBAR (S (NP (NNS practitioners)) (VP (VBP pick) (PRT (RP up)) (NP (JJ adaptive) (NN gradient) (NNS methods)) (ADVP (RB once) (RB again)) (PP (IN for) (NP (NP (JJR faster) (NN training)) (PP (IN of) (NP (JJ deep) (JJ neural) (NNS networks)))))))))) (. .))
