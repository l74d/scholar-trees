(S (NP (DT This) (NN work)) (VP (VBZ introduces) (NP (NP (NNP TapirXLA)) (, ,) (NP (NP (DT a) (NN replacement)) (PP (IN for) (NP (NP (NNP TensorFlow) (POS 's)) (NNP XLA) (NN compiler))) (SBAR (WHNP (WDT that)) (S (VP (VBZ embeds) (NP (JJ recursive) (JJ fork-join) (NN parallelism)) (PP (IN into) (NP (NP (NP (NNP XLA) (POS 's)) (JJ low-level) (NN representation)) (PP (IN of) (NP (NN code))))))))))) (. .))
(S (S (NP (JJ Machine-learning) (NNS applications)) (VP (VBP rely) (PP (IN on) (NP (JJ efficient) (JJ parallel) (NN processing))) (S (VP (TO to) (VP (VB achieve) (NP (NN performance))))))) (, ,) (CC and) (S (NP (PRP they)) (VP (VBP employ) (NP (NP (NP (DT a) (NN variety)) (PP (IN of) (NP (NNS technologies))))) (S (VP (TO to) (VP (VB improve) (NP (NN performance))))) (, ,) (PP (VBG including) (NP (NN compiler) (NN technology))))) (. .))
(S (CC But) (NP (NP (NNS compilers)) (PP (IN in) (NP (JJ machine-learning) (NNS frameworks)))) (VP (VBP lack) (NP (NP (DT a) (JJ deep) (NN understanding)) (PP (IN of) (NP (NN parallelism)))) (, ,) (S (VP (VBG causing) (S (NP (PRP them)) (VP (TO to) (VP (VB lose) (NP (NN performance)) (PP (IN by) (S (VP (VBG missing) (NP (NP (NNS optimizations)) (PP (IN on) (NP (JJ parallel) (NN computation))))))))))))) (. .))
(S (NP (DT This) (NN work)) (VP (NNS studies) (SBAR (WHADVP (WRB how)) (S (NP (NP (NNP Tapir)) (, ,) (NP (NP (DT a) (NN compiler) (JJ intermediate) (NN representation)) (PRN (-LRB- -LRB-) (NP (NNP IR)) (-RRB- -RRB-)) (SBAR (WHNP (WDT that)) (S (VP (VBZ embeds) (NP (NN parallelism)) (PP (IN into) (NP (DT a) (JJ mainstream) (NN compiler) (NNP IR))))))) (, ,)) (VP (MD can) (VP (VB be) (VP (VBN incorporated) (PP (IN into) (NP (NP (DT a) (NN compiler)) (PP (IN for) (NP (NN machine) (NN learning))))) (S (VP (TO to) (VP (VB remedy) (NP (DT this) (NN problem))))))))))) (. .))
(S (NP (NNP TapirXLA)) (VP (VBZ modifies) (NP (NP (DT the) (NNP XLA) (NN compiler)) (PP (IN in) (NP (NNP TensorFlow)))) (S (VP (TO to) (VP (VB employ) (NP (DT the) (NNP Tapir/LLVM) (NN compiler)) (S (VP (TO to) (VP (VB optimize) (NP (JJ low-level) (JJ parallel) (NN computation))))))))) (. .))
(S (NP (NNP TapirXLA)) (VP (VBZ encodes) (NP (DT the) (NN parallelism)) (PP (IN within) (NP (JJ high-level) (NNP TensorFlow) (NNS operations))) (S (VP (VBG using) (NP (NP (NP (NNP Tapir) (POS 's)) (NN representation)) (PP (IN of) (NP (JJ fork-join) (NN parallelism))))))) (. .))
(S (NP (NNP TapirXLA)) (ADVP (RB also)) (VP (VBZ exposes) (PP (TO to) (NP (DT the) (NN compiler))) (NP (NP (NNS implementations)) (PP (IN of) (NP (NP (JJ linear-algebra) (JJ library) (NNS routines)) (SBAR (WHNP (WP$ whose) (JJ parallel) (NNS operations)) (S (VP (VBP are) (VP (VBN encoded) (S (VP (VBG using) (NP (NP (NNP Tapir) (POS 's)) (NN representation)))))))))))) (. .))
(S (NP (PRP We)) (VP (VBN compared) (NP (NP (DT the) (NN performance)) (PP (IN of) (NP (NP (NNP TensorFlow)) (VP (VBG using) (NP (NNP TapirXLA)))))) (PP (IN against) (NP (NP (NNP TensorFlow)) (VP (VBG using) (NP (DT an) (JJ unmodified) (NNP XLA) (NN compiler)))))) (. .))
(S (PP (IN On) (NP (CD four) (NN neural-network) (NNS benchmarks))) (, ,) (NP (NNP TapirXLA)) (VP (VBZ speeds) (PRT (RP up)) (NP (NP (DT the) (JJ parallel) (JJ running) (NN time)) (PP (IN of) (NP (DT the) (NN network)))) (PP (IN by) (NP (NP (DT a) (JJ geometric-mean) (JJ multiplicative) (NN factor)) (PP (IN of) (NP (QP (CD 30) (NN %) (TO to) (CD 100) (NN %)))))) (, ,) (PP (IN across) (NP (CD four) (NNP CPU) (NNS architectures)))) (. .))
