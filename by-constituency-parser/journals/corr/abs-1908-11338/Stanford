(S (NP (DT This) (NN work)) (VP (VBZ introduces) (NP (NP (NP (NNP TapirXLA)) (, ,) (NP (NP (DT a) (NN replacement)) (PP (IN for) (NP (NP (NNP TensorFlow) (POS 's)) (NNP XLA) (NN compiler))))) (SBAR (WHNP (WDT that)) (S (VP (VBZ embeds) (S (NP (JJ recursive) (NML (NN fork) (HYPH -))) (VP (VB join) (NP (NN parallelism)) (PP (IN into) (NP (NP (NP (NNP XLA) (POS 's)) (NML (JJ low) (HYPH -) (NN level)) (NN representation)) (PP (IN of) (NP (NN code)))))))))))) (. .))
(S (S (NP (NML (NN Machine) (HYPH -) (NN learning)) (NNS applications)) (VP (VBP rely) (PP (IN on) (NP (JJ efficient) (NN parallel) (NN processing))) (S (VP (TO to) (VP (VB achieve) (NP (NN performance))))))) (, ,) (CC and) (S (NP (PRP they)) (VP (VBP employ) (NP (NP (DT a) (NN variety)) (PP (IN of) (NP (NNS technologies)))) (S (VP (TO to) (VP (VB improve) (NP (NP (NN performance)) (, ,) (PP (VBG including) (NP (NN compiler) (NN technology))))))))) (. .))
(S (CC But) (NP (NP (NNS compilers)) (PP (IN in) (NP (NML (NN machine) (HYPH -) (NN learning)) (NNS frameworks)))) (VP (VBP lack) (NP (NP (DT a) (JJ deep) (NN understanding)) (PP (IN of) (NP (NN parallelism)))) (, ,) (S (VP (VBG causing) (NP (PRP them)) (S (VP (TO to) (VP (VB lose) (NP (NN performance)) (PP (IN by) (NP (NP (VBG missing) (NNS optimizations)) (PP (IN on) (NP (JJ parallel) (NN computation))))))))))) (. .))
(NP (NP (DT This) (NN work) (NNS studies)) (SBAR (WHADVP (WRB how)) (S (NP (NP (NNP Tapir)) (, ,) (NP (NP (DT a) (NN compiler) (JJ intermediate) (NN representation) (PRN (-LRB- -LRB-) (NP (NN IR)) (-RRB- -RRB-))) (SBAR (WHNP (WDT that)) (S (VP (VBZ embeds) (NP (NN parallelism)) (PP (IN into) (NP (DT a) (NN mainstream) (NN compiler) (NN IR))))))) (, ,)) (VP (MD can) (VP (VB be) (VP (VBN incorporated) (PP (IN into) (NP (NP (DT a) (NN compiler)) (PP (IN for) (NP (NN machine) (NN learning))))) (S (VP (TO to) (VP (VB remedy) (NP (DT this) (NN problem)))))))))) (. .))
(S (NP (NNP TapirXLA)) (VP (VBZ modifies) (NP (NP (DT the) (NN XLA) (NN compiler)) (PP (IN in) (NP (NNP TensorFlow)))) (S (VP (TO to) (VP (VB employ) (NP (DT the) (NML (NN Tapir) (HYPH /) (NN LLVM)) (NN compiler)) (S (VP (TO to) (VP (VB optimize) (NP (NML (JJ low) (HYPH -) (NN level)) (JJ parallel) (NN computation))))))))) (. .))
(S (NP (NNP TapirXLA)) (VP (VBZ encodes) (NP (NP (DT the) (NN parallelism)) (PP (IN within) (NP (NML (JJ high) (HYPH -) (NN level)) (NNP TensorFlow) (NNS operations)))) (S (VP (VBG using) (NP (NP (NP (NNP Tapir) (POS 's)) (NN representation)) (PP (IN of) (NP (NML (S (NP (NN fork)) (HYPH -) (VP (VB join)))) (NN parallelism))))))) (. .))
(S (NP (NNP TapirXLA)) (ADVP (RB also)) (VP (VBZ exposes) (PP (IN to) (NP (NP (DT the) (NN compiler) (NNS implementations)) (PP (IN of) (NP (NML (JJ linear) (HYPH -) (NN algebra)) (NN library) (NNS routines))))) (SBAR (WHNP (WP$ whose)) (S (NP (JJ parallel) (NNS operations)) (VP (VBP are) (VP (VBN encoded) (S (VP (VBG using) (NP (NP (NNP Tapir) (POS 's)) (NN representation))))))))) (. .))
(S (NP (PRP We)) (VP (VBD compared) (NP (NP (DT the) (NN performance)) (PP (IN of) (NP (NNP TensorFlow)))) (S (VP (VBG using) (NP (NN TapirXLA)) (PP (IN against) (NP (NNP TensorFlow))) (S (VP (VBG using) (NP (DT an) (JJ unmodified) (NN XLA) (NN compiler))))))) (. .))
(S (PP (IN On) (NP (CD four) (NML (JJ neural) (HYPH -) (NN network)) (NNS benchmarks))) (, ,) (NP (NP (NN TapirXLA) (NNS speeds)) (PP (IN up) (NP (NP (DT the) (JJ parallel) (NN running) (NN time)) (PP (IN of) (NP (NP (DT the) (NN network)) (PP (IN by) (NP (DT a) (NML (JJ geometric) (HYPH -) (JJ mean)) (JJ multiplicative) (NN factor))))))) (PP (IN of) (NP (QP (CD 30) (NN %) (IN to) (CD 100) (NN %)))) (, ,) (PP (IN across) (NP (CD four) (NN CPU) (NNS architectures)))) (. .))
