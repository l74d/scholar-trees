(S (S (VP (VBG Tuning) (NP (NP (NNS hyperparameters)) (PP (IN of) (NP (VBG learning) (NN algorithms)))))) (VP (VBZ is) (ADJP (JJ hard)) (SBAR (IN because) (S (NP (NNS gradients)) (VP (VBP are) (ADVP (RB usually)) (ADJP (JJ unavailable)))))) (. .))
(S (NP (PRP We)) (VP (VBP compute) (NP (NP (JJ exact) (NNS gradients)) (PP (IN of) (NP (NN cross-validation) (NN performance))) (PP (IN with) (NP (NP (NN respect)) (PP (TO to) (NP (DT all) (NNS hyperparameters)))))) (PP (IN by) (S (VP (VBG chaining) (NP (NNS derivatives)) (ADVP (NNS backwards)) (PP (IN through) (NP (DT the) (JJ entire) (NN training) (NN procedure))))))) (. .))
(S (NP (DT These) (NNS gradients)) (VP (VBP allow) (S (NP (PRP us)) (VP (TO to) (VP (VB optimize) (NP (NP (NNS thousands)) (PP (IN of) (NP (NNS hyperparameters))) (, ,) (PP (VBG including) (NP (NP (JJ step-size) (CC and) (NN momentum) (NNS schedules)) (, ,) (NP (JJ weight) (NN initialization) (NNS distributions)) (, ,) (NP (ADJP (RB richly) (VBN parameterized)) (NN regularization) (NNS schemes)) (, ,) (CC and) (NP (JJ neural) (NN network) (NNS architectures))))))))) (. .))
(S (NP (PRP We)) (VP (VBP compute) (NP (JJ hyperparameter) (NNS gradients)) (PP (IN by) (S (VP (ADVP (RB exactly)) (VBG reversing) (NP (NP (DT the) (NNS dynamics)) (PP (IN of) (NP (NP (JJ stochastic) (NN gradient) (NN descent)) (PP (IN with) (NP (NN momentum)))))))))) (. .))
