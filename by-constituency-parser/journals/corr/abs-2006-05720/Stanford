(S (NP (JJ Deep) (NN learning) (NNS networks)) (VP (VBP are) (ADVP (RB typically)) (VP (VBN trained) (PP (IN by) (NP (NP (NML (NNP Stochastic) (NNP Gradient) (NNP Descent) (-LRB- -LRB-) (NNP SGD) (-RRB- -RRB-)) (NNS methods)) (SBAR (WHNP (WDT that)) (S (ADVP (RB iteratively)) (VP (VBP improve) (NP (DT the) (NN model) (NNS parameters)) (PP (IN by) (S (VP (VBG estimating) (NP (DT a) (NN gradient)) (PP (IN on) (NP (NP (DT a) (ADJP (RB very) (JJ small)) (NN fraction)) (PP (IN of) (NP (DT the) (NN training) (NNS data))))))))))))))) (. .))
(S (NP (NP (DT A) (JJ major) (NN roadblock)) (VP (VBN faced) (SBAR (WHADVP (WRB when)) (S (VP (VBG increasing) (NP (DT the) (NN batch) (NN size)) (PP (IN to) (NP (NP (DT a) (JJ substantial) (NN fraction)) (PP (IN of) (NP (DT the) (NN training) (NNS data))))) (PP (IN for) (S (VP (VBG improving) (NP (NN training) (NN time)))))))))) (VP (VBZ is) (NP (NP (DT the) (JJ persistent) (NN degradation)) (PP (IN in) (NP (NN performance)))) (PRN (-LRB- -LRB-) (NP (NN generalization) (NN gap)) (-RRB- -RRB-))) (. .))
(S (S (VP (TO To) (VP (VB address) (NP (DT this) (NN issue))))) (, ,) (NP (JJ recent) (NN work)) (VP (VB propose) (S (VP (TO to) (VP (VB add) (NP (JJ small) (NNS perturbations)) (PP (IN to) (NP (DT the) (NN model) (NNS parameters))) (SBAR (WHADVP (WRB when)) (S (S (VP (VBG computing) (NP (NP (DT the) (JJ stochastic) (NNS gradients)) (CC and) (NP (NN report))))) (VP (VBD improved) (NP (NN generalization) (NN performance)) (PP (IN due) (IN to) (NP (VBG smoothing) (NNS effects)))))))))) (. .))
(S (S (ADVP (RB However)) (, ,) (NP (DT this) (NN approach)) (VP (VBZ is) (ADJP (RB poorly) (VBN understood)))) (: ;) (S (NP (PRP it)) (VP (VBZ requires) (NP (NP (ADJP (NP (RB often) (JJ model)) (HYPH -) (JJ specific)) (NN noise)) (CC and) (NP (JJ fine) (HYPH -) (NN tuning))))) (. .))
(S (S (VP (TO To) (VP (VB alleviate) (NP (DT these) (NNS drawbacks))))) (, ,) (NP (PRP we)) (VP (VBP propose) (S (VP (TO to) (VP (VB use) (PP (RB instead) (ADVP (RB computationally))) (NP (NP (JJ efficient) (NN extrapolation) (-LRB- -LRB-) (NN extragradient) (-RRB- -RRB-)) (S (VP (TO to) (VP (VB stabilize) (NP (DT the) (NN optimization) (NN trajectory)) (SBAR (IN while) (S (ADVP (RB still)) (VP (VBG benefiting) (PP (IN from) (S (VP (VBG smoothing) (S (VP (TO to) (VP (VB avoid) (NP (JJ sharp) (NN minima))))))))))))))))))) (. .))
(S (S (NP (DT This) (JJ principled) (NN approach)) (VP (VBZ is) (ADVP (RB well)) (VP (VBN grounded) (PP (IN from) (NP (DT an) (NN optimization) (NN perspective)))))) (CC and) (S (NP (PRP we)) (VP (VBP show) (SBAR (IN that) (S (NP (NP (DT a) (NN host)) (PP (IN of) (NP (NNS variations)))) (VP (MD can) (VP (VB be) (VP (VBN covered) (PP (IN in) (NP (NP (DT a) (JJ unified) (NN framework)) (SBAR (WHNP (WDT that)) (S (NP (PRP we)) (VP (VBP propose))))))))))))) (. .))
(S (NP (PRP We)) (VP (VP (VBP prove) (NP (NP (DT the) (NN convergence)) (PP (IN of) (NP (DT this) (JJ novel) (NN scheme))))) (CC and) (ADVP (RB rigorously)) (VP (VB evaluate) (NP (PRP$ its) (JJ empirical) (NN performance)) (PP (IN on) (NP (NNP ResNet) (, ,) (NNP LSTM) (, ,) (CC and) (NNP Transformer))))) (. .))
(S (NP (PRP We)) (VP (VBP demonstrate) (SBAR (IN that) (S (PP (IN in) (NP (NP (DT a) (NN variety)) (PP (IN of) (NP (NNS experiments))))) (NP (DT the) (NN scheme)) (VP (VBZ allows) (S (VP (VBG scaling) (PP (TO to) (NP (ADJP (RB much) (JJR larger)) (NN batch) (NNS sizes))) (ADVP (IN than) (PP (IN before) (PP (IN whilst) (S (VP (VBG reaching) (CC or) (VBG surpassing) (NP (NNP SOTA) (NN accuracy))))))))))))) (. .))
