(S (NP (PRP$ Our) (NN community)) (VP (VBZ has) (ADVP (RB greatly)) (VP (VBN improved) (NP (NP (DT the) (NN efficiency)) (PP (IN of) (NP (NML (JJ deep) (NN learning)) (NNS applications)))) (, ,) (PP (VBG including) (PP (IN by) (S (VP (VBG exploiting) (NP (NN sparsity)) (PP (IN in) (NP (NNS inputs))))))))) (. .))
(S (NP (NP (JJS Most)) (PP (IN of) (NP (DT that) (NN work)))) (, ,) (ADVP (RB though)) (, ,) (VP (VBZ is) (PP (PP (IN for) (NP (NP (NN inference)) (, ,) (SBAR (WHADVP (WRB where)) (S (NP (NN weight) (NN sparsity)) (VP (VBZ is) (VP (VBN known) (ADVP (RB statically)))))))) (, ,) (CONJP (CC and) (HYPH /) (CC or)) (PP (IN for) (NP (JJ specialized) (NN hardware))))) (. .))
(S (NP (PRP We)) (VP (VBP propose) (NP (DT a) (NN scheme)) (S (VP (TO to) (VP (VB leverage) (NP (JJ dynamic) (NN sparsity)) (PP (IN during) (NP (NN training))))))) (. .))
(S (PP (IN In) (ADJP (JJ particular))) (, ,) (NP (PRP we)) (VP (VBP exploit) (SBAR (S (NP (NP (NNS zeros)) (VP (VBN introduced) (PP (IN by) (NP (DT the) (NN ReLU) (NN activation) (NN function))) (PP (IN to) (NP (DT both) (NN feature))))) (VP (VP (VBZ maps)) (CC and) (NP (PRP$ their) (NNS gradients)))))) (. .))
(S (S (NP (DT This)) (VP (VBZ is) (ADJP (JJ challenging)) (SBAR (IN because) (S (NP (DT the) (NN sparsity) (NN degree)) (VP (VBZ is) (ADJP (JJ moderate))))))) (CC and) (S (NP (NP (DT the) (NNS locations)) (PP (IN of) (NP (NNS zeros)))) (VP (VBP change) (PP (IN over) (NP (NN time))))) (. .))
(S (NP (PRP We)) (ADVP (RB also)) (VP (VBP rely) (ADVP (RB purely)) (PP (IN on) (NP (NN software)))) (. .))
(S (NP (PRP We)) (VP (VP (VBP identify) (NP (NP (NNS zeros)) (PP (IN in) (NP (DT a) (JJ dense) (NNS data) (NN representation)))) (PP (IN without) (S (VP (VBG transforming) (NP (DT the) (NNS data)))))) (CC and) (VP (VBZ performs) (NP (JJ conventional) (VBN vectorized) (NN computation)))) (. .))
(S (NP (NP (NNS Variations)) (PP (IN of) (NP (DT the) (NN scheme)))) (VP (VBP are) (ADJP (JJ applicable) (PP (IN to) (NP (NP (NP (DT all) (JJ major) (NNS components)) (PP (IN of) (NP (NN training))) (: :) (RB forward) (NP (NN propagation))) (, ,) (NP (NP (JJ backward) (NN propagation)) (PP (IN by) (NP (NNS inputs)))) (, ,) (CC and) (NP (NP (JJ backward) (NN propagation)) (PP (IN by) (NP (NNS weights)))))))) (. .))
(S (NP (PRP$ Our) (NN method)) (ADVP (RB significantly)) (VP (VBZ outperforms) (NP (NP (DT a) (ADJP (RB highly) (HYPH -) (VBN optimized)) (JJ dense) (JJ direct) (NN convolution)) (PP (IN on) (NP (JJ several) (JJ popular) (JJ deep) (JJ neural) (NNS networks))))) (. .))
(S (PP (IN At) (NP (JJ realistic) (NN sparsity))) (, ,) (NP (PRP we)) (VP (VBP speed) (PRT (RP up)) (NP (NP (DT the) (NN training)) (PP (IN of) (NP (NP (DT the) (JJ non-initial) (JJ convolutional) (NNS layers)) (PP (IN in) (NP (NP (NML (NML (NN VGG16)) (, ,) (NML (NNP ResNet) (HYPH -) (CD 34)) (, ,) (NML (NNP ResNet) (HYPH -) (CD 50)) (, ,) (CC and) (NML (NNP Fixup) (NNP ResNet))) (HYPH -) (CD 50)) (PP (IN by) (NP (NP (CD 2.19) (NN x)) (, ,) (NP (CD 1.37) (NN x)) (, ,) (NP (CD 1.31) (NN x)) (, ,) (CC and) (NP (CD 1.51) (SYM x))))))))) (PP (ADVP (RB respectively)) (IN on) (NP (DT an) (NNP Intel) (NML (NNP Skylake) (HYPH -) (NNP X)) (NN CPU)))) (. .))
