(S (NP (NP (JJ Recurrent) (NNP Neural) (NNP Networks)) (PRN (-LRB- -LRB-) (NP (NNP RNNs)) (-RRB- -RRB-))) (VP (VBP are) (VP (VBN used) (PP (IN in) (NP (JJ state-of-the-art) (NNS models))) (PP (IN in) (NP (NP (NNS domains)) (PP (JJ such) (IN as) (NP (NP (NN speech) (NN recognition)) (, ,) (NP (NN machine) (NN translation)) (, ,) (CC and) (NP (NN language) (NN modelling)))))))) (. .))
(S (NP (NN Sparsity)) (VP (VBZ is) (NP (NP (DT a) (NN technique)) (SBAR (S (VP (TO to) (VP (VB reduce) (NP (NP (NN compute) (CC and) (NN memory) (NNS requirements)) (PP (IN of) (NP (JJ deep) (NN learning) (NNS models)))))))))) (. .))
(S (NP (NNP Sparse) (NNP RNNs)) (VP (VBP are) (ADJP (JJR easier) (SBAR (S (VP (SBAR (S (VP (TO to) (VP (VB deploy))))) (PP (IN on) (NP (NP (NNS devices)) (CC and) (NP (JJ high-end) (NN server) (NNS processors))))))))) (. .))
(S (SBAR (RB Even) (IN though) (S (NP (JJ sparse) (NNS operations)) (VP (VBP need) (NP (NP (JJR less) (JJ compute) (CC and) (JJ memory)) (ADVP (NN relative) (PP (TO to) (NP (PRP$ their) (NN dense) (NNS counterparts)))))))) (, ,) (NP (NP (DT the) (JJ speed-up)) (VP (VBN observed) (PP (IN by) (S (VP (VBG using) (NP (JJ sparse) (NNS operations))))))) (VP (VBZ is) (ADJP (ADJP (JJR less)) (SBAR (IN than) (S (VP (VBN expected) (PP (IN on) (NP (JJ different) (NN hardware) (NNS platforms)))))))) (. .))
(S (SBAR (IN In) (NN order) (S (VP (TO to) (VP (VB address) (NP (DT this) (NN issue)))))) (, ,) (NP (PRP we)) (VP (VBP investigate) (NP (NP (NP (CD two) (JJ different) (NNS approaches)) (SBAR (S (VP (TO to) (VP (VB induce) (NP (NN block) (NN sparsity)) (PP (IN in) (NP (NNP RNNs)))))))) (: :) (S (VP (VP (NN pruning) (NP (NP (NNS blocks)) (PP (IN of) (NP (NNS weights)))) (PP (IN in) (NP (DT a) (NN layer)))) (CC and) (VP (VBG using) (NP (NN group) (VBZ lasso) (NN regularization)) (S (VP (TO to) (VP (VB create) (NP (NP (NNS blocks)) (PP (IN of) (NP (NNS weights))) (PP (IN with) (NP (NN zeros)))))))))))) (. .))
(S (S (VP (VBG Using) (NP (DT these) (NNS techniques)))) (, ,) (NP (PRP we)) (VP (VBP demonstrate) (SBAR (IN that) (S (NP (PRP we)) (VP (MD can) (VP (VB create) (NP (JJ block-sparse) (NNP RNNs)) (PP (IN with) (NP (NP (NN sparsity)) (VP (VBG ranging) (PP (PP (IN from) (NP (CD 80) (NN %))) (PP (TO to) (NP (CD 90) (NN %))))))) (PP (IN with) (NP (NP (JJ small) (NN loss)) (PP (IN in) (NP (NN accuracy)))))))))) (. .))
(S (NP (DT This)) (VP (VBZ allows) (S (NP (PRP us)) (VP (TO to) (VP (VB reduce) (NP (DT the) (NN model) (NN size)) (PP (IN by) (NP (QP (RB roughly) (CD 10x)))))))) (. .))
(S (ADVP (RB Additionally)) (, ,) (NP (PRP we)) (VP (MD can) (VP (VB prune) (NP (DT a) (JJR larger) (NN dense) (NN network)) (S (VP (TO to) (VP (VB recover) (NP (NP (DT this) (NN loss)) (PP (IN in) (NP (NN accuracy))))))) (SBAR (IN while) (S (VP (VP (VBG maintaining) (NP (JJ high) (NN block) (NN sparsity))) (CC and) (VP (VBG reducing) (NP (DT the) (JJ overall) (NN parameter) (NN count)))))))) (. .))
(S (NP (PRP$ Our) (NN technique)) (VP (VBZ works) (PP (IN with) (NP (NP (DT a) (NN variety)) (PP (PP (IN of) (NP (NN block) (VBZ sizes))) (PP (IN up) (PP (TO to) (NP (CD 32x32)))))))) (. .))
(S (NP (JJ Block-sparse) (NNP RNNs)) (VP (NN eliminate) (NP (NP (NNS overheads)) (VP (VBN related) (PP (TO to) (NP (NP (NNS data) (NN storage)) (CC and) (NP (JJ irregular) (NN memory) (NNS accesses)))))) (SBAR (IN while) (S (VP (VBG increasing) (NP (NP (NN hardware) (NN efficiency)) (PP (VBN compared) (PP (TO to) (NP (JJ unstructured) (NN sparsity))))))))) (. .))
