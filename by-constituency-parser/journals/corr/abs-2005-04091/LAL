(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (NP (PRP we)) (VP (VBP demonstrate) (NP (NP (DT a) (NN compiler)) (SBAR (WHNP (WDT that)) (S (VP (MD can) (VP (VB optimize) (NP (NP (NN sparse) (CC and) (JJ recurrent) (JJ neural) (NNS networks)) (, ,) (SBAR (WHNP (WHNP (DT both)) (WHPP (IN of) (WHNP (WDT which)))) (S (VP (VBP are) (ADVP (RB currently)) (PP (IN outside) (PP (IN of) (NP (NP (DT the) (NN scope)) (PP (IN of) (NP (VBG existing) (JJ neural) (NN network) (NNS compilers))))))))))))))) (PRN (-LRB- -LRB-) (S (NP (JJ sparse) (JJ neural) (NNS networks)) (ADVP (RB here)) (VP (VBP stand) (PP (IN for) (NP (NP (NNS networks)) (SBAR (WHNP (WDT that)) (S (VP (MD can) (VP (VB be) (VP (VBN accelerated) (PP (IN with) (NP (NN sparse) (NN tensor) (IN algebra) (NNS techniques)))))))))))) (-RRB- -RRB-))) (. .))
(S (NP (PRP$ Our) (NN demonstration)) (VP (VBZ includes) (NP (NP (NP (DT a) (NN mapping)) (PP (IN of) (NP (NN sparse) (CC and) (JJ recurrent) (JJ neural) (NNS networks))) (PP (TO to) (NP (DT the) (JJ polyhedral) (NN model)))) (PP (IN along) (IN with) (NP (NP (DT an) (NN implementation)) (PP (IN of) (NP (PRP$ our) (NN approach))) (PP (IN in) (NP (NP (NNP TIRAMISU)) (, ,) (NP (PRP$ our) (JJ state-of-the-art) (JJ polyhedral) (NN compiler)))))))) (. .))
(S (NP (PRP We)) (VP (VP (VBP evaluate) (NP (PRP$ our) (NN approach)) (PP (IN on) (NP (NP (DT a) (NN set)) (PP (IN of) (NP (JJ deep) (NN learning) (NNS benchmarks)))))) (CC and) (VP (VB compare) (NP (PRP$ our) (NNS results)) (PP (IN with) (NP (JJ hand-optimized) (JJ industrial) (NNS libraries))))) (. .))
(S (NP (PRP$ Our) (NNS results)) (VP (VBP show) (SBAR (IN that) (S (NP (PRP$ our) (NN approach)) (VP (VP (ADVP (IN at) (JJS least)) (NNS matches) (NP (NNP Intel) (NNP MKL-DNN))) (CC and) (VP (PP (IN in) (NP (DT some) (NNS cases))) (VBP outperforms) (NP (PRP it)) (PP (IN by) (NP (CD 5x))) (PRN (-LRB- -LRB-) (PP (IN on) (NP (NN multicore-CPUs))) (-RRB- -RRB-))))))) (. .))
