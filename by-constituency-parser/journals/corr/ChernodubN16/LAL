(S (NP (PRP We)) (VP (VBP propose) (NP (NP (DT a) (JJ novel) (NN activation) (NN function)) (SBAR (WHNP (IN that)) (S (VP (VBZ implements) (NP (NP (JJ piece-wise) (JJ orthogonal) (JJ non-linear) (NNS mappings)) (VP (VBN based) (PP (IN on) (NP (NNS permutations)))))))))) (. .))
(S (S (NP (PRP It)) (VP (VBZ is) (ADJP (ADJP (JJ straightforward) (SBAR (S (VP (TO to) (VP (VB implement)))))) (, ,) (CC and) (ADJP (RB very) (RB computationally) (JJ efficient))))) (, ,) (S (ADVP (RB also)) (NP (PRP it)) (VP (VBZ has) (NP (JJ little) (NN memory) (NNS requirements)))) (. .))
(S (S (NP (PRP We)) (VP (VBD tested) (NP (PRP it)) (PP (IN on) (NP (NP (CD two) (NN toy) (NNS problems)) (PP (IN for) (NP (NN feedforward) (CC and) (NN recurrent) (NNS networks))))))) (, ,) (S (NP (PRP it)) (VP (VBZ shows) (NP (NP (JJ similar) (NN performance)) (PP (TO to) (NP (VB tanh) (CC and) (NNP ReLU)))))) (. .))
(S (S (NP (NNP OPLU) (NN activation) (NN function)) (VP (VBZ ensures) (NP (NP (JJ norm) (NN preservance)) (PP (IN of) (NP (DT the) (JJ backpropagated) (NNS gradients)))))) (, ,) (S (ADVP (IN therefore)) (NP (PRP it)) (VP (VBZ is) (ADJP (RB potentially) (JJ good) (PP (IN for) (NP (NP (DT the) (NN training)) (PP (IN of) (NP (ADJP (JJ deep) (, ,) (ADJP (JJ extra) (NN deep)) (, ,) (CC and) (JJ recurrent)) (JJ neural) (NNS networks)))))))) (. .))
