(S (NP (PRP We)) (VP (VBP address) (NP (NP (DT the) (NN problem)) (PP (IN of) (S (VP (ADVP (RB simultaneously)) (VBG learning) (NP (DT a) (NX (JJ k-means) (NN clustering)) (CC and) (NX (JJ deep) (NN feature) (NN representation))) (PP (IN from) (NP (JJ unlabelled) (NNS data)))))) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ is) (PP (IN of) (NP (NN interest))) (PP (JJ due) (TO to) (NP (NP (DT the) (NN potential)) (PP (IN of) (NP (JJ deep) (NNS k-means))) (S (VP (TO to) (VP (VB outperform) (NP (JJ traditional) (JJ two-step) (NN feature) (NN extraction) (CC and) (JJ shallow-clustering) (NNS strategies)))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP achieve) (NP (DT this)) (PP (IN by) (S (VP (VBG developing) (NP (NP (DT a) (NN gradient-estimator)) (PP (IN for) (NP (DT the) (JJ non-differentiable) (NNS k-means) (JJ objective)))) (PP (IN via) (NP (DT the) (NNP Gumbel-Softmax) (NN reparameterisation) (NN trick))))))) (. .))
(S (PP (IN In) (NP (NP (NN contrast)) (PP (TO to) (NP (NP (JJ previous) (NNS attempts)) (PP (IN at) (NP (JJ deep) (NN clustering))))))) (, ,) (NP (PRP$ our) (JJ concrete) (NNS k-means) (NN model)) (VP (VP (MD can) (VP (VB be) (VP (VBN optimised) (PP (IN with) (NP (NP (NN respect)) (PP (TO to) (NP (DT the) (JJ canonical) (NNS k-means) (JJ objective)))))))) (CC and) (VP (VBZ is) (VP (ADVP (RB easily)) (VBN trained) (ADVP (NN end-to-end)) (PP (IN without) (S (VP (VBG resorting) (PP (TO to) (NP (VBG alternating) (NN optimisation))))))))) (. .))
(S (NP (PRP We)) (VP (VBP demonstrate) (NP (NP (DT the) (NN efficacy)) (PP (IN of) (NP (PRP$ our) (NN method)))) (PP (IN on) (NP (JJ standard) (VBG clustering) (NNS benchmarks)))) (. .))
