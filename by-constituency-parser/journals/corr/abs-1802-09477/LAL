(S (PP (IN In) (NP (NP (JJ value-based) (NN reinforcement) (VBG learning) (NNS methods)) (PP (JJ such) (IN as) (NP (JJ deep) (NN Q-learning))))) (, ,) (NP (NN function) (NN approximation) (NNS errors)) (VP (VBP are) (VP (VBN known) (S (VP (TO to) (VP (VB lead) (PP (TO to) (NP (NP (JJ overestimated) (NN value) (NNS estimates)) (CC and) (NP (JJ suboptimal) (NNS policies))))))))) (. .))
(S (NP (PRP We)) (VP (VP (VBP show) (SBAR (IN that) (S (NP (DT this) (NN problem)) (VP (VBZ persists) (PP (IN in) (NP (DT an) (JJ actor-critic) (NN setting))))))) (CC and) (VP (VB propose) (NP (NP (JJ novel) (NNS mechanisms)) (SBAR (S (VP (TO to) (VP (VB minimize) (NP (NP (PRP$ its) (NNS effects)) (PP (IN on) (NP (DT both) (NP (DT the) (NN actor)) (CC and) (NP (DT the) (NN critic)))))))))))) (. .))
(S (NP (PRP$ Our) (NN algorithm)) (VP (VBZ builds) (PP (IN on) (NP (NNP Double) (NNP Q-learning))) (, ,) (PP (IN by) (S (VP (VBG taking) (NP (NP (DT the) (JJ minimum) (NN value)) (PP (IN between) (NP (NP (DT a) (NN pair)) (PP (IN of) (NP (NNS critics)))))) (S (VP (TO to) (VP (VB limit) (NP (NN overestimation))))))))) (. .))
(S (NP (PRP We)) (VP (VP (VBP draw) (NP (NP (DT the) (NN connection)) (PP (IN between) (NP (NP (NN target) (NNS networks)) (CC and) (NP (NN overestimation) (NN bias)))))) (, ,) (CC and) (VP (JJS suggest) (S (VP (NN delaying) (NP (NN policy) (VBZ updates)) (S (VP (TO to) (VP (VP (VB reduce) (NP (JJ per-update) (NN error))) (CC and) (VP (ADVP (JJ further)) (VB improve) (NP (NN performance)))))))))) (. .))
(S (NP (PRP We)) (VP (VBP evaluate) (NP (PRP$ our) (NN method)) (PP (IN on) (NP (NP (DT the) (NN suite)) (PP (IN of) (NP (NNP OpenAI) (NN gym) (NNS tasks))))) (, ,) (S (VP (VBG outperforming) (NP (NP (DT the) (NN state)) (PP (IN of) (NP (DT the) (NN art)))) (PP (IN in) (NP (NP (DT every) (NN environment)) (VP (VBN tested))))))) (. .))
