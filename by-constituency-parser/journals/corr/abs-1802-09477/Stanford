(S (PP (IN In) (NP (NP (ADJP (NN value) (HYPH -) (VBN based)) (NML (NN reinforcement) (NN learning)) (NNS methods)) (PP (JJ such) (IN as) (NP (JJ deep) (NML (NN Q) (HYPH -) (NN learning)))))) (, ,) (NP (NN function) (NN approximation) (NNS errors)) (VP (VBP are) (VP (VBN known) (S (VP (TO to) (VP (VB lead) (PP (IN to) (NP (NP (VBN overestimated) (NN value) (NNS estimates)) (CC and) (NP (JJ suboptimal) (NNS policies))))))))) (. .))
(S (NP (PRP We)) (VP (VBP show) (SBAR (IN that) (S (NP (DT this) (NN problem)) (VP (VP (VBZ persists) (PP (IN in) (NP (DT an) (NML (NN actor) (HYPH -) (NN critic)) (NN setting)))) (CC and) (VP (VB propose) (NP (JJ novel) (NNS mechanisms)) (S (VP (TO to) (VP (VB minimize) (NP (PRP$ its) (NNS effects)) (PP (IN on) (NP (CC both) (NP (DT the) (NN actor)) (CC and) (NP (DT the) (NN critic)))))))))))) (. .))
(S (NP (PRP$ Our) (NN algorithm)) (VP (VBZ builds) (PP (IN on) (NP (JJ Double) (NML (NN Q) (HYPH -) (NN learning)))) (, ,) (PP (IN by) (S (VP (VBG taking) (NP (NP (DT the) (JJ minimum) (NN value)) (PP (IN between) (NP (NP (DT a) (NN pair)) (PP (IN of) (NP (NNS critics)))))) (S (VP (TO to) (VP (VB limit) (NP (NN overestimation))))))))) (. .))
(S (NP (PRP We)) (VP (VP (VBP draw) (NP (NP (DT the) (NN connection)) (PP (IN between) (NP (NP (NN target) (NNS networks)) (CC and) (NP (NN overestimation) (NN bias)))))) (, ,) (CC and) (VP (VBP suggest) (S (VP (VBG delaying) (NP (NN policy) (NNS updates)) (S (VP (TO to) (VP (VP (VB reduce) (NP (NML (PP (IN per) (HYPH -) (NP (NN update)))) (NN error))) (CC and) (ADVP (RB further)) (VP (VB improve) (NP (NN performance)))))))))) (. .))
(S (NP (PRP We)) (VP (VBP evaluate) (NP (PRP$ our) (NN method)) (PP (IN on) (NP (NP (DT the) (NN suite)) (PP (IN of) (NP (NNP OpenAI) (NN gym) (NNS tasks))))) (, ,) (S (VP (VBG outperforming) (NP (NP (DT the) (NN state)) (PP (IN of) (NP (NP (DT the) (NN art)) (PP (IN in) (NP (NP (DT every) (NN environment)) (VP (VBN tested)))))))))) (. .))
