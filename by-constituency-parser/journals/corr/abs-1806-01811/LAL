(S (S (NP (NP (JJ Adaptive) (NN gradient) (NNS methods)) (PP (JJ such) (IN as) (NP (NP (NNP AdaGrad)) (CC and) (NP (PRP$ its) (NNS variants))))) (VP (VBP update) (NP (NP (DT the) (NN stepsize)) (PP (IN in) (NP (JJ stochastic) (NN gradient) (NN descent)))) (PP (IN on) (NP (DT the) (NN fly))) (PP (VBG according) (PP (TO to) (NP (NP (DT the) (NNS gradients)) (VP (VBN received) (PP (IN along) (NP (DT the) (NN way))))))))) (: ;) (S (NP (JJ such) (NNS methods)) (VP (VBP have) (VP (VBN gained) (NP (NP (JJ widespread) (NN use)) (PP (IN in) (NP (JJ large-scale) (NN optimization)))) (PP (IN for) (NP (PRP$ their) (NN ability) (S (VP (TO to) (VP (VB converge) (ADVP (RB robustly)) (, ,) (PP (IN without) (NP (DT the) (NN need) (S (VP (TO to) (VP (VB fine-tune) (NP (DT the) (NN stepsize) (NN schedule))))))))))))))) (. .))
(S (ADVP (RB Yet)) (, ,) (NP (NP (DT the) (JJ theoretical) (NNS guarantees)) (PP (TO to) (NP (NN date))) (PP (IN for) (NP (NNP AdaGrad)))) (VP (VBP are) (PP (IN for) (NP (NN online) (CC and) (JJ convex) (NN optimization)))) (. .))
(S (NP (PRP We)) (VP (VBP bridge) (NP (DT this) (NN gap)) (PP (IN by) (S (VP (VBG providing) (NP (NP (JJ theoretical) (NNS guarantees)) (PP (IN for) (NP (NP (DT the) (NN convergence)) (PP (IN of) (NP (NNP AdaGrad))) (PP (IN for) (NP (NN smooth) (, ,) (JJ nonconvex) (NNS functions)))))))))) (. .))
(S (PP (IN In) (NP (JJ particular))) (, ,) (NP (NP (DT the) (NN convergence)) (PP (IN of) (NP (NNP AdaGrad-Norm)))) (VP (VBZ is) (ADJP (JJ robust) (PP (TO to) (NP (NP (DT the) (NN choice)) (PP (IN of) (NP (NP (DT all) (NNS hyper-parameters)) (PP (IN of) (NP (DT the) (NN algorithm)))))))) (, ,) (PP (IN in) (NP (NP (NN contrast)) (PP (TO to) (NP (NP (JJ stochastic) (NN gradient) (NN descent)) (SBAR (WHNP (WP$ whose) (NN convergence)) (S (VP (VBZ depends) (ADVP (RB crucially)) (PP (IN on) (S (VP (VBG tuning) (NP (DT the) (NN step-size)) (PP (TO to) (NP (NP (DT the) (PRN (-LRB- -LRB-) (ADJP (RB generally) (VBN unknown)) (-RRB- -RRB-)) (NNP Lipschitz) (NN smoothness) (NN constant)) (CC and) (NP (NP (NN level)) (PP (IN of) (NP (NP (JJ stochastic) (NN noise)) (PP (IN on) (NP (DT the) (NN gradient))))))))))))))))))) (. .))
(S (S (NP (JJ Extensive) (JJ numerical) (NNS experiments)) (VP (VBP are) (VP (VBN provided) (S (VP (TO to) (VP (VB corroborate) (NP (PRP$ our) (NN theory)))))))) (: ;) (S (ADVP (RB moreover)) (, ,) (NP (DT the) (NNS experiments)) (VP (VBP suggest) (SBAR (IN that) (S (NP (NP (DT the) (NN robustness)) (PP (IN of) (NP (NNP AdaGrad-Norm)))) (VP (NNS extends) (PP (TO to) (NP (NP (JJ state-of-the-art) (NNS models)) (PP (IN in) (NP (JJ deep) (NN learning))))) (, ,) (PP (IN without) (S (VP (VBG sacrificing) (NP (NN generalization)))))))))) (. .))
