(S (NP (NP (DT The) (JJ early) (NNS layers)) (PP (IN of) (NP (DT a) (JJ deep) (JJ neural) (NN net)))) (VP (VP (VBP have) (NP (DT the) (JJS fewest) (NNS parameters))) (, ,) (CC but) (VP (VB take) (PRT (RP up)) (NP (DT the) (RBS most) (NN computation)))) (. .))
(S (PP (IN In) (NP (DT this) (JJ extended) (NN abstract))) (, ,) (NP (PRP we)) (VP (VBP propose) (S (VP (TO to) (VP (ADVP (RB only)) (VB train) (NP (DT the) (JJ hidden) (NNS layers)) (PP (IN for) (NP (NP (DT a) (JJ set) (NN portion)) (PP (IN of) (NP (DT the) (NN training) (NN run))))) (, ,) (S (VP (VP (VBG freezing) (NP (PRP them)) (PRT (IN out)) (ADVP (NN one-by-one))) (CC and) (VP (VBG excluding) (NP (PRP them)) (PP (IN from) (NP (DT the) (NN backward) (NN pass)))))))))) (. .))
(S (PP (IN Through) (NP (NP (NNS experiments)) (PP (IN on) (NP (NNP CIFAR))))) (, ,) (NP (PRP we)) (VP (ADVP (RB empirically)) (VBP demonstrate) (SBAR (IN that) (S (NP (NNP FreezeOut)) (VP (NNS yields) (NP (NP (NNS savings)) (PP (IN of) (NP (ADJP (QP (IN up) (TO to) (CD 20)) (NN %)) (JJ wall-clock) (NN time)))) (PP (IN during) (NP (VBG training))) (PP (IN with) (NP (NP (NP (CD 3) (NN %) (NN loss)) (PP (IN in) (NP (NN accuracy))) (PP (IN for) (NP (NNP DenseNets)))) (, ,) (NP (NP (DT a) (ADJP (CD 20) (NN %)) (NN speedup)) (PP (IN without) (NP (NP (NN loss)) (PP (IN of) (NP (NN accuracy))))) (PP (IN for) (NP (NNP ResNets)))) (, ,) (CC and) (NP (NP (DT no) (NN improvement)) (PP (IN for) (NP (NNP VGG) (NNS networks)))))))))) (. .))
(S (NP (PRP$ Our) (NN code)) (VP (VBZ is) (ADJP (RB publicly) (JJ available)) (PP (IN at) (NP (DT this) (NN https) (NNP URL)))))
