(S (NP (PRP We)) (VP (VBP propose) (NP (NP (DT a) (VBN projected) (JJ semi-stochastic) (NN gradient) (NN descent) (NN method)) (PP (IN with) (NP (NP (NN mini-batch)) (PP (IN for) (S (VP (VBG improving) (NP (CC both) (NP (DT the) (JJ theoretical) (NN complexity)) (CC and) (NP (NP (JJ practical) (NN performance)) (PP (IN of) (NP (DT the) (JJ general) (JJ stochastic) (NN gradient) (NN descent) (NN method))))))))))) (PRN (-LRB- -LRB-) (NP (NNP SGD)) (-RRB- -RRB-))) (. .))
(S (NP (PRP We)) (VP (VBP are) (ADJP (JJ able) (S (VP (TO to) (VP (VB prove) (NP (JJ linear) (NN convergence)) (PP (IN under) (NP (JJ weak) (JJ strong) (NN convexity) (NN assumption)))))))) (. .))
(S (NP (DT This)) (VP (VBZ requires) (SBAR (S (NP (NP (DT no) (JJ strong) (NN convexity) (NN assumption)) (PP (IN for) (S (VP (VBG minimizing) (NP (NP (DT the) (NN sum)) (PP (IN of) (NP (JJ smooth) (NN convex)))))))) (VP (VBZ functions) (ADJP (JJ subject) (PP (IN to) (NP (NP (DT a) (JJ compact) (NN polyhedral) (NN set)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ remains) (ADJP (JJ popular) (PP (IN across) (NP (NML (NN machine) (NN learning)) (NN community)))))))))))))) (. .))
(S (NP (PRP$ Our) (NN PS2GD)) (VP (VP (VBZ preserves) (NP (NP (DT the) (NML (NML (NML (JJ low) (HYPH -) (NN cost)) (PP (IN per) (NP (NN iteration)))) (CC and) (NML (JJ high) (NN optimization))) (NN accuracy)) (PP (IN via) (NP (ADJP (NP (JJ stochastic) (NN gradient) (NN variance)) (HYPH -) (VBN reduced)) (NN technique))))) (, ,) (CC and) (VP (VBZ admits) (NP (DT a) (JJ simple) (JJ parallel) (NN implementation)) (PP (IN with) (NP (NNS mini-batches))))) (. .))
(S (ADVP (RB Moreover)) (, ,) (NP (NN PS2GD)) (VP (VBZ is) (ADVP (RB also)) (ADJP (JJ applicable) (PP (IN to) (NP (NP (JJ dual) (NN problem)) (PP (IN of) (NP (NN SVM)))))) (PP (IN with) (NP (NN hinge) (NN loss)))) (. .))
