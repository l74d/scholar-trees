(S (NP (PRP We)) (VP (VP (VBP introduce) (NP (DT a) (NN temperature)) (PP (IN into) (NP (DT the) (JJ exponential) (NN function)))) (CC and) (VP (VB replace) (NP (NP (DT the) (NN softmax) (NN output) (NN layer)) (PP (IN of) (NP (JJ neural) (NNS nets)))) (PP (IN by) (NP (DT a) (JJ high) (NN temperature) (NN generalization))))) (. .))
(S (ADVP (RB Similarly)) (, ,) (NP (NP (DT the) (NN logarithm)) (PP (IN in) (NP (NP (DT the) (JJ log) (NN loss)) (SBAR (S (NP (PRP we)) (VP (VBP use) (PP (IN for) (NP (NN training))))))))) (VP (VBZ is) (VP (VBN replaced) (PP (IN by) (NP (DT a) (JJ low) (NN temperature) (NN logarithm))))) (. .))
(S (PP (IN By) (S (VP (VBG tuning) (NP (DT the) (CD two) (NNS temperatures))))) (NP (PRP we)) (VP (VBP create) (NP (NP (NN loss) (NNS functions)) (SBAR (WHNP (WDT that)) (S (VP (VBP are) (ADJP (JJ non-convex)) (ADVP (RB already)) (PP (IN in) (NP (DT the) (JJ single) (NN layer) (NN case)))))))) (. .))
(S (SBAR (WHADVP (WRB When)) (S (VP (VBG replacing) (NP (NP (DT the) (JJ last) (NN layer)) (PP (IN of) (NP (DT the) (JJ neural) (NNS nets)))) (PP (IN by) (NP (NP (PRP$ our) (JJ bi-temperature) (NN generalization)) (PP (IN of) (NP (JJ logistic) (NN loss)))))))) (, ,) (NP (DT the) (NN training)) (VP (VBZ becomes) (ADJP (RBR more) (JJ robust) (PP (TO to) (NP (VB noise))))) (. .))
(S (NP (PRP We)) (VP (VP (VBP visualize) (NP (NP (DT the) (NN effect)) (PP (IN of) (S (VP (VBG tuning) (NP (DT the) (CD two) (NNS temperatures)) (PP (IN in) (NP (DT a) (JJ simple) (NN setting)))))))) (CC and) (VP (VB show) (NP (NP (DT the) (NN efficacy)) (PP (IN of) (NP (PRP$ our) (NN method)))) (PP (IN on) (NP (JJ large) (NNS data) (NNS sets))))) (. .))
(S (NP (PRP$ Our) (NN methodology)) (VP (VP (VBZ is) (VP (VBN based) (PP (IN on) (NP (NNP Bregman) (NNS divergences))))) (CC and) (VP (VBZ is) (ADJP (JJ superior) (PP (TO to) (NP (NP (DT a) (JJ related) (NN two-temperature) (NN method)) (VP (VBG using) (NP (DT the) (NNP Tsallis) (NN divergence)))))))) (. .))
