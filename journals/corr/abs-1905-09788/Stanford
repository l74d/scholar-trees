(S (S (NP (NN Dropout)) (VP (VBZ is) (NP (NP (DT a) (ADJP (JJ simple) (CC but) (JJ efficient)) (NN regularization) (NN technique)) (PP (IN for) (S (VP (VBG achieving) (NP (NP (JJR better) (NN generalization)) (PP (IN of) (NP (JJ deep) (JJ neural) (NNS networks))))))) (-LRB- -LRB-) (NP (NNS DNNs)) (-RRB- -RRB-)))) (: ;) (S (ADVP (RB hence)) (NP (PRP it)) (VP (VBZ is) (ADVP (RB widely)) (VP (VBN used) (PP (IN in) (NP (NNS tasks))) (PP (VBN based) (PP (IN on) (NP (NNS DNNs))))))) (. .))
(S (PP (IN During) (NP (NN training))) (, ,) (NP (NN dropout)) (ADVP (RB randomly)) (VP (VBZ discards) (NP (NP (DT a) (NN portion)) (PP (IN of) (NP (DT the) (NNS neurons)))) (S (VP (TO to) (VP (VB avoid) (NP (NN overfitting)))))) (. .))
(S (NP (DT This) (NN paper)) (VP (VBZ presents) (NP (NP (DT an) (VBN enhanced) (NN dropout) (NN technique)) (, ,) (SBAR (WHNP (WDT which)) (S (NP (PRP we)) (VP (VBP call) (NP (JJ multi-sample) (NN dropout))))) (, ,)) (PP (IN for) (NP (NP (DT both) (VBG accelerating) (NN training) (CC and) (VBG improving) (NN generalization)) (PP (IN over) (NP (DT the) (JJ original) (NN dropout)))))) (. .))
(S (NP (DT The) (JJ original) (NN dropout)) (VP (VBZ creates) (S (NP (NP (DT a) (ADJP (RB randomly) (VBN selected)) (NN subset)) (-LRB- -LRB-) (VP (VBN called) (NP (DT a) (NN dropout) (NN sample))) (-RRB- -RRB-))) (PP (IN from) (NP (NP (DT the) (NN input)) (PP (IN in) (NP (DT each) (NN training) (NN iteration))))) (SBAR (IN while) (S (NP (DT the) (JJ multi-sample) (NN dropout)) (VP (VBZ creates) (NP (JJ multiple) (NN dropout) (NNS samples)))))) (. .))
(S (S (NP (DT The) (NN loss)) (VP (VBZ is) (VP (VBN calculated) (PP (IN for) (NP (DT each) (NN sample)))))) (, ,) (CC and) (S (ADVP (RB then)) (NP (DT the) (NN sample) (NNS losses)) (VP (VBP are) (VP (VBN averaged) (S (VP (TO to) (VP (VB obtain) (NP (DT the) (JJ final) (NN loss)))))))) (. .))
(S (NP (DT This) (NN technique)) (VP (MD can) (VP (VB be) (ADVP (RB easily)) (VP (VBN implemented) (PP (IN without) (S (VP (VBG implementing) (NP (DT a) (JJ new) (NN operator)) (PP (IN by) (S (VP (VBG duplicating) (NP (NP (DT a) (NN part)) (PP (IN of) (NP (DT the) (NN network)))) (PP (IN after) (NP (DT the) (NN dropout) (NN layer))) (PP (IN while) (S (VP (VBG sharing) (NP (NP (DT the) (NNS weights)) (PP (IN among) (NP (DT the) (ADJP (VBN duplicated) (ADJP (RB fully) (JJ connected))) (NNS layers)))))))))))))))) (. .))
(S (NP (JJ Experimental) (NNS results)) (VP (VBD showed) (SBAR (IN that) (S (NP (JJ multi-sample) (NN dropout)) (ADVP (RB significantly)) (VP (VBZ accelerates) (NP (NN training)) (PP (IN by) (S (VP (VBG reducing) (NP (NP (DT the) (NN number)) (PP (IN of) (NP (NNS iterations)))) (PP (IN until) (S (NP (NP (NN convergence)) (PP (IN for) (NP (NML (NN image) (NN classification)) (NNS tasks)))) (VP (VBG using) (NP (DT the) (NML (NML (NNP ImageNet)) (, ,) (NML (NNP CIFAR) (HYPH -) (CD 10)) (, ,) (NML (NN CIFAR) (HYPH -) (CD 100)) (, ,) (CC and) (NML (NN SVHN))) (NNS datasets)))))))))))) (. .))
(S (NP (JJ Multi-sample) (NN dropout)) (VP (VBZ does) (RB not) (ADVP (RB significantly)) (VP (VB increase) (NP (NP (JJ computation) (NN cost)) (PP (IN per) (NP (NN iteration)))) (SBAR (IN because) (S (NP (NP (JJS most)) (PP (IN of) (NP (DT the) (NN computation) (NN time)))) (VP (VBZ is) (VP (VBN consumed) (PP (IN in) (NP (DT the) (NN convolution) (NNS layers))) (PP (IN before) (NP (NP (DT the) (NN dropout) (NN layer)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBP are) (RB not) (VP (VBN duplicated))))))))))))) (. .))
(S (NP (NNS Experiments)) (ADVP (RB also)) (VP (VBD showed) (SBAR (IN that) (S (NP (NP (NNS networks)) (VP (VBN trained) (S (VP (VBG using) (NP (JJ multi-sample) (NN dropout)))))) (VP (VBD achieved) (NP (NP (JJR lower) (NN error) (NNS rates) (CC and) (NNS losses)) (PP (IN for) (NP (CC both) (NP (DT the) (NN training) (NN set)) (CC and) (NP (NN validation) (NN set))))))))) (. .))
