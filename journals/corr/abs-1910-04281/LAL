(S (NP (DT This) (NN paper)) (VP (VBZ investigates) (SBAR (WHADVP (WRB how)) (S (VP (TO to) (VP (ADVP (VB efficiently)) (NN transition) (CC and) (JJ update) (NP (NP (NNS policies)) (, ,) (VP (VBD trained) (ADVP (RB initially)) (PP (IN with) (NP (NNS demonstrations)))) (, ,)) (S (VP (VBG using) (NP (JJ off-policy) (JJ actor-critic) (NN reinforcement) (NN learning))))))))) (. .))
(S (NP (NP (PRP It))) (VP (VBZ is) (ADJP (JJ well-known)) (SBAR (IN that) (S (NP (NP (NP (NNS techniques)) (VP (VBN based) (PP (IN on) (NP (NP (VBG Learning)) (PP (IN from) (NP (NNP Demonstrations))))))) (, ,) (NP (PP (IN for) (NP (NN example))) (NP (NN behavior) (NN cloning))) (, ,)) (VP (MD can) (VP (VB lead) (PP (TO to) (NP (JJ proficient) (NNS policies))) (PP (VBN given) (NP (JJ limited) (NNS data)))))))) (. .))
(S (ADVP (RB However)) (, ,) (NP (NP (PRP it))) (VP (VBZ is) (ADVP (RB currently)) (ADJP (JJ unclear)) (SBAR (WHADVP (WRB how)) (S (VP (TO to) (VP (ADVP (RB efficiently)) (VB update) (NP (DT that) (NN policy)) (S (VP (VBG using) (NP (JJ reinforcement) (NN learning)))))))) (SBAR (IN as) (S (NP (DT these) (NNS approaches)) (VP (VBP are) (ADVP (RB inherently)) (VP (VBG optimizing) (NP (JJ different) (JJ objective) (NNS functions))))))) (. .))
(S (NP (JJ Previous) (NNS works)) (VP (VBP have) (VP (VBN used) (NP (NP (NN loss) (NNS functions)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBP combine) (NP (IN behavior) (VBG cloning) (NNS losses)) (PP (IN with) (NP (JJ reinforcement) (VBG learning) (NNS losses))) (S (VP (TO to) (VP (VB enable) (NP (DT this) (NN update))))))))))) (. .))
(S (ADVP (RB However)) (, ,) (S (NP (NP (DT the) (NNS components)) (PP (IN of) (NP (DT these) (NN loss) (NNS functions)))) (VP (VBP are) (ADVP (RB often)) (VP (VBN set) (ADVP (RB anecdotally))))) (, ,) (CC and) (S (NP (PRP$ their) (JJ individual) (NNS contributions)) (VP (VBP are) (RB not) (VP (ADVP (RB well)) (RB understood)))) (. .))
(S (PP (IN In) (NP (DT this) (NN work))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (DT the) (NNP Cycle-of-Learning) (PRN (-LRB- -LRB-) (NNP CoL) (-RRB- -RRB-)) (NN framework)) (SBAR (WHNP (WDT that)) (S (VP (VBZ uses) (NP (NP (DT an) (JJ actor-critic) (NN architecture)) (PP (IN with) (NP (DT a) (NN loss) (NN function))) (SBAR (WHNP (IN that)) (S (VP (VBZ combines) (NP (NP (RB behavior) (JJ cloning)) (CC and) (NP (JJ 1-step) (JJ Q-learning) (NNS losses))) (PP (IN with) (NP (NP (DT an) (JJ off-policy) (JJ pre-training) (NN step)) (PP (IN from) (NP (JJ human) (NNS demonstrations)))))))))))))) (. .))
(S (NP (DT This)) (VP (VP (VBZ enables) (NP (NP (NN transition)) (PP (IN from) (NP (JJ behavior) (VBG cloning))) (PP (TO to) (NP (VB reinforcement) (VBG learning))) (PP (IN without) (NP (NN performance) (NN degradation))))) (CC and) (VP (VBZ improves) (NP (JJ reinforcement) (VBG learning)) (PP (IN in) (NP (NP (NNS terms)) (PP (IN of) (NP (NP (JJ overall) (NN performance)) (CC and) (NP (NN training) (NN time)))))))) (. .))
(S (ADVP (RB Additionally)) (, ,) (NP (PRP we)) (VP (ADVP (RB carefully)) (VBP study) (NP (NP (NP (DT the) (NN composition)) (PP (IN of) (NP (DT these) (VBN combined) (NNS losses)))) (CC and) (NP (NP (PRP$ their) (NN impact)) (PP (IN on) (NP (JJ overall) (NN policy) (NN learning)))))) (. .))
(S (NP (PRP We)) (VP (VBP show) (SBAR (IN that) (S (NP (PRP$ our) (NN approach)) (VP (VBZ outperforms) (NP (NP (JJ state-of-the-art) (NNS techniques)) (PP (IN for) (S (VP (VBG combining) (NP (NP (JJ behavior) (NN cloning)) (CC and) (NP (NN reinforcement) (NN learning))) (PP (IN for) (NP (ADJP (DT both) (NN dense) (CC and) (VB sparse)) (NN reward) (NNS scenarios))))))))))) (. .))
(S (NP (PRP$ Our) (NNS results)) (ADVP (RB also)) (VP (VBP suggest) (SBAR (IN that) (S (S (VP (ADVP (RB directly)) (VBG including) (NP (DT the) (JJ behavior) (NN cloning) (NN loss)) (PP (IN on) (NP (NN demonstration) (NNS data))))) (VP (VBZ helps) (S (VP (TO to) (VP (VP (VB ensure) (NP (JJ stable) (NN learning))) (CC and) (VP (NN ground) (NP (NN future) (NN policy) (NNS updates)))))))))) (. .))
