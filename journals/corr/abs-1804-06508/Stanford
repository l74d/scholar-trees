(S (NP (NP (JJ Convolutional) (JJ Neural) (NNS Networks)) (-LRB- -LRB-) (NP (NNS CNNs)) (-RRB- -RRB-)) (VP (VBP have) (VP (VBN begun) (S (VP (TO to) (VP (VB permeate) (NP (NP (DT all) (NNS corners)) (PP (IN of) (NP (NP (JJ electronic) (NN society)) (-LRB- -LRB-) (PP (IN from) (NP (NP (NN voice) (NN recognition)) (PP (IN to) (NP (NN scene) (NN generation))))) (-RRB- -RRB-)))) (PP (IN due) (PP (IN to) (NP (NP (PRP$ their) (NML (NML (JJ high) (NN accuracy)) (CC and) (NML (NN machine))) (NN efficiency)) (PP (IN per) (NP (NN operation))))))))))) (. .))
(S (PP (IN At) (NP (PRP$ their) (NN core))) (, ,) (NP (NNP CNN) (NNS computations)) (VP (VBP are) (VP (VBN made) (PRT (IN up)) (PP (IN of) (NP (NP (JJ multi-dimensional) (NN dot) (NNS products)) (PP (IN between) (NP (NML (NN weight) (CC and) (NN input)) (NNS vectors))))))) (. .))
(NP (NP (DT This) (NN paper) (NNS studies)) (SBAR (WHADVP (WRB how)) (S (NP (NN weight) (NN repetition)) (, ---) (SBAR (WHADVP (WRB when)) (S (NP (DT the) (JJ same) (NN weight)) (VP (VBZ occurs) (NP (JJ multiple) (NNS times)) (PP (IN in) (CC or) (IN across) (NP (NN weight) (NNS vectors)))))) (: ---) (VP (MD can) (VP (VB be) (VP (VBN exploited) (S (VP (TO to) (VP (VP (VB save) (NP (NN energy))) (CC and) (VP (VB improve) (NP (NN performance)) (PP (IN during) (NP (NNP CNN) (NN inference)))))))))))) (. .))
(S (NP (DT This)) (VP (VBZ generalizes) (NP (NP (DT a) (JJ popular) (NN line)) (PP (IN of) (NP (NN work) (S (VP (TO to) (VP (VB improve) (NP (NN efficiency)) (PP (IN from) (NP (NNP CNN) (NN weight) (NN sparsity))) (, ,) (SBAR (IN as) (S (S (VP (VBG reducing) (NP (NN computation)) (PP (IN due) (IN to) (NP (VBN repeated) (NML (CD zero) (NNS weights)))))) (VP (VBZ is) (NP (NP (DT a) (JJ special) (NN case)) (PP (IN of) (S (VP (VBG reducing) (NP (NN computation)) (PP (IN due) (IN to) (NP (VBN repeated) (NNS weights))))))))))))))))) (. .))
(S (S (VP (TO To) (VP (VB exploit) (NP (NN weight) (NN repetition))))) (, ,) (NP (DT this) (NN paper)) (VP (VBZ proposes) (SBAR (S (NP (DT a) (JJ new) (NNP CNN) (NN accelerator)) (VP (VBD called) (NP (DT the) (NNP Unique) (NML (NNP Weight) (NNP CNN)) (NNP Accelerator)) (PRN (-LRB- -LRB-) (NP (NNP UCNN)) (-RRB- -RRB-)))))) (. .))
(S (S (NP (NNP UCNN)) (VP (VBZ uses) (NP (NN weight) (NN repetition)) (PP (IN to) (VP (VB reuse) (NP (NP (NNP CNN) (NNS sub-computations) (PRN (-LRB- -LRB-) (NP (ADVP (FW e.g.)) (, ,) (NN dot) (NN products)) (-RRB- -RRB-))) (CC and) (S (VP (TO to) (VP (VB reduce) (NP (NNP CNN) (NN model) (NN size)) (SBAR (WHADVP (WRB when)) (S (VP (VBN stored) (PP (IN in) (NP (NML (RB off) (HYPH -) (NN chip)) (NNP DRAM)))))))))))))) (: ---) (FRAG (WHNP (NP (DT both)) (WHPP (IN of) (WHNP (WDT which)))) (PP (IN save) (NP (NN energy)))) (. .))
(S (NP (NNP UCNN)) (ADVP (RB further)) (VP (VBZ improves) (NP (NN performance)) (PP (IN by) (S (VP (VBG exploiting) (NP (NN sparsity)) (PP (IN in) (NP (NNS weights))))))) (. .))
(S (NP (PRP We)) (VP (VBP evaluate) (NP (NNP UCNN)) (PP (PP (IN with) (NP (DT an) (NML (NN accelerator) (HYPH -) (NN level)) (NML (NN cycle) (CC and) (NN energy)) (NN model))) (CC and) (PP (IN with) (NP (NP (DT an) (NN RTL) (NN implementation)) (PP (IN of) (NP (DT the) (NML (NNP UCNN) (NN processing)) (NN element))))))) (. .))
(S (PP (IN On) (NP (CD three) (JJ contemporary) (NNS CNNs))) (, ,) (NP (NN UCNN)) (VP (VBZ improves) (NP (NP (NP (NN throughput)) (HYPH -) (VP (VBN normalized) (NP (NN energy) (NN consumption)) (PP (IN by) (NP (NP (CD 1.2) (SYM x) (HYPH -) (NN 4x)) (, ,) (NP (ADJP (JJ relative) (PP (IN to) (NP (DT a) (RB similarly) (VBN provisioned) (NN baseline)))) (NN accelerator)))))) (SBAR (WHNP (WDT that)) (S (VP (VBZ uses) (NP (NML (NNP Eyeriss) (HYPH -) (NN style)) (NN sparsity) (NNS optimizations))))))) (. .))
(S (PP (IN At) (NP (DT the) (JJ same) (NN time))) (, ,) (NP (DT the) (NML (NNP UCNN) (NN processing)) (NN element)) (VP (VBZ adds) (NP (NP (QP (RB only) (CD 17) (HYPH -) (CD 24)) (NN %)) (ADJP (NP (NP (NN area)) (ADVP (RB overhead))) (JJ relative))) (PP (IN to) (NP (DT the) (JJ same) (NN baseline)))) (. .))
