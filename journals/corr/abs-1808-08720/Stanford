(S (S (VP (VBG Inducing) (NP (NN sparseness)) (PP (IN while) (S (VP (VBG training) (NP (JJ neural) (NNS networks))))))) (VP (VBZ has) (VP (VBN been) (VP (VBN shown) (S (VP (TO to) (VP (VB yield) (NP (NNS models)) (PP (IN with) (NP (NP (DT a) (JJR lower) (NN memory) (NN footprint)) (PP (CC but) (NP (JJ similar) (NN effectiveness))))) (PP (IN to) (NP (JJ dense) (NNS models))))))))) (. .))
(S (ADVP (RB However)) (, ,) (S (NP (NN sparseness)) (VP (VBZ is) (ADVP (RB typically)) (VP (VBN induced) (PP (VBG starting) (PP (IN from) (NP (DT a) (JJ dense) (NN model))))))) (, ,) (CC and) (S (ADVP (RB thus)) (NP (DT this) (NN advantage)) (VP (VBZ does) (RB not) (VP (VB hold) (PP (IN during) (NP (NN training)))))) (. .))
(S (NP (PRP We)) (VP (VBP propose) (NP (NNS techniques)) (S (VP (TO to) (VP (VB enforce) (NP (NP (NN sparseness) (JJ upfront)) (PP (IN in) (NP (ADJP (JJ recurrent)) (NN sequence) (NNS models)))) (PP (IN for) (NP (NN NLP) (NNS applications))) (, ,) (S (VP (TO to) (ADVP (RB also)) (VP (VB benefit) (NP (NN training))))))))) (. .))
(S (ADVP (RB First)) (, ,) (PP (IN in) (NP (NN language) (NN modeling))) (, ,) (NP (PRP we)) (VP (VBP show) (SBAR (WHADVP (WRB how)) (S (VP (TO to) (VP (VB increase) (NP (NP (JJ hidden) (NN state) (NNS sizes)) (PP (IN in) (NP (ADJP (JJ recurrent)) (NNS layers)))) (PP (IN without) (S (VP (VP (VBG increasing) (NP (NP (DT the) (NN number)) (PP (IN of) (NP (NNS parameters))))) (, ,) (VP (VBG leading) (PP (IN to) (NP (ADJP (JJR more) (JJ expressive)) (NNS models)))))))))))) (. .))
(S (ADVP (RB Second)) (, ,) (PP (IN for) (NP (NN sequence) (NN labeling))) (, ,) (NP (PRP we)) (VP (VBP show) (SBAR (IN that) (S (NP (NP (NN word) (NNS embeddings)) (PP (IN with) (NP (NP (VBN predefined) (NN sparseness) (NN lead)) (PP (IN to) (NP (NP (JJ similar) (NN performance)) (PP (IN as) (NP (JJ dense) (NNS embeddings)))))))) (, ,) (PP (IN at) (NP (NP (DT a) (NN fraction)) (PP (IN of) (NP (NP (DT the) (NN number)) (PP (IN of) (NP (ADJP (JJ trainable)) (NNS parameters)))))))))) (. .))
