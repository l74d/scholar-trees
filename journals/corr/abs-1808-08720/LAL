(S (S (VP (VBG Inducing) (NP (NN sparseness)) (SBAR (IN while) (S (VP (VBG training) (NP (JJ neural) (NNS networks))))))) (VP (VBZ has) (VP (VBN been) (VP (VBN shown) (S (VP (TO to) (VP (VB yield) (NP (NP (NNS models)) (PP (IN with) (NP (NP (DT a) (JJR lower) (NN memory) (NN footprint)) (CC but) (NP (NP (JJ similar) (NN effectiveness)) (PP (TO to) (NP (VB dense) (NNS models))))))))))))) (. .))
(S (ADVP (RB However)) (, ,) (S (NP (NN sparseness)) (VP (VBZ is) (ADVP (RB typically)) (VP (JJ induced) (S (VP (VBG starting) (PP (IN from) (NP (DT a) (NN dense) (NN model)))))))) (, ,) (CC and) (S (ADVP (RB thus)) (NP (DT this) (NN advantage)) (VP (VBZ does) (RB not) (VP (VB hold) (PP (IN during) (NP (NN training)))))) (. .))
(S (NP (PRP We)) (VP (VBP propose) (NP (NP (NNS techniques)) (SBAR (S (VP (TO to) (VP (VB enforce) (NP (JJ sparseness)) (ADVP (NN upfront)) (PP (IN in) (NP (JJ recurrent) (NN sequence) (NNS models))) (PP (IN for) (NP (NNP NLP) (NNS applications))))))) (, ,) (SBAR (S (VP (TO to) (ADVP (RB also)) (VP (VB benefit) (NP (NN training)))))))) (. .))
(S (ADVP (RB First)) (, ,) (PP (IN in) (NP (NN language) (NN modeling))) (, ,) (NP (PRP we)) (VP (VBP show) (SBAR (WHADVP (WRB how)) (S (VP (TO to) (VP (VB increase) (NP (NP (JJ hidden) (NN state) (NNS sizes)) (PP (IN in) (NP (NN recurrent) (NNS layers)))) (PP (IN without) (S (VP (VBG increasing) (NP (NP (DT the) (NN number)) (PP (IN of) (NP (NNS parameters))))))) (, ,) (S (VP (VBG leading) (PP (TO to) (NP (ADJP (RBR more) (JJ expressive)) (NNS models)))))))))) (. .))
(S (ADVP (JJ Second)) (, ,) (PP (IN for) (NP (NN sequence) (NN labeling))) (, ,) (NP (PRP we)) (VP (VBP show) (SBAR (IN that) (S (NP (NP (NN word) (NNS embeddings)) (PP (IN with) (NP (JJ predefined) (JJ sparseness)))) (VP (NN lead) (PP (TO to) (NP (NP (JJ similar) (NN performance)) (PP (IN as) (NP (JJ dense) (NNS embeddings))))) (, ,) (PP (IN at) (NP (NP (DT a) (NN fraction)) (PP (IN of) (NP (NP (DT the) (NN number)) (PP (IN of) (NP (JJ trainable) (NNS parameters))))))))))) (. .))
