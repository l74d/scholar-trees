(S (NP (JJ Recurrent) (JJ neural) (NNS networks)) (VP (VP (VBP show) (NP (NP (NML (NML (NN state)) (HYPH -) (PP (IN of) (HYPH -) (NP (DT the) (HYPH -) (NN art)))) (NNS results)) (PP (IN in) (NP (JJ many) (NN text) (NN analysis) (NNS tasks))))) (CC but) (VP (ADVP (RB often)) (VBP require) (NP (NP (DT a) (NN lot)) (PP (IN of) (NP (NN memory) (S (VP (TO to) (VP (VB store) (NP (PRP$ their) (NNS weights)))))))))) (. .))
(S (ADVP (RB Recently)) (S (VP (VBN proposed) (S (ADJP (JJ Sparse))))) (NP (NNP Variational) (NNP Dropout)) (VP (VBZ eliminates) (NP (NP (DT the) (NN majority)) (PP (IN of) (NP (NP (DT the) (NNS weights)) (PP (IN in) (NP (DT a) (NML (NN feed) (HYPH -) (JJ forward)) (JJ neural) (NN network)))))) (PP (IN without) (NP (NP (JJ significant) (NN loss)) (PP (IN of) (NP (NN quality)))))) (. .))
(S (NP (PRP We)) (VP (VBP apply) (NP (DT this) (NN technique)) (PP (IN to) (NP (ADJP (JJ sparsify) (JJ recurrent)) (JJ neural) (NNS networks)))) (. .))
(S (S (VP (TO To) (VP (VB account) (PP (IN for) (NP (ADJP (JJ recurrent)) (NNS specifics)))))) (NP (PRP we)) (ADVP (RB also)) (VP (VBP rely) (PP (IN on) (NP (NP (NNP Binary) (NNP Variational) (NNP Dropout)) (PP (IN for) (NP (NNP RNN)))))) (. .))
(S (NP (PRP We)) (VP (VBP report) (NP (NP (NML (CD 99.5) (NN %)) (NN sparsity) (NN level)) (PP (IN on) (NP (NP (NP (NN sentiment) (NN analysis) (NN task)) (PP (IN without) (NP (DT a) (NN quality) (NN drop)))) (CC and) (NP (ADVP (IN up) (PP (IN to) (NP (CD 87) (NN %)))) (NP (NN sparsity) (NN level)) (PP (IN on) (NP (NML (NN language) (NN modeling)) (NN task))))))) (PP (IN with) (NP (NP (JJ slight) (NN loss)) (PP (IN of) (NP (NN accuracy)))))) (. .))
