(S (NP (DT The) (NNP Adam) (NN algorithm)) (VP (VBZ has) (VP (VBN become) (S (ADJP (RB extremely) (JJ popular))) (PP (IN for) (NP (NML (JJ large) (HYPH -) (NN scale)) (NN machine) (NN learning))))) (. .))
(S (ADVP (RB However)) (, ,) (SBAR (IN whether) (S (NP (JJ strong) (NN convexity)) (VP (MD can) (VP (VB be) (VP (VBN utilized) (S (VP (TO to) (ADVP (RB further)) (VP (VB improve) (NP (DT the) (NN performance)))))))))) (VP (VBZ remains) (NP (DT an) (JJ open) (NN problem))) (. .))
(S (NP (DT The) (JJ essential) (NN idea)) (VP (VBZ is) (S (VP (TO to) (VP (VB maintain) (NP (DT a) (ADJP (RBR faster) (JJ decaying))) (PP (ADVP (RB yet)) (IN under) (NP (NP (VBN controlled) (NN step) (NN size)) (PP (IN for) (S (VP (VBG exploiting) (NP (JJ strong) (NN convexity))))))))))) (. .))
(S (PP (IN In) (NP (NN addition))) (, ,) (PP (IN under) (NP (NP (DT a) (JJ special) (NN configuration)) (PP (IN of) (NP (NNS hyperparameters))))) (, ,) (NP (PRP$ our) (NNP SAdam)) (VP (VBZ reduces) (PP (IN to) (NP (NP (NN SC) (HYPH -) (NN RMSprop)) (, ,) (NP (NP (DT a) (RB recently) (VBN proposed) (NN variant)) (PP (IN of) (NP (NP (NNP RMSprop)) (PP (IN for) (NP (NP (ADVP (RB strongly)) (JJ convex) (NNS functions)) (, ,) (SBAR (WHPP (IN for) (WHNP (WDT which))) (S (NP (PRP we)) (VP (VBP provide) (NP (NP (DT the) (JJ first) (ADJP (NN data) (HYPH -) (JJ dependent)) (JJ logarithmic) (NN regret)) (VP (VBN bound)))))))))))))) (. .))
(S (NP (NP (JJ Empirical) (NNS results)) (PP (IN on) (S (VP (VBG optimizing) (ADVP (RB strongly)) (NP (NP (JJ convex) (NNS functions)) (CC and) (NP (NN training) (JJ deep) (NNS networks))))))) (VP (VBP demonstrate) (NP (NP (DT the) (NN effectiveness)) (PP (IN of) (NP (PRP$ our) (NN method))))) (. .))
