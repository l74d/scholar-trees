(S (PP (IN In) (NP (NP (JJ data-parallel) (JJ synchronous) (NN training)) (PP (IN of) (NP (JJ deep) (JJ neural) (NNS networks))))) (, ,) (S (NP (NP (JJ different) (NNS devices)) (PRN (-LRB- -LRB-) (NP (NN replicas)) (-RRB- -RRB-))) (VP (VBP run) (NP (NP (DT the) (JJ same) (NN program)) (PP (IN with) (NP (NP (JJ different) (NNS partitions)) (PP (IN of) (NP (DT the) (NN training) (NN batch)))))))) (, ,) (CC but) (S (NP (VBD weight) (JJ update) (NN computation)) (VP (VBZ is) (VP (VBN repeated) (PP (IN on) (NP (DT all) (NN replicas))) (, ,) (SBAR (IN because) (S (NP (DT the) (NNS weights)) (VP (VBP do) (RB not) (VP (VB have) (NP (NP (DT a) (NN batch) (NN dimension)) (SBAR (S (VP (TO to) (VP (NN partition))))))))))))) (. .))
(S (NP (DT This)) (VP (MD can) (VP (VB be) (NP (NP (DT a) (NN bottleneck)) (PP (IN for) (NP (NN performance) (CC and) (NN scalability)))) (PP (IN in) (NP (NP (NP (JJ typical) (NN language) (NNS models)) (PP (IN with) (NP (JJ large) (NNS weights)))) (, ,) (CC and) (NP (NP (NNS models)) (PP (IN with) (NP (NP (JJ small) (JJ per-replica) (NN batch) (NN size)) (SBAR (WHNP (WDT which)) (S (VP (VBZ is) (ADJP (JJ typical)) (PP (IN in) (NP (JJ large-scale) (NN training))))))))))))) (. .))
(S (NP (DT This) (NN paper)) (VP (VBZ presents) (NP (NP (DT an) (NN approach)) (SBAR (S (VP (TO to) (VP (ADVP (RB automatically)) (VB shard) (NP (DT the) (NN weight) (JJ update) (NN computation)) (PP (IN across) (NP (NP (NN replicas)) (PP (IN with) (NP (JJ efficient) (NX (NX (NN communication) (NNS primitives)) (CC and) (NX (NNS data) (NN formatting))))))) (, ,) (S (VP (VBG using) (NP (NP (JJ static) (NN analysis)) (CC and) (NP (NP (NNS transformations)) (PP (IN on) (NP (DT the) (NN training) (NN computation) (NN graph))))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP show) (SBAR (S (NP (DT this) (NN technique)) (VP (VBZ achieves) (NP (NP (JJ substantial) (NNS speedups)) (PP (IN on) (NP (JJ typical) (NN image) (CC and) (NN language) (NNS models)))) (PP (IN on) (NP (NNP Cloud) (NNP TPUs))) (, ,) (S (VP (VBG requiring) (NP (NP (DT no) (NN change)) (PP (TO to) (NP (VB model) (NN code)))))))))) (. .))
(S (NP (DT This) (NN technique)) (VP (VBZ helps) (S (VP (VB close) (NP (NP (DT the) (NN gap)) (PP (IN between) (NP (ADJP (ADJP (ADJP (RB traditionally) (JJ expensive)) (PRN (-LRB- -LRB-) (S (VP (NNP ADAM))) (-RRB- -RRB-))) (CC and) (ADJP (ADJP (JJ cheap)) (PRN (-LRB- -LRB-) (NNP SGD) (-RRB- -RRB-)))) (NNS optimizers)))))) (, ,) (SBAR (IN as) (S (NP (PRP they)) (VP (VP (MD will) (ADVP (RB only)) (VP (VB take) (NP (NP (DT a) (JJ small) (NN part)) (PP (IN of) (NP (VBG training) (NN step) (NN time)))))) (CC and) (VP (VBP have) (NP (JJ similar) (JJ peak) (NN memory) (NN usage))))))) (. .))
(S (NP (PRP It)) (VP (VBD helped) (S (NP (PRP us)) (VP (TO to) (VP (VB achieve) (NP (JJ state-of-the-art) (NN training) (NN performance)) (PP (IN in) (NP (NP (NNP Google) (POS 's)) (NNP MLPerf) (CD 0.6) (NN submission))))))) (. .))
