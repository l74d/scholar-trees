(S (NP (NP (CD One) (JJ major) (NN challenge)) (PP (IN in) (NP (NN training) (JJ Deep) (JJ Neural) (NNS Networks)))) (VP (VBZ is) (VP (VBG preventing) (NP (NN overfitting)))) (. .))
(S (NP (NP (JJ Many) (NNS techniques)) (PP (JJ such) (IN as) (NP (NP (NNS data) (NN augmentation)) (CC and) (NP (NP (JJ novel) (NNS regularizers)) (PP (JJ such) (IN as) (NP (NNP Dropout))))))) (VP (VBP have) (VP (VBN been) (VP (VBN proposed) (S (VP (TO to) (VP (VB prevent) (NP (NN overfitting)) (PP (IN without) (S (VP (VBG requiring) (NP (NP (DT a) (JJ massive) (NN amount)) (PP (IN of) (NP (NN training) (NNS data))))))))))))) (. .))
(S (PP (IN In) (NP (DT this) (NN work))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (NP (DT a) (JJ new) (NN regularizer)) (VP (VBN called) (NP (NP (NNP DeCov)) (SBAR (WHNP (WDT which)) (S (VP (VBZ leads) (PP (IN to) (NP (NP (ADJP (RB significantly) (VBN reduced)) (NN overfitting)) (SBAR (-LRB- -LRB-) (IN as) (S (VP (VBN indicated) (PP (IN by) (NP (NP (DT the) (NN difference)) (PP (IN between) (NP (NML (NN train) (CC and) (NN val)) (NN performance))))))) (-RRB- -RRB-)))))))))) (, ,) (CC and) (NP (JJR better) (NN generalization)))) (. .))
(S (NP (PRP$ Our) (NN regularizer)) (VP (VBZ encourages) (NP (NP (ADJP (JJ diverse) (CC or) (JJ non-redundant)) (NNS representations)) (PP (IN in) (NP (JJ Deep) (JJ Neural) (NNS Networks)))) (PP (IN by) (S (VP (VBG minimizing) (NP (NP (DT the) (NN cross-covariance)) (PP (IN of) (NP (JJ hidden) (NNS activations)))))))) (. .))
(S (NP (DT This) (JJ simple) (NN intuition)) (VP (VP (VBZ has) (VP (VBN been) (VP (VBN explored) (PP (IN in) (NP (NP (DT a) (NN number)) (PP (IN of) (NP (JJ past) (NNS works)))))))) (CC but) (ADVP (RB surprisingly)) (VP (VBZ has) (ADVP (RB never)) (VP (VBN been) (VP (VBN applied) (PP (IN as) (NP (NP (DT a) (NN regularizer)) (PP (IN in) (NP (JJ supervised) (NN learning))))))))) (. .))
(S (NP (NP (NNS Experiments)) (PP (IN across) (NP (NP (DT a) (NN range)) (PP (IN of) (NP (NP (NNS datasets)) (CC and) (NP (NN network) (NNS architectures))))))) (VP (VBP show) (SBAR (IN that) (S (NP (DT this) (NN loss)) (ADVP (RB always)) (VP (VBZ reduces) (NP (NN overfitting)) (SBAR (IN while) (S (S (ADVP (RB almost)) (ADVP (RB always)) (VP (VBG maintaining) (CC or) (VBG increasing) (NP (NN generalization) (NN performance)))) (CC and) (ADVP (RB often)) (S (VP (VBG improving) (NP (NN performance)) (PP (IN over) (NP (NN Dropout))))))))))) (. .))
