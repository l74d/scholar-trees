(S (NP (JJ Deep) (NN reinforcement) (NN learning)) (VP (VBZ has) (VP (VBN been) (VP (ADVP (RB successfully)) (VBN applied) (PP (TO to) (NP (JJ several) (JJ visual-input) (NNS tasks))) (S (VP (VBG using) (NP (JJ model-free) (NNS methods))))))) (. .))
(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (DT a) (JJ model-based) (NN approach)) (SBAR (WHNP (IN that)) (S (VP (NNS combines) (S (VP (VBG learning) (NP (DT a) (JJ DNN-based) (NN transition) (NN model)))) (PP (IN with) (NP (NNP Monte) (NNP Carlo) (VBP tree) (NN search))) (S (VP (TO to) (VP (VB solve) (NP (NP (DT a) (JJ block-placing) (NN task)) (PP (IN in) (NP (NNP Minecraft)))))))))))) (. .))
(S (NP (PRP$ Our) (JJ learned) (NN transition) (NN model)) (VP (VBZ predicts) (NP (NP (NP (DT the) (JJ next) (NN frame)) (CC and) (NP (DT the) (NNS rewards))) (ADVP (NP (CD one) (NN step)) (RB ahead))) (PP (VBN given) (NP (NP (NP (DT the) (JJ last) (CD four) (NNS frames)) (PP (IN of) (NP (NP (DT the) (NN agent) (POS 's)) (JJ first-person-view) (NN image)))) (CC and) (NP (DT the) (JJ current) (NN action))))) (. .))
(S (ADVP (RB Then)) (NP (DT a) (NNP Monte) (NNP Carlo) (VBP tree) (NN search) (NN algorithm)) (VP (VBZ uses) (NP (DT this) (NN model)) (S (VP (TO to) (VP (VB plan) (NP (NP (DT the) (JJS best) (NN sequence)) (PP (IN of) (NP (NNS actions))) (SBAR (IN for) (S (NP (DT the) (NN agent)) (VP (TO to) (VP (VB perform)))))))))) (. .))
(S (PP (IN On) (NP (NP (DT the) (VBN proposed) (NN task)) (PP (IN in) (NP (NNP Minecraft))))) (, ,) (NP (PRP$ our) (JJ model-based) (NN approach)) (VP (VP (VBZ reaches) (NP (NP (DT the) (NN performance)) (ADJP (JJ comparable) (PP (TO to) (NP (DT the) (NNP Deep) (NNP Q-Network) (POS 's)))))) (, ,) (CC but) (VP (VBZ learns) (ADVP (RBR faster))) (CC and) (VP (PRN (, ,) (ADVP (RB thus)) (, ,)) (VBZ is) (ADJP (RBR more) (JJ training) (JJ sample) (NN efficient)))) (. .))
