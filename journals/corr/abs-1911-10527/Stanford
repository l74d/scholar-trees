(S (NP (NP (NP (JJ Deep) (NN reinforcement) (NN learning)) (-LRB- -LRB-) (NP (NN DRL)) (-RRB- -RRB-)) (PP (IN on) (NP (NP (NP (NNP Markov) (NN decision) (NNS processes)) (-LRB- -LRB-) (NP (NNS MDPs)) (-RRB- -RRB-)) (PP (IN with) (NP (JJ continuous) (NN action) (NNS spaces)))))) (VP (VBZ is) (ADVP (RB often)) (VP (VBN approached) (PP (IN by) (S (ADVP (RB directly)) (VP (VBG training) (NP (JJ parametric) (NNS policies)) (PP (IN along) (NP (NP (DT the) (NN direction)) (PP (IN of) (NP (VBN estimated) (NN policy) (NNS gradients)))))))) (PRN (-LRB- -LRB-) (NP (NNS PGs)) (-RRB- -RRB-)))) (. .))
(S (NP (JJ Previous) (NN research)) (VP (VBD revealed) (SBAR (IN that) (S (NP (NP (DT the) (NN performance)) (PP (IN of) (NP (DT these) (NN PG) (NNS algorithms)))) (VP (VBZ depends) (ADVP (RB heavily)) (PP (IN on) (NP (NP (NP (DT the) (NN bias)) (HYPH -) (NP (NN variance) (NNS tradeoffs))) (VP (VP (VBN involved) (PP (IN in) (NP (NN estimating)))) (CC and) (VP (VBG using) (NP (NNS PGs)))))))))) (. .))
(S (NP (NP (DT A) (JJ notable) (NN approach)) (PP (IN towards) (S (VP (VBG balancing) (NP (DT this) (NN tradeoff)))))) (VP (VBZ is) (S (VP (TO to) (VP (VB merge) (PP (DT both) (PP (IN on) (HYPH -) (NP (NN policy))) (CC and) (PP (IN off) (HYPH -) (NP (NN policy) (NN gradient) (NNS estimations)))))))) (. .))
(S (ADVP (RB However)) (NP (NP (VBG existing) (NN PG)) (VP (VBG merging) (NP (NNS methods)))) (VP (VP (MD can) (VP (VB be) (ADJP (NP (NN sample)) (JJ inefficient)))) (CC and) (VP (VBP are) (RB not) (ADJP (JJ suitable) (S (VP (TO to) (VP (VB train) (NP (JJ deterministic) (NNS policies)) (ADVP (RB directly)))))))) (. .))
(S (S (VP (TO To) (VP (VB address) (NP (DT these) (NNS issues))))) (, ,) (NP (DT this) (NN paper)) (VP (VP (VBZ introduces) (NP (JJ elite) (NNS PGs))) (CC and) (VP (VBZ strengthens) (NP (PRP$ their) (NN variance) (NN reduction) (NN effect)) (PP (IN by) (S (VP (VBG adopting) (NP (NML (NN elitism) (CC and) (NN policy)) (NN consolidation) (NNS techniques)) (S (VP (TO to) (VP (VB regularize) (NP (NN policy) (NN training)) (PP (VBN based) (PP (IN on) (NP (NP (NN policy) (JJ behavioral) (NN knowledge)) (VP (VBN extracted) (PP (IN from) (NP (JJ elite) (NNS trajectories))))))))))))))) (. .))
(S (ADVP (RB Meanwhile)) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (DT a) (NML (CD two) (HYPH -) (NN step)) (NN method)) (S (VP (TO to) (VP (VB merge) (NP (NP (JJ elite) (NNS PGs)) (CC and) (NP (JJ conventional) (NNS PGs))) (PP (IN as) (NP (NP (DT a) (JJ new) (NN extension)) (PP (IN of) (NP (NP (DT the) (JJ conventional) (NN interpolation)) (VP (VBG merging) (NP (NN method))))))))))) (. .))
(S (PP (IN At) (NP (CC both) (DT the) (ADJP (JJ theoretical) (CC and) (JJ experimental)) (NNS levels))) (, ,) (NP (PRP we)) (VP (VBP show) (SBAR (IN that) (S (NP (NP (DT both) (NML (CD two) (HYPH -) (NN step)) (NN merging)) (CC and) (NP (NN interpolation) (NN merging))) (VP (MD can) (VP (VB induce) (NP (ADJP (JJ varied) (NP (NN bias) (HYPH -) (NN variance))) (NNS tradeoffs)) (PP (IN during) (NP (NN policy) (NN training)))))))) (. .))
(S (NP (PRP They)) (VP (VBP enable) (S (NP (PRP us)) (VP (TO to) (ADVP (RB effectively)) (VP (VP (VB use) (NP (JJ elite) (NNS PGs))) (CC and) (VP (VB mitigate) (NP (PRP$ their) (NN performance) (NN impact)) (PP (IN on) (NP (VBN trained) (NNS policies)))))))) (. .))
(S (NP (PRP$ Our) (NNS experiments)) (ADVP (RB also)) (VP (VBP show) (SBAR (IN that) (S (NP (NML (CD two) (HYPH -) (NN step)) (NN merging)) (VP (MD can) (VP (VB outperform) (NP (NP (NN interpolation) (VBG merging)) (CC and) (NP (NP (JJ several) (NML (NML (NN state)) (HYPH -) (PP (IN of) (HYPH -) (NP (DT the) (HYPH -) (NN art)))) (NNS algorithms)) (PP (IN on) (NP (CD six) (NN benchmark) (NN control) (NNS tasks)))))))))) (. .))
