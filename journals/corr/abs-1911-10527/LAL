(S (NP (NP (JJ Deep) (NN reinforcement) (NN learning)) (PRN (-LRB- -LRB-) (NP (NNP DRL)) (-RRB- -RRB-)) (PP (IN on) (NP (NP (NNP Markov) (NN decision) (NNS processes)) (PRN (-LRB- -LRB-) (NP (NNP MDPs)) (-RRB- -RRB-)) (PP (IN with) (NP (JJ continuous) (NN action) (NNS spaces)))))) (VP (VBZ is) (ADVP (RB often)) (VP (VBN approached) (PP (IN by) (S (VP (ADVP (RB directly)) (VBG training) (NP (JJ parametric) (NNS policies)) (PP (IN along) (NP (NP (DT the) (NN direction)) (PP (IN of) (NP (NP (VBN estimated) (NN policy) (NNS gradients)) (PRN (-LRB- -LRB-) (NP (NNP PGs)) (-RRB- -RRB-))))))))))) (. .))
(S (NP (JJ Previous) (NN research)) (VP (VBD revealed) (SBAR (IN that) (S (NP (NP (DT the) (NN performance)) (PP (IN of) (NP (DT these) (NNP PG) (NN algorithms)))) (VP (VBZ depends) (ADVP (RB heavily)) (PP (IN on) (NP (NP (DT the) (NN bias-variance) (NNS tradeoffs)) (VP (VBN involved) (PP (IN in) (S (VP (VBG estimating) (CC and) (VBG using) (NP (NNP PGs)))))))))))) (. .))
(S (NP (NP (DT A) (JJ notable) (NN approach)) (PP (NNS towards) (S (VP (VBG balancing) (NP (DT this) (NN tradeoff)))))) (VP (VBZ is) (S (VP (TO to) (VP (VB merge) (NP (ADJP (DT both) (JJ on-policy) (CC and) (JJ off-policy)) (NN gradient) (NNS estimations)))))) (. .))
(S (ADVP (RB However)) (NP (VBG existing) (NNP PG) (VBG merging) (NNS methods)) (VP (VP (MD can) (VP (VB be) (ADJP (JJ sample) (NN inefficient)))) (CC and) (VP (VBP are) (RB not) (ADJP (JJ suitable) (S (VP (TO to) (VP (VB train) (NP (JJ deterministic) (NNS policies)) (ADVP (RB directly)))))))) (. .))
(S (S (VP (TO To) (VP (VB address) (NP (DT these) (NNS issues))))) (, ,) (NP (DT this) (NN paper)) (VP (VP (VBZ introduces) (NP (JJ elite) (NNP PGs))) (CC and) (VP (VP (VBZ strengthens) (NP (PRP$ their) (NN variance) (NN reduction) (NN effect))) (PP (IN by) (S (VP (VBG adopting) (NP (NN elitism) (CC and) (NN policy) (NN consolidation) (NNS techniques)) (S (VP (TO to) (VP (VB regularize) (NP (NP (NN policy) (NN training)) (VP (VBN based) (PP (IN on) (NP (NP (NN policy) (JJ behavioral) (NN knowledge)) (VP (VBN extracted) (PP (IN from) (NP (JJ elite) (NNS trajectories)))))))))))))))) (. .))
(S (ADVP (RB Meanwhile)) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (DT a) (JJ two-step) (NN method)) (SBAR (S (VP (TO to) (VP (VB merge) (NP (NP (JJ elite) (NNP PGs)) (CC and) (NP (JJ conventional) (NNP PGs)))))))) (PP (IN as) (NP (NP (DT a) (JJ new) (NN extension)) (PP (IN of) (NP (DT the) (JJ conventional) (NN interpolation) (VBG merging) (NN method)))))) (. .))
(S (PP (IN At) (NP (PDT both) (DT the) (ADJP (JJ theoretical) (CC and) (JJ experimental)) (NNS levels))) (, ,) (NP (PRP we)) (VP (VBP show) (SBAR (IN that) (S (NP (DT both) (NP (JJ two-step) (NN merging)) (CC and) (NP (NN interpolation) (NN merging))) (VP (MD can) (VP (VB induce) (NP (JJ varied) (NN bias-variance) (NNS tradeoffs)) (PP (IN during) (NP (NN policy) (NN training)))))))) (. .))
(S (NP (PRP They)) (VP (VBP enable) (S (NP (PRP us)) (VP (TO to) (VP (VP (ADVP (RB effectively)) (VB use) (NP (JJ elite) (NNP PGs))) (CC and) (VP (VB mitigate) (NP (NP (PRP$ their) (NN performance) (NN impact)) (PP (IN on) (NP (JJ trained) (NNS policies))))))))) (. .))
(S (NP (PRP$ Our) (NNS experiments)) (ADVP (RB also)) (VP (VBP show) (SBAR (IN that) (S (NP (JJ two-step) (NN merging)) (VP (MD can) (VP (VB outperform) (NP (NP (NN interpolation) (NN merging)) (CC and) (NP (JJ several) (JJ state-of-the-art) (NNS algorithms))) (PP (IN on) (NP (CD six) (NN benchmark) (NN control) (NNS tasks)))))))) (. .))
