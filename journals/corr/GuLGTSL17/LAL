(S (NP (NP (JJ Off-policy) (JJ model-free) (JJ deep) (NN reinforcement) (VBG learning) (NNS methods)) (VP (VBG using) (NP (ADJP (RB previously) (VBN collected)) (NNS data)))) (VP (MD can) (VP (VB improve) (NP (NP (JJ sample) (NN efficiency)) (PP (IN over) (NP (NN on-policy) (NN policy) (NN gradient) (NNS techniques)))))) (. .))
(S (PP (IN On) (NP (DT the) (JJ other) (NN hand))) (, ,) (NP (NN on-policy) (NN algorithms)) (VP (VBP are) (ADVP (RB often)) (ADJP (ADJP (RBR more) (JJ stable)) (CC and) (ADJP (JJR easier) (SBAR (S (VP (TO to) (VP (VB use)))))))) (. .))
(S (NP (DT This) (NN paper)) (VP (VBZ examines) (, ,) (ADVP (DT both) (RB theoretically) (CC and) (RB empirically)) (, ,) (NP (NP (NNS approaches)) (PP (TO to) (S (VP (VBG merging) (NP (ADJP (JJ on-) (CC and) (JJ off-policy)) (NNS updates)) (PP (IN for) (NP (JJ deep) (NN reinforcement) (NN learning)))))))) (. .))
(S (NP (JJ Theoretical) (NNS results)) (VP (VBP show) (SBAR (IN that) (S (NP (NP (NN off-policy) (NNS updates)) (PP (IN with) (NP (DT a) (NN value) (NN function) (NN estimator)))) (VP (MD can) (VP (VB be) (VP (VBN interpolated) (PP (IN with) (NP (NN on-policy) (NN policy) (NN gradient) (VBZ updates))) (SBAR (WP whilst) (S (ADVP (RB still)) (VP (VBG satisfying) (NP (NN performance) (NNS bounds))))))))))) (. .))
(S (NP (PRP$ Our) (NN analysis)) (VP (VBZ uses) (NP (VB control) (JJ variate) (NNS methods)) (S (VP (TO to) (VP (VB produce) (NP (NP (DT a) (NN family)) (PP (IN of) (NP (NN policy) (NN gradient) (NN algorithms)))) (, ,) (PP (IN with) (S (NP (JJ several) (ADJP (RB recently) (VBD proposed)) (NN algorithms)) (VP (VBG being) (NP (NP (JJ special) (NNS cases)) (PP (IN of) (NP (DT this) (NN family))))))))))) (. .))
(S (NP (PRP We)) (ADVP (RB then)) (VP (VP (VBP provide) (NP (NP (DT an) (JJ empirical) (NN comparison)) (PP (IN of) (NP (DT these) (NNS techniques))) (SBAR (IN with) (S (NP (DT the) (VBG remaining) (JJ algorithmic) (NNS details)) (ADJP (VBN fixed)))))) (, ,) (CC and) (VP (VB show) (SBAR (WHADVP (WRB how)) (S (NP (NP (JJ different) (NN mixing)) (PP (IN of) (NP (JJ off-policy) (NN gradient) (NNS estimates))) (PP (IN with) (NP (NN on-policy) (NNS samples)))) (VP (VBP contribute) (PP (TO to) (NP (NP (NNS improvements)) (PP (IN in) (NP (JJ empirical) (NN performance)))))))))) (. .))
(S (NP (DT The) (JJ final) (NN algorithm)) (VP (VP (VBZ provides) (NP (NP (DT a) (NN generalization) (CC and) (NN unification)) (PP (IN of) (NP (VBG existing) (JJ deep) (NN policy) (NN gradient) (NNS techniques))))) (, ,) (VP (VBZ has) (NP (NP (JJ theoretical) (NNS guarantees)) (PP (IN on) (NP (NP (DT the) (NN bias)) (VP (VBN introduced) (PP (IN by) (NP (NN off-policy) (NNS updates)))))))) (, ,) (CC and) (VP (VBZ improves) (PP (IN on) (NP (DT the) (JJ state-of-the-art) (JJ model-free) (JJ deep) (NNP RL) (NNS methods))) (PP (IN on) (NP (NP (DT a) (NN number)) (PP (IN of) (NP (NNP OpenAI) (NNP Gym) (JJ continuous) (NN control) (NNS benchmarks))))))) (. .))
