(S (S (VP (VBG Learning) (S (VP (TO to) (VP (VB walk) (PP (IN over) (NP (DT a) (NN graph))) (PP (IN towards) (NP (NP (NP (DT a) (NN target) (NN node)) (PP (IN for) (NP (DT a) (VBN given) (NN query)))) (CC and) (NP (DT a) (NN source) (NN node))))))))) (VP (VBZ is) (NP (NP (DT an) (JJ important) (NN problem)) (PP (IN in) (NP (NP (NNS applications)) (PP (JJ such) (IN as) (NP (NP (NN knowledge) (NN base) (NN completion)) (PRN (-LRB- -LRB-) (NP (NNP KBC)) (-RRB- -RRB-)))))))) (. .))
(S (NP (PRP It)) (VP (MD can) (VP (VB be) (VP (VBN formulated) (PP (IN as) (NP (NP (DT a) (NN reinforcement) (NN learning) (PRN (-LRB- -LRB-) (NNP RL) (-RRB- -RRB-)) (NN problem)) (PP (IN with) (NP (DT a) (VBN known) (NN state) (NN transition) (NN model)))))))) (. .))
(S (S (VP (TO To) (VP (VB overcome) (NP (NP (DT the) (NN challenge)) (PP (IN of) (NP (NN sparse) (NNS rewards))))))) (, ,) (NP (PRP we)) (VP (VBP develop) (NP (NP (DT a) (JJ graph-walking) (NN agent)) (VP (VBN called) (S (NP (NNP M-Walk)))) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ consists) (PP (IN of) (NP (NP (NP (DT a) (JJ deep) (NN recurrent) (JJ neural) (NN network)) (PRN (-LRB- -LRB-) (NP (NNP RNN)) (-RRB- -RRB-))) (CC and) (NP (NP (NNP Monte) (NNP Carlo) (NNP Tree) (NNP Search)) (PRN (-LRB- -LRB-) (NP (NNP MCTS)) (-RRB- -RRB-)))))))))) (. .))
(S (NP (DT The) (NNP RNN)) (VP (VP (VBZ encodes) (NP (NP (DT the) (NN state)) (PRN (-LRB- -LRB-) (INTJ (JJ i.e.)) (, ,) (NP (NP (NN history)) (PP (IN of) (NP (DT the) (JJ walked) (NN path)))) (-RRB- -RRB-)))) (CC and) (VP (VBZ maps) (NP (PRP it)) (ADVP (RB separately)) (PP (TO to) (NP (NP (DT a) (NN policy)) (CC and) (NP (NNS Q-values)))))) (. .))
(S (SBAR (IN In) (NN order) (S (VP (TO to) (VP (ADVP (RB effectively)) (VB train) (NP (DT the) (NN agent)) (PP (IN from) (NP (NN sparse) (NNS rewards))))))) (, ,) (NP (PRP we)) (VP (VBP combine) (NP (NNP MCTS)) (PP (IN with) (NP (DT the) (JJ neural) (NN policy))) (S (VP (TO to) (VP (VB generate) (NP (NP (NNS trajectories)) (VP (VBG yielding) (NP (ADJP (RBR more) (JJ positive)) (NNS rewards)))))))) (. .))
(S (PP (IN From) (NP (DT these) (NNS trajectories))) (, ,) (NP (DT the) (NN network)) (VP (VBZ is) (VP (VBN improved) (PP (IN in) (NP (DT an) (JJ off-policy) (NN manner))) (S (VP (VBG using) (NP (NP (NNP Q-learning)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ modifies) (NP (DT the) (NNP RNN) (NN policy)) (PP (IN via) (NP (NN parameter) (NN sharing))))))))))) (. .))
(S (NP (PRP$ Our) (VBN proposed) (NNP RL) (VBZ algorithm)) (VP (ADVP (RB repeatedly)) (VBZ applies) (NP (DT this) (JJ policy-improvement) (NN step)) (S (VP (TO to) (VP (VB learn) (NP (DT the) (NN model)))))) (. .))
(S (PP (IN At) (NP (NN test) (NN time))) (, ,) (NP (NNP MCTS)) (VP (VBZ is) (VP (VBN combined) (PP (IN with) (NP (DT the) (JJ neural) (NN policy))) (S (VP (TO to) (VP (VB predict) (NP (DT the) (NN target) (NN node))))))) (. .))
(S (NP (NP (JJ Experimental) (NNS results)) (PP (IN on) (NP (JJ several) (JJ graph-walking) (NNS benchmarks)))) (VP (VBP show) (SBAR (IN that) (S (NP (NNP M-Walk)) (VP (VBZ is) (ADJP (JJ able) (S (VP (TO to) (VP (VB learn) (NP (NP (JJR better) (NNS policies)) (PP (IN than) (NP (NP (JJ other) (JJ RL-based) (NNS methods)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBP are) (ADVP (RB mainly)) (VP (VBN based) (PP (IN on) (NP (NN policy) (NNS gradients)))))))))))))))))) (. .))
(S (NP (NNP M-Walk)) (ADVP (RB also)) (VP (VBZ outperforms) (NP (JJ traditional) (NNP KBC) (NNS baselines))) (. .))
