(S (S (VP (VBG Learning) (S (VP (TO to) (VP (VB walk) (PP (IN over) (NP (NP (DT a) (NN graph)) (PP (IN towards) (NP (DT a) (NN target) (NN node))))) (PP (IN for) (NP (DT a) (VBN given) (NN query)))))))) (CC and) (S (NP (DT a) (NN source) (NN node)) (VP (VBZ is) (NP (NP (DT an) (JJ important) (NN problem)) (PP (IN in) (NP (NP (NNS applications)) (PP (JJ such) (IN as) (NP (NN knowledge) (NN base) (NN completion) (PRN (-LRB- -LRB-) (NP (NN KBC)) (-RRB- -RRB-))))))))) (. .))
(S (NP (PRP It)) (VP (MD can) (VP (VB be) (VP (VBN formulated) (PP (IN as) (NP (DT a) (NML (NML (NN reinforcement) (NN learning)) (-LRB- -LRB-) (NML (NN RL)) (-RRB- -RRB-)) (NN problem))) (PP (IN with) (NP (DT a) (JJ known) (NN state) (NN transition) (NN model)))))) (. .))
(S (S (VP (TO To) (VP (VB overcome) (NP (NP (DT the) (NN challenge)) (PP (IN of) (NP (JJ sparse) (NNS rewards))))))) (, ,) (NP (PRP we)) (VP (VBP develop) (NP (NP (DT a) (NML (NN graph) (HYPH -) (VBG walking)) (NN agent)) (VP (VBN called) (NP (NP (NNP M) (HYPH -) (NNP Walk)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ consists) (PP (IN of) (NP (DT a) (NML (NML (ADJP (JJ deep) (JJ recurrent)) (JJ neural) (NML (NN network) (-LRB- -LRB-) (NN RNN) (-RRB- -RRB-))) (CC and) (NML (NNP Monte) (NNP Carlo))) (NNP Tree) (NNP Search))))))))) (PRN (-LRB- -LRB-) (NP (NNP MCTS)) (-RRB- -RRB-))) (. .))
(S (NP (DT The) (NNP RNN)) (VP (VP (VBZ encodes) (NP (NP (DT the) (NN state)) (-LRB- -LRB-) (NP (ADVP (FW i.e.)) (, ,) (NP (NN history)) (PP (IN of) (NP (DT the) (VBN walked) (NN path)))) (-RRB- -RRB-))) (CC and) (VP (VBZ maps) (NP (PRP it)) (ADVP (RB separately)) (UCP (PP (IN to) (NP (DT a) (NN policy))) (CC and) (ADVP (NML (NN Q) (HYPH -) (NNS values)))))) (. .))
(S (PP (IN In) (NP (NN order) (S (VP (TO to) (ADVP (RB effectively)) (VP (VB train) (NP (DT the) (NN agent)) (PP (IN from) (NP (JJ sparse) (NNS rewards)))))))) (, ,) (NP (PRP we)) (VP (VBP combine) (NP (NP (NNP MCTS)) (PP (IN with) (NP (DT the) (JJ neural) (NN policy)))) (S (VP (TO to) (VP (VB generate) (NP (NNS trajectories)) (S (VP (VBG yielding) (NP (ADJP (RBR more) (JJ positive)) (NNS rewards)))))))) (. .))
(S (PP (IN From) (NP (DT these) (NNS trajectories))) (, ,) (NP (DT the) (NN network)) (VP (VBZ is) (VP (VBN improved) (PP (IN in) (NP (NP (DT an) (NML (NN off) (HYPH -) (NN policy)) (NN manner)) (VP (VBG using) (NP (NP (NN Q) (HYPH -) (NN learning)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ modifies) (NP (NP (DT the) (NN RNN) (NN policy)) (PP (IN via) (NP (NN parameter) (NN sharing))))))))))))) (. .))
(S (NP (PRP$ Our) (VBN proposed) (NN RL) (NN algorithm)) (ADVP (RB repeatedly)) (VP (VBZ applies) (NP (DT this) (NML (NN policy) (HYPH -) (NN improvement)) (NN step)) (S (VP (TO to) (VP (VB learn) (NP (DT the) (NN model)))))) (. .))
(S (PP (IN At) (NP (NN test) (NN time))) (, ,) (NP (NNP MCTS)) (VP (VBZ is) (VP (VBN combined) (PP (IN with) (NP (DT the) (JJ neural) (NN policy))) (S (VP (TO to) (VP (VB predict) (NP (DT the) (NN target) (NN node))))))) (. .))
(S (NP (NP (JJ Experimental) (NNS results)) (PP (IN on) (NP (JJ several) (NML (NN graph) (HYPH -) (VBG walking)) (NNS benchmarks)))) (VP (VBP show) (SBAR (IN that) (S (NP (NNP M) (HYPH -) (NNP Walk)) (VP (VBZ is) (ADJP (JJ able) (S (VP (TO to) (VP (VB learn) (NP (JJR better) (NNS policies)) (PP (IN than) (NP (NP (ADJP (NP (JJ other) (NN RL)) (HYPH -) (VBN based)) (NNS methods)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBP are) (ADVP (RB mainly)) (VP (VBN based) (PP (IN on) (NP (NN policy) (NNS gradients))))))))))))))))) (. .))
(S (NP (NNP M) (HYPH -) (NNP Walk)) (ADVP (RB also)) (VP (VBZ outperforms) (NP (JJ traditional) (NN KBC) (NNS baselines))) (. .))
