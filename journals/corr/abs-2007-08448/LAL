(S (NP (PRP We)) (VP (VBP study) (NP (NP (NP (JJ bandit) (NN convex) (NN optimization) (NNS methods)) (SBAR (WHNP (WDT that)) (S (VP (VBP adapt) (PP (TO to) (NP (NP (DT the) (NN norm)) (PP (IN of) (NP (DT the) (NN comparator))))))))) (, ,) (NP (NP (DT a) (NN topic)) (SBAR (WHNP (WDT that)) (S (VP (VBZ has) (ADVP (RB only)) (VP (VBN been) (VP (VBN studied) (ADVP (IN before)) (PP (IN for) (NP (PRP$ its) (JJ full-information) (NN counterpart))))))))))) (. .))
(S (ADVP (RB Specifically)) (, ,) (NP (PRP we)) (VP (VBP develop) (NP (NP (JJ convex) (NN bandit) (NN algorithms)) (PP (IN with) (NP (JJ regret) (NNS bounds))) (SBAR (WHNP (WDT that)) (S (VP (VBP are) (ADJP (JJ small)) (SBAR (IN whenever) (S (NP (NP (DT the) (NN norm)) (PP (IN of) (NP (DT the) (NN comparator)))) (VP (VBZ is) (ADJP (JJ small)))))))))) (. .))
(S (NP (PRP We)) (ADVP (RB first)) (VP (VBP use) (NP (NP (NNS techniques)) (PP (IN from) (NP (DT the) (NN full-information) (NN setting)))) (S (VP (TO to) (VP (VB develop) (NP (NP (JJ comparator-adaptive) (NN algorithms)) (PP (IN for) (NP (JJ linear) (NNS bandits)))))))) (. .))
(S (ADVP (RB Then)) (, ,) (NP (PRP we)) (VP (VBP extend) (NP (DT the) (NNS ideas)) (S (VP (TO to) (VP (VP (VB convex) (NP (NNS bandits)) (PP (IN with) (NP (NNP Lipschitz)))) (CC or) (VP (JJ smooth) (NP (NN loss) (NNS functions))) (, ,) (S (VP (VBG using) (NP (NP (DT a) (JJ new) (JJ single-point) (NN gradient) (NN estimator)) (CC and) (NP (ADJP (RB carefully) (VBN designed)) (NN surrogate) (NNS losses))))))))) (. .))
