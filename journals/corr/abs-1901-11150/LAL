(S (NP (NP (JJ Adaptive) (JJ gradient-based) (NNS optimizers)) (PP (JJ such) (IN as) (NP (NNP Adagrad) (CC and) (NNP Adam)))) (VP (VBP are) (ADJP (JJ crucial) (PP (IN for) (S (VP (VBG achieving) (NP (NP (JJ state-of-the-art) (NN performance)) (PP (IN in) (NP (NP (NN machine) (NN translation)) (CC and) (NP (NN language) (NN modeling)))))))))) (. .))
(S (ADVP (RB However)) (, ,) (NP (DT these) (NNS methods)) (VP (VBP maintain) (NP (JJ second-order) (NNS statistics)) (PP (IN for) (NP (DT each) (NN parameter))) (, ,) (S (ADVP (RB thus)) (VP (VBG introducing) (NP (NP (JJ significant) (NN memory) (NNS overheads)) (SBAR (WHNP (WDT that)) (S (VP (VBP restrict) (NP (NP (NP (DT the) (NN size)) (PP (IN of) (NP (NP (DT the) (NN model)) (VP (VBG being) (VP (VBN used)))))) (CONJP (RB as) (RB well) (IN as)) (NP (NP (DT the) (NN number)) (PP (IN of) (NP (NNS examples))) (PP (IN in) (NP (DT a) (NN mini-batch)))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP describe) (NP (NP (DT an) (ADJP (JJ effective) (CC and) (JJ flexible)) (JJ adaptive) (NN optimization) (NN method)) (PP (IN with) (NP (ADJP (RB greatly) (VBN reduced)) (NN memory) (NN overhead))))) (. .))
(S (NP (PRP$ Our) (NN method)) (VP (VBZ retains) (NP (NP (DT the) (NNS benefits)) (PP (IN of) (NP (JJ per-parameter) (NN adaptivity)))) (SBAR (IN while) (S (VP (VBG allowing) (NP (NX (ADJP (RB significantly) (JJR larger)) (NX (NNS models))) (CC and) (NX (NN batch) (NNS sizes))))))) (. .))
(S (NP (PRP We)) (VP (VP (VBP give) (NP (NP (NN convergence) (NNS guarantees)) (PP (IN for) (NP (PRP$ our) (NN method))))) (, ,) (CC and) (VP (VB demonstrate) (NP (NP (PRP$ its) (NN effectiveness)) (PP (IN in) (S (VP (VBG training) (NP (ADJP (RB very) (JJ large)) (NN translation) (CC and) (NN language) (NNS models)) (PP (IN with) (NP (NP (ADJP (IN up) (TO to) (JJ 2-fold)) (NNS speedups)) (PP (VBN compared) (PP (TO to) (NP (DT the) (NN state-of-the-art)))))))))))) (. .))
