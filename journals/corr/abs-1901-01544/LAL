(S (NP (NP (PRP It))) (VP (VBZ is) (ADJP (JJ inevitable)) (S (VP (TO to) (VP (VB train) (NP (JJ large) (JJ deep) (NN learning) (NNS models)) (PP (IN on) (NP (NP (DT a) (JJ large-scale) (NN cluster)) (VP (VBD equipped) (PP (IN with) (NP (NNS accelerators) (NN system)))))))))) (. .))
(S (NP (JJ Deep) (NN gradient) (NN compression)) (VP (MD would) (VP (VP (VP (ADVP (RB highly)) (VB increase) (NP (DT the) (JJ bandwidth) (NN utilization))) (CC and) (VP (NN speed) (PRT (RP up)) (NP (DT the) (NN training) (NN process)))) (CC but) (ADJP (JJ hard) (SBAR (S (VP (TO to) (VP (VB implement) (PP (IN on) (NP (NN ring) (NN structure)))))))))) (. .))
(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (NP (PRP we)) (VP (VBP find) (SBAR (DT that) (S (NP (JJ redundant) (NN gradient) (CC and) (NN gradient) (NN staleness)) (VP (VBZ has) (NP (NP (JJ negative) (NN effect)) (PP (IN on) (NP (NN training)))))))) (. .))
(S (NP (PRP We)) (VP (VBP have) (VP (VBN observed) (SBAR (IN that) (S (PP (IN in) (NP (NP (JJ different) (NN epoch)) (CC and) (NP (JJ different) (NNS steps)))) (, ,) (NP (DT the) (JJ neural) (NNS networks)) (VP (VBP focus) (PP (IN on) (S (VP (VBG updating) (NP (NP (JJ different) (NNS layers)) (CC and) (NP (JJ different) (NNS parameters))))))))))) (. .))
(S (SBAR (IN In) (NN order) (S (VP (TO to) (VP (VP (VB save) (NP (JJR more) (NN communication) (NN bandwidth))) (CC and) (VP (VB preserve) (NP (NP (DT the) (NN accuracy)) (PP (IN on) (NP (NP (NN ring) (NN structure)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBP break) (NP (DT the) (NN restrict)) (SBAR (IN as) (S (NP (DT the) (JJ node)) (VP (NN increase))))))))))))))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (DT a) (JJ new) (NN algorithm)) (SBAR (S (VP (TO to) (VP (VB measure) (NP (NP (DT the) (NN importance)) (PP (IN of) (NP (NNS gradients))) (PP (IN on) (NP (NP (JJ large-scale) (NN cluster)) (VP (VBG implementing) (NP (VBG ring) (NN all-reduce)))))) (PP (VBN based) (PP (IN on) (NP (NP (DT the) (NN size)) (PP (IN of) (NP (NP (DT the) (NN ratio)) (PP (IN of) (NP (NN parameter) (NN calculation) (NN gradient))) (PP (TO to) (NP (VB parameter) (NN value)))))))))))))) (. .))
(S (NP (PRP$ Our) (NN importance) (VBD weighted) (VBG pruning) (NN approach)) (VP (VP (VBD achieved) (NP (NP (CD 64X) (CC and) (CD 58.8X)) (PP (IN of) (NP (JJ gradient) (NN compression) (NN ratio)))) (PP (IN on) (NP (NNP AlexNet)))) (CC and) (VP (NP (NNP ResNet50)) (PP (IN on) (NP (NNP ImageNet))))) (. .))
(S (ADVP (RB Meanwhile)) (, ,) (SBAR (IN in) (NN order) (S (VP (TO to) (VP (VB maintain) (NP (NP (DT the) (NN sparseness)) (PP (IN of) (NP (DT the) (NN gradient) (NN propagation)))))))) (, ,) (NP (PRP we)) (VP (ADVP (VBP randomly)) (IN broadcast) (NP (NP (DT the) (NN index)) (PP (IN of) (NP (JJ important) (NNS gradients)))) (PP (IN on) (NP (DT each) (NN node)))) (. .))
(SBAR (IN While) (S (NP (DT the) (VBG remaining) (NNS nodes)) (VP (VP (VBP are) (ADJP (JJ ready) (PP (IN for) (NP (DT the) (NN index) (NN gradient))))) (CC and) (VP (VB perform) (NP (JJ all-reduce) (NN update))))) (. .))
(S (NP (DT This)) (VP (MD would) (VP (VP (VB speed) (PRT (RP up)) (NP (NP (DT the) (NN convergence)) (PP (IN of) (NP (DT the) (NN model))))) (CC and) (VP (VB preserve) (NP (DT the) (NN training) (NN accuracy))))) (. .))
