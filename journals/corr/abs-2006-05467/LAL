(S (S (VP (VBG Pruning) (NP (NP (DT the) (NNS parameters)) (PP (IN of) (NP (JJ deep) (JJ neural) (NNS networks)))))) (VP (VBZ has) (VP (VBN generated) (NP (JJ intense) (NN interest)) (PP (JJ due) (TO to) (NP (NP (JJ potential) (NNS savings)) (PP (IN in) (NP (NN time) (, ,) (NN memory) (CC and) (NN energy))) (PP (DT both) (PP (IN during) (NP (NN training))) (CC and) (PP (IN at) (NP (NN test) (NN time)))))))) (. .))
(S (NP (JJ Recent) (NNS works)) (VP (VBP have) (VP (VBN identified) (, ,) (PP (IN through) (NP (NP (DT an) (JJ expensive) (NN sequence)) (PP (IN of) (NP (NN training) (CC and) (NN pruning) (NNS cycles))))) (, ,) (NP (NP (DT the) (NN existence)) (PP (PP (IN of) (NP (NP (VBG winning) (JJ lottery) (NNS tickets)) (CC or) (NP (NN sparse) (JJ trainable) (NNS subnetworks)))) (PP (IN at) (NP (NN initialization))))))) (. .))
(S (NP (DT This)) (VP (VBZ raises) (NP (NP (DT a) (JJ foundational) (NN question)) (: :) (SQ (MD can) (NP (PRP we)) (VP (VB identify) (NP (ADJP (RB highly) (JJ sparse)) (JJ trainable) (NNS subnetworks)) (PP (IN at) (NP (NN initialization))) (, ,) (PP (PP (IN without) (S (ADVP (RB ever)) (VP (NN training)))) (, ,) (CC or) (RB indeed) (PP (IN without) (S (ADVP (RB ever)) (VP (VBG looking) (PP (IN at) (NP (DT the) (NNS data))))))))))) (. ?))
(S (NP (PRP We)) (VP (VBP provide) (NP (NP (DT an) (JJ affirmative) (NN answer)) (PP (TO to) (NP (DT this) (NN question)))) (PP (IN through) (NP (ADJP (NN theory) (VBN driven)) (JJ algorithm) (NN design)))) (. .))
(S (NP (PRP We)) (ADVP (RB first)) (VP (VP (ADVP (RB mathematically)) (JJ formulate)) (CC and) (VP (ADVP (RB experimentally)) (VB verify)) (NP (NP (DT a) (NN conservation) (NN law)) (SBAR (WHNP (WDT that)) (S (VP (VBZ explains) (SBAR (WHADVP (WRB why)) (S (NP (NP (VBG existing) (JJ gradient-based) (NN pruning) (NN algorithms)) (PP (IN at) (NP (NN initialization)))) (VP (NN suffer) (PP (IN from) (NP (NP (NN layer-collapse)) (, ,) (NP (NP (DT the) (NN premature) (NN pruning)) (PP (IN of) (NP (DT an) (JJ entire) (NN layer))) (VP (VBG rendering) (S (NP (DT a) (NN network)) (ADJP (JJ untrainable))))))))))))))) (. .))
(S (NP (DT This) (NN theory)) (ADVP (RB also)) (VP (VBZ elucidates) (SBAR (WHADVP (WRB how)) (S (NP (JJ layer-collapse)) (VP (MD can) (VP (VB be) (VP (ADVP (RB entirely)) (VBN avoided)))))) (, ,) (S (VP (VBG motivating) (NP (NP (DT a) (JJ novel) (NN pruning) (NN algorithm)) (NP (NP (NNP Iterative) (NNP Synaptic) (NNP Flow) (NNP Pruning)) (PRN (-LRB- -LRB-) (NP (NNP SynFlow)) (-RRB- -RRB-))))))) (. .))
(S (NP (DT This) (NN algorithm)) (VP (MD can) (VP (VB be) (VP (VBN interpreted) (PP (IN as) (S (VP (VBG preserving) (NP (NP (DT the) (JJ total) (NN flow)) (PP (IN of) (NP (JJ synaptic) (NNS strengths))) (PP (IN through) (NP (DT the) (NN network))) (PP (IN at) (NP (NN initialization))) (ADJP (NN subject) (PP (TO to) (NP (DT a) (NN sparsity) (NN constraint))))))))))) (. .))
(S (ADVP (RB Notably)) (, ,) (NP (DT this) (NN algorithm)) (VP (VP (VBZ makes) (NP (NP (DT no) (NN reference)) (PP (TO to) (NP (DT the) (NN training) (NNS data))))) (CC and) (VP (ADVP (RB consistently)) (VBZ outperforms) (NP (VBG existing) (JJ state-of-the-art) (NN pruning) (NN algorithms)) (PP (IN at) (NP (NN initialization))) (PP (IN over) (NP (NP (DT a) (NN range)) (PP (IN of) (NP (NP (NP (NNS models)) (PRN (-LRB- -LRB-) (NP (NNP VGG) (CC and) (NNP ResNet)) (-RRB- -RRB-))) (, ,) (NP (NP (NNS datasets)) (PRN (-LRB- -LRB-) (NP (NP (NNP CIFAR-10/100)) (CC and) (NP (NNP Tiny) (NNP ImageNet))) (-RRB- -RRB-))) (, ,) (CC and) (NP (NP (NN sparsity) (NNS constraints)) (PRN (-LRB- -LRB-) (NP (QP (IN up) (TO to) (CD 99.9)) (NN percent)) (-RRB- -RRB-))))))))) (. .))
(S (ADVP (RB Thus)) (NP (PRP$ our) (JJ data-agnostic) (NN pruning) (NN algorithm)) (VP (VBZ challenges) (NP (DT the) (VBG existing) (NN paradigm) (SBAR (WDT that) (S (NP (VBZ data)) (VP (MD must) (VP (VB be) (VP (VBN used) (S (VP (TO to) (VP (VB quantify) (SBAR (WHNP (WDT which) (NNS synapses)) (S (VP (VBP are) (ADJP (JJ important))))))))))))))) (. .))
