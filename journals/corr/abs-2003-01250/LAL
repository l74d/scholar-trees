(S (NP (NP (VBG Spiking) (NNP Neural) (NNP Networks)) (PRN (-LRB- -LRB-) (NP (NNP SNNs)) (-RRB- -RRB-))) (VP (VBP are) (VP (VBG being) (VP (VBN explored) (PP (IN for) (NP (NP (PRP$ their) (JJ potential) (NN energy) (NN efficiency)) (VP (VBG resulting) (PP (IN from) (NP (NN sparse) (, ,) (JJ event-driven) (NNS computations))))))))) (. .))
(S (NP (JJ Many) (JJ recent) (NNS works)) (VP (VBP have) (VP (VBN demonstrated) (NP (NP (JJ effective) (NN backpropagation)) (PP (IN for) (NP (NP (JJ deep) (NNP Spiking) (NNP Neural) (NNP Networks)) (PRN (-LRB- -LRB-) (NP (NNP SNNs)) (-RRB- -RRB-))))) (PP (IN by) (S (VP (VBG approximating) (NP (NP (NNS gradients)) (PP (IN over) (NP (JJ discontinuous) (NX (NX (JJ neuron) (NNS spikes)) (CC or) (NX (VBG firing) (NNS events))))))))))) (. .))
(S (NP (NP (DT A) (JJ beneficial) (NN side-effect)) (PP (IN of) (NP (DT these) (JJ surrogate) (NN gradient) (VBG spiking) (NN backpropagation) (NN algorithms)))) (VP (VBZ is) (SBAR (IN that) (S (NP (NP (DT the) (NNS spikes)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBP trigger) (NP (JJ additional) (NNS computations))))) (, ,)) (VP (MD may) (ADVP (RB now)) (NP (NP (PRP themselves)) (VP (VB be) (VP (ADVP (RB directly)) (VBN considered) (PP (IN in) (NP (DT the) (NN gradient) (NNS calculations)))))))))) (. .))
(S (NP (PRP We)) (VP (VBP propose) (NP (NP (NP (DT an) (JJ explicit) (NN inclusion)) (PP (IN of) (NP (JJ spike) (NNS counts))) (PP (IN in) (NP (DT the) (NN loss) (NN function)))) (, ,) (ADVP (IN along) (PP (IN with) (NP (DT a) (JJ traditional) (NN error) (NN loss)))) (, ,) (S (VP (VBG causing) (S (NP (DT the) (NN backpropagation) (VBG learning) (NNS algorithms)) (VP (TO to) (VP (VB optimize) (NP (NN weight) (NNS parameters)) (PP (IN for) (NP (DT both) (NP (NN accuracy)) (CC and) (NP (VBG spiking) (NN sparsity))))))))))) (. .))
(S (SBAR (IN As) (S (VP (VBN supported) (PP (IN by) (NP (NP (VBG existing) (NN theory)) (PP (IN of) (NP (JJ over-parameterized) (JJ neural) (NNS networks)))))))) (, ,) (NP (EX there)) (VP (VBP are) (NP (NP (JJ many) (NN solution) (NNS states)) (PP (IN with) (NP (ADJP (RB effectively) (JJ equivalent)) (NN accuracy))))) (. .))
(S (PP (IN As) (NP (JJ such))) (, ,) (NP (NP (JJ appropriate) (NN weighting)) (PP (IN of) (NP (DT the) (CD two) (NN loss) (NNS goals))) (PP (IN during) (NP (NP (NN training)) (PP (IN in) (NP (DT this) (JJ multi-objective) (NN optimization) (NN process)))))) (VP (MD can) (VP (VB yield) (NP (NP (DT an) (NN improvement)) (PP (IN in) (NP (VBG spiking) (NN sparsity)))) (PP (IN without) (NP (NP (DT a) (JJ significant) (NN loss)) (PP (IN of) (NP (NN accuracy))))))) (. .))
(S (NP (PRP We)) (ADVP (RB additionally)) (VP (VBP explore) (NP (DT a) (JJ simulated) (JJ annealing-inspired) (NN loss) (NN weighting) (NN technique)) (S (VP (TO to) (VP (VB increase) (NP (NP (DT the) (NN weighting)) (PP (IN for) (NP (NN sparsity)))) (SBAR (IN as) (S (NP (NN training) (NN time)) (VP (NNS increases)))))))) (. .))
(S (NP (NP (PRP$ Our) (JJ preliminary) (NNS results)) (PP (IN on) (NP (DT the) (NNP Cifar-10) (NN dataset)))) (VP (NN show) (NP (NP (NP (ADJP (QP (RB up) (TO to) (CD 70.1)) (NN %)) (NN reduction)) (PP (IN in) (NP (VBG spiking) (NN activity))) (PP (IN with) (NP (NN iso-accuracy))) (PP (VBN compared) (PP (TO to) (NP (NP (DT an) (JJ equivalent) (NN SNN)) (VP (VBD trained) (PP (ADVP (RB only)) (IN for) (NP (NN accuracy)))))))) (CC and) (NP (NP (ADJP (QP (RB up) (TO to) (CD 73.3)) (NN %)) (NN reduction)) (PP (IN in) (NP (VBG spiking) (NN activity))) (SBAR (IN if) (S (VP (VBN allowed) (NP (NP (DT a) (NN trade-off)) (PP (IN of) (NP (NP (ADJP (CD 1) (NN %)) (NN reduction)) (PP (IN in) (NP (NN classification) (NN accuracy)))))))))))) (. .))
