(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (DT a) (JJ new) (JJ first-order) (JJ gradient-based) (NN algorithm)) (SBAR (S (VP (TO to) (VP (VB train) (NP (JJ deep) (JJ neural) (NNS networks)))))))) (. .))
(S (NP (PRP We)) (ADVP (RB first)) (VP (VBP introduce) (NP (NP (DT the) (NN sign) (NN operation)) (PP (IN of) (NP (JJ stochastic) (NNS gradients))) (PRN (-LRB- -LRB-) (PP (IN as) (PP (IN in) (NP (NP (JJ sign-based) (NNS methods)) (, ,) (NP (NN e.g.)) (, ,) (NP (NNP SIGN-SGD))))) (-RRB- -RRB-))) (PP (IN into) (NP (NP (NNP ADAM)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ is) (VP (VBN called) (PP (IN as) (NP (NN signADAM)))))))))) (. .))
(S (ADVP (RB Moreover)) (, ,) (SBAR (IN in) (NN order) (S (VP (TO to) (VP (VB make) (S (NP (NP (DT the) (NN rate)) (PP (IN of) (S (VP (VBG fitting) (NP (DT each) (NN feature)))))) (ADJP (JJR closer))))))) (, ,) (NP (PRP we)) (VP (VP (VBP define) (NP (DT a) (NN confidence) (NN function)) (SBAR (S (VP (TO to) (VP (VB distinguish) (NP (NP (JJ different) (NNS components)) (PP (IN of) (NP (NNS gradients))))))))) (CC and) (VP (VB apply) (NP (PRP it)) (PP (TO to) (NP (PRP$ our) (NN algorithm))))) (. .))
(S (NP (PRP It)) (VP (MD can) (VP (VB generate) (NP (NP (ADJP (JJR more) (NN sparse)) (NNS gradients)) (SBAR (IN than) (S (NP (VBG existing) (JJ algorithms)) (VP (VBP do))))))) (. .))
(S (NP (PRP We)) (VP (VBP call) (S (NP (DT this) (JJ new) (NN algorithm)) (NP (NN signADAM++)))) (. .))
(S (PP (IN In) (NP (JJ particular))) (, ,) (NP (DT both) (PRP$ our) (NN algorithms)) (VP (VP (VBP are) (ADJP (JJ easy) (SBAR (S (VP (TO to) (VP (VB implement))))))) (CC and) (VP (MD can) (VP (VB speed) (PRT (RP up)) (NP (NP (NN training)) (PP (IN of) (NP (JJ various) (JJ deep) (JJ neural) (NNS networks))))))) (. .))
(S (NP (NP (DT The) (NN motivation)) (PP (IN of) (NP (NN signADAM++)))) (VP (VBZ is) (ADVP (RB preferably)) (VP (VBG learning) (NP (NNS features)) (PP (IN from) (NP (DT the) (ADJP (RBS most) (JJ different)) (NNS samples))) (PP (IN by) (S (VP (VBG updating) (NP (ADJP (JJ large) (CC and) (JJ useful)) (NNS gradients)) (ADVP (NN regardless) (PP (IN of) (NP (NP (JJ useless) (NN information)) (PP (IN in) (NP (JJ stochastic) (NNS gradients))))))))))) (. .))
(S (NP (PRP We)) (ADVP (RB also)) (VP (VB establish) (NP (NP (JJ theoretical) (NN convergence) (NNS guarantees)) (PP (IN for) (NP (PRP$ our) (NN algorithms))))) (. .))
(S (NP (NP (JJ Empirical) (NNS results)) (PP (IN on) (NP (JJ various) (NNS datasets) (CC and) (NNS models)))) (VP (VBP show) (SBAR (IN that) (S (NP (PRP$ our) (JJ algorithms)) (VP (NN yield) (NP (NP (ADJP (RB much) (JJR better)) (NN performance)) (PP (IN than) (NP (NP (JJ many) (JJ state-of-the-art) (NNS algorithms)) (PP (VBG including) (NP (NNP SIGN-SGD) (, ,) (NNP SIGNUM) (CC and) (NNP ADAM)))))))))) (. .))
(S (NP (PRP We)) (ADVP (RB also)) (VP (VP (VBP analyze) (NP (DT the) (NN performance)) (PP (IN from) (NP (NP (JJ multiple) (NNS perspectives)) (PP (VBG including) (NP (DT the) (NN loss) (NN landscape)))))) (CC and) (VP (VB develop) (NP (NP (DT an) (JJ adaptive) (NN method)) (SBAR (S (VP (TO to) (ADVP (RBR further)) (VP (VB improve) (NP (NN generalization))))))))) (. .))
(S (NP (DT The) (NN source) (NN code)) (VP (VBZ is) (ADJP (JJ available)) (PP (IN at) (NP (DT this) (NN https) (NNP URL)))))
