(S (NP (PRP We)) (VP (VBP present) (NP (JJ Tiny) (HYPH -) (NN Transfer) (HYPH -) (NN Learning)) (PRN (-LRB- -LRB-) (NP (NN TinyTL)) (-RRB- -RRB-)) (, ,) (NP (DT an) (ADJP (JJ efficient) (PP (IN on) (HYPH -) (NP (NN device)))) (NN learning) (NN method) (S (VP (TO to) (VP (VB adapt) (NP (JJ pre-trained) (NNS models)) (PP (IN to) (NP (NP (ADJP (RB newly) (VBN collected)) (NNS data)) (PP (IN on) (NP (NN edge) (NNS devices)))))))))) (. .))
(S (NP (NP (JJ Different)) (PP (IN from) (NP (NP (NP (JJ conventional) (NML (NN transfer) (NN learning)) (NNS methods)) (SBAR (WHNP (WDT that)) (S (VP (RB fine) (HYPH -) (VB tune) (NP (DT the) (JJ full) (NN network)))))) (CC or) (NP (DT the) (JJ last) (NN layer) (PRN (, ,) (S (NP (NN TinyTL)) (VP (VBZ freezes) (NP (NP (DT the) (NNS weights)) (PP (IN of) (NP (DT the) (NN feature) (NN extractor)))) (SBAR (IN while) (S (ADVP (RB only)) (VP (VBG learning) (NP (DT the) (NNS biases))))))) (, ,)))))) (ADVP (RB thus)) (VP (VBZ does) (RB n't) (VP (VB require) (S (VP (VBG storing) (NP (NP (DT the) (JJ intermediate) (NNS activations)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ is) (NP (NP (DT the) (JJ major) (NN memory) (NN bottleneck)) (PP (IN for) (NP (NML (PP (IN on) (HYPH -) (NP (NN device)))) (NN learning)))))))))))) (. .))
(S (S (VP (TO To) (VP (VB maintain) (NP (DT the) (NN adaptation) (NN capacity)) (PP (IN without) (S (VP (VBG updating) (NP (DT the) (NNS weights)))))))) (, ,) (NP (NNP TinyTL)) (VP (VBZ introduces) (NP (ADJP (NN memory) (HYPH -) (JJ efficient)) (JJ lite) (JJ residual) (NNS modules)) (S (VP (TO to) (VP (VB refine) (NP (DT the) (NN feature) (NN extractor)) (PP (IN by) (S (VP (VBG learning) (NP (JJ small) (JJ residual) (NN feature) (NNS maps)) (PP (IN in) (NP (DT the) (NN middle)))))))))) (. .))
(S (S (ADVP (RB Besides)) (, ,) (PP (RB instead) (IN of) (S (VP (VBG using) (NP (DT the) (JJ same) (NN feature) (NN extractor))))) (, ,) (NP (NN TinyTL)) (VP (VBZ adapts) (NP (NP (DT the) (NN architecture)) (PP (IN of) (NP (DT the) (NN feature) (NN extractor)))) (S (VP (TO to) (VP (VB fit) (NP (JJ different) (NN target) (NNS datasets)) (PP (IN while) (S (VP (VBG fixing) (NP (DT the) (NNS weights)))))))))) (: :) (S (NP (NNP TinyTL)) (VP (VBZ pre-trains) (NP (NP (DT a) (JJ large) (NN super-net)) (SBAR (WHNP (WDT that)) (S (VP (VBZ contains) (NP (NP (JJ many) (NML (NN weight) (HYPH -) (VBN shared)) (NNS sub-nets)) (SBAR (WHNP (WDT that)) (S (VP (MD can) (ADVP (RB individually)) (VP (VB operate)))))))))))) (: ;) (S (NP (JJ different) (NN target) (NN dataset)) (VP (VBZ selects) (NP (DT the) (NN sub-net) (SBAR (WHNP (WDT that)) (S (NP (NP (JJS best) (NN match)) (NP (DT the) (NN dataset)))))))) (. .))
(S (NP (DT This) (NML (NN backpropagation) (HYPH -) (JJ free)) (JJ discrete) (NN sub-net) (NN selection)) (VP (VBZ incurs) (NP (DT no) (NN memory) (NN overhead))) (. .))
(S (S (NP (JJ Extensive) (NNS experiments)) (VP (VBP show) (SBAR (IN that) (S (NP (NNP TinyTL)) (VP (MD can) (VP (VB reduce) (NP (NP (NP (DT the) (NML (NN training) (NN memory)) (NN cost)) (PP (IN by) (NP (NP (NN order)) (PP (IN of) (NP (NN magnitude)))))) (-LRB- -LRB-) (NP (QP (IN up) (IN to) (CD 13.3)))))))))) (S (LST (LS x) (-RRB- -RRB-)) (VP (VP (PP (IN without)) (VBG sacrificing) (NP (NN accuracy)) (PP (VBN compared) (PP (IN to) (NP (JJ fine) (HYPH -) (NN tuning))))) (NP (DT the) (ADJP (JJ full)) (NN network)))) (. .))
