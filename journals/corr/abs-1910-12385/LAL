(S (NP (NP (JJ Mixed) (NN precision) (NN training)) (PRN (-LRB- -LRB-) (NP (NNP MPT)) (-RRB- -RRB-))) (VP (VBZ is) (VP (VBG becoming) (NP (NP (DT a) (JJ practical) (NN technique)) (SBAR (S (VP (TO to) (VP (VB improve) (NP (NP (DT the) (NN speed) (CC and) (NN energy) (NN efficiency)) (PP (IN of) (S (VP (VBG training) (NP (JJ deep) (JJ neural) (NNS networks)))))) (PP (IN by) (S (VP (VBG leveraging) (NP (NP (DT the) (NN fast) (NN hardware) (NN support)) (PP (IN for) (NP (NNP IEEE) (NN half-precision) (VBG floating) (NN point))) (SBAR (WHNP (WDT that)) (S (VP (VBZ is) (ADJP (JJ available) (PP (IN in) (NP (VBG existing) (NNP GPUs)))))))))))))))))) (. .))
(S (NP (NNP MPT)) (VP (VBZ is) (ADVP (RB typically)) (VP (VBN used) (PP (IN in) (NP (NP (NN combination)) (PP (IN with) (NP (NP (DT a) (NN technique)) (VP (VBN called) (S (NP (NN loss) (NN scaling)))) (, ,) (SBAR (WHNP (WDT that)) (S (VP (VBZ works) (PP (IN by) (S (VP (VBG scaling) (PRT (RP up)) (NP (DT the) (NN loss) (NN value)) (ADVP (RP up)) (PP (IN before) (NP (NP (DT the) (NN start)) (PP (IN of) (NP (NN backpropagation))))) (SBAR (IN in) (NN order) (S (VP (TO to) (VP (VB minimize) (NP (NP (DT the) (NN impact)) (PP (IN of) (NP (JJ numerical) (NN underflow))) (PP (IN on) (NP (NN training)))))))))))))))))))) (. .))
(S (ADVP (RB Unfortunately)) (, ,) (S (NP (VBG existing) (NNS methods)) (VP (VBP make) (S (NP (DT this) (NN loss) (NN scale) (NN value)) (NP (NP (DT a) (NN hyperparameter)) (SBAR (WHNP (WDT that)) (S (VP (VBZ needs) (S (VP (TO to) (VP (VB be) (VP (VBN tuned) (ADVP (JJ per-model))))))))))))) (, ,) (CC and) (S (NP (DT a) (JJ single) (NN scale)) (VP (MD can) (RB not) (VP (VB be) (VP (VBN adapted) (PP (TO to) (NP (NP (JJ different) (NNS layers)) (PP (IN at) (NP (JJ different) (NN training) (NNS stages))))))))) (. .))
(S (NP (PRP We)) (VP (VBP introduce) (NP (NP (DT a) (ADJP (NN loss) (JJ scaling-based)) (NN training) (NN method)) (VP (VBN called) (S (NP (JJ adaptive) (NN loss) (NN scaling)))) (SBAR (WHNP (WDT that)) (S (VP (VBZ makes) (S (NP (NNP MPT)) (ADJP (ADJP (JJR easier)) (CC and) (ADJP (RBR more) (JJ practical)) (SBAR (S (VP (TO to) (VP (VB use))))))) (, ,) (PP (IN by) (S (VP (VBG removing) (NP (DT the) (NN need) (S (VP (TO to) (VP (VB tune) (NP (DT a) (JJ model-specific) (NN loss) (NN scale) (NN hyperparameter)))))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP achieve) (NP (DT this)) (PP (IN by) (S (VP (VBG introducing) (NP (NP (JJ layer-wise) (NN loss) (NN scale) (NNS values)) (SBAR (WHNP (WDT which)) (S (VP (VBP are) (VP (ADVP (RB automatically)) (VBN computed) (PP (IN during) (NP (NN training))) (S (VP (TO to) (VP (VB deal) (PP (IN with) (NP (JJ underflow))) (ADVP (ADVP (RBR more) (RB effectively)) (PP (IN than) (NP (VBG existing) (NNS methods)))))))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP present) (NP (NP (JJ experimental) (NNS results)) (PP (IN on) (NP (NP (DT a) (NN variety)) (PP (IN of) (NP (NNS networks) (CC and) (NNS tasks))))) (SBAR (WHNP (WDT that)) (S (VP (VBP show) (SBAR (S (NP (PRP$ our) (NN approach)) (VP (MD can) (VP (VP (VB shorten) (NP (NP (DT the) (NN time)) (PP (TO to) (NP (NN convergence))))) (CC and) (VP (VB improve) (NP (NN accuracy))) (PP (VBN compared) (PP (TO to) (NP (DT the) (NX (VBG existing) (NX (JJ state-of-the-art) (NNP MPT))) (CC and) (NX (NN single-precision) (NN floating) (NN point)))))))))))))))
