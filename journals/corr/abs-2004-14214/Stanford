(S (NP (NP (NN Implementation)) (PP (IN of) (NP (NP (VBN quantized) (JJ neural) (NNS networks)) (PP (IN on) (NP (VBG computing) (NN hardware)))))) (VP (VBZ leads) (PP (IN to) (NP (NP (JJ considerable) (NN speed) (RP up)) (CC and) (NP (NN memory) (NN saving))))) (. .))
(S (ADVP (RB However)) (, ,) (S (NP (VBN quantized) (JJ deep) (NNS networks)) (VP (VBP are) (ADJP (JJ difficult) (S (VP (TO to) (VP (VB train))))))) (CC and) (S (NP (NP (NN batch)) (PP (SYM ~) (NP (NN normalization) (-LRB- -LRB-) (NNP BatchNorm) (-RRB- -RRB-) (NN layer)))) (VP (VBZ plays) (NP (NP (DT an) (JJ important) (NN role)) (PP (IN in) (NP (NP (NN training) (JJ full) (HYPH -) (NN precision)) (CC and) (NP (VBN quantized) (NNS networks))))))) (. .))
(S (S (NP (NP (JJS Most) (NNS studies)) (PP (IN on) (NP (NNP BatchNorm)))) (VP (VBP are) (VP (VBN focused) (PP (IN on) (NP (NML (JJ full) (HYPH -) (NN precision)) (NNS networks)))))) (, ,) (CC and) (S (NP (EX there)) (VP (VBZ is) (NP (NP (JJ little) (NN research)) (PP (IN in) (S (VP (VBG understanding) (S (NP (NNP BatchNorm)) (VP (VB affect) (PP (IN in) (NP (NP (VBN quantized) (NN training)) (SBAR (WHNP (WDT which)) (S (NP (PRP we)) (VP (VBP address) (ADVP (RB here))))))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP show) (SBAR (S (NP (NNP BatchNorm)) (VP (VBZ avoids) (NP (NP (NN gradient) (NN explosion)) (SBAR (WHNP (WDT which)) (S (VP (VBZ is) (UCP (ADJP (JJ counter-intuitive)) (CC and) (ADVP (RB recently)) (VP (VBN observed) (PP (IN in) (NP (JJ numerical) (NNS experiments))) (PP (IN by) (NP (JJ other) (NNS researchers))))))))))))) (. .))
