(SINV (ADVP (RB Successfully)) (VP (VBG training) (NP (JJ deep) (JJ neural) (NNS networks)) (ADVP (RB often))) (VP (VBZ requires) (PP (CC either) (NP (NN batch)))) (NP (NP (NN normalization)) (, ,) (NP (NP (JJ appropriate) (NN weight) (NN initialization)) (, ,) (SBAR (WHNP (NP (DT both)) (WHPP (IN of) (WHNP (WDT which)))) (S (VP (VBP come) (PP (IN with) (NP (PRP$ their) (JJ own) (NNS challenges)))))))) (. .))
(S (NP (PRP We)) (VP (VBP propose) (NP (NP (DT an) (NN alternative)) (, ,) (NP (NP (ADJP (ADVP (RB geometrically)) (JJ motivated)) (NN method)) (PP (IN for) (NP (NN training)))))) (. .))
(S (S (S (VP (VBG Using) (NP (JJ elementary) (NNS results)) (PP (IN from) (NP (JJ linear) (NN programming))))) (, ,) (NP (PRP we)) (VP (VBP introduce) (NP (NNP Farkas) (NNS layers)))) (: :) (S (NP (NP (DT a) (NN method)) (SBAR (WHNP (WDT that)) (S (VP (VBZ ensures) (NP (QP (ADVP (IN at) (RBS least)) (CD one)) (NN neuron)))))) (VP (VBZ is) (ADJP (JJ active) (PP (IN at) (NP (DT a) (VBN given) (NN layer)))))) (. .))
(S (S (VP (VBG Focusing) (PP (IN on) (NP (JJ residual) (NNS networks))) (PP (IN with) (NP (NN ReLU) (NN activation))))) (, ,) (NP (PRP we)) (ADVP (RB empirically)) (VP (VBP demonstrate) (NP (NP (DT a) (JJ significant) (NN improvement)) (PP (IN in) (NP (NN training) (NN capacity)))) (PP (IN in) (NP (NP (NP (DT the) (NN absence)) (PP (IN of) (NP (NN batch) (NN normalization)))) (CC or) (NP (NP (NNS methods)) (PP (IN of) (NP (NN initialization)))))) (PP (IN across) (NP (NP (DT a) (JJ broad) (NN range)) (PP (IN of) (NP (NP (NN network) (NNS sizes)) (PP (IN on) (NP (NN benchmark) (NNS datasets)))))))) (. .))
