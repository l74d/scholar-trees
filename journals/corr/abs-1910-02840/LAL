(S (S (VP (ADVP (RB Successfully)) (VP (VBG training) (NP (JJ deep) (JJ neural) (NNS networks))))) (ADVP (RB often)) (VP (VBZ requires) (NP (NP (CC either) (NP (JJ batch) (NN normalization)) (, ,) (NP (JJ appropriate) (NN weight) (NN initialization))) (, ,) (SBAR (WHNP (WHNP (DT both)) (WHPP (IN of) (WHNP (WDT which)))) (S (VP (VBP come) (PP (IN with) (NP (PRP$ their) (JJ own) (NNS challenges)))))))) (. .))
(S (NP (PRP We)) (VP (VBP propose) (NP (NP (DT an) (NN alternative) (, ,) (ADJP (RB geometrically) (VBN motivated)) (NN method)) (PP (IN for) (NP (NN training))))) (. .))
(S (S (VP (VBG Using) (NP (NP (JJ elementary) (NNS results)) (PP (IN from) (NP (JJ linear) (NN programming)))))) (, ,) (NP (PRP we)) (VP (VBP introduce) (NP (NP (JJ Farkas) (NNS layers)) (: :) (NP (NP (DT a) (NN method)) (SBAR (WHNP (WDT that)) (S (VP (VBZ ensures) (SBAR (S (NP (QP (IN at) (JJS least) (CD one)) (NN neuron)) (VP (VBZ is) (ADJP (JJ active)) (PP (IN at) (NP (DT a) (VBN given) (NN layer)))))))))))) (. .))
(S (S (VP (VBG Focusing) (PP (IN on) (NP (NP (JJ residual) (NNS networks)) (PP (IN with) (NP (NNP ReLU) (NN activation))))))) (, ,) (NP (PRP we)) (VP (ADVP (RB empirically)) (VBP demonstrate) (NP (NP (DT a) (JJ significant) (NN improvement)) (PP (IN in) (NP (VBG training) (NN capacity))) (PP (IN in) (NP (NP (DT the) (NN absence)) (PP (IN of) (NP (NP (NN batch) (NN normalization)) (CC or) (NP (NP (NNS methods)) (PP (IN of) (NP (NN initialization))))))))) (PP (IN across) (NP (NP (DT a) (JJ broad) (NN range)) (PP (IN of) (NP (NN network) (NNS sizes))))) (PP (IN on) (NP (NN benchmark) (NNS datasets)))) (. .))
