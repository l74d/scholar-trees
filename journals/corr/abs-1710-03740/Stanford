(S (NP (JJ Deep) (JJ neural) (NNS networks)) (VP (VBP have) (VP (VBN enabled) (NP (NN progress)) (PP (IN in) (NP (NP (DT a) (JJ wide) (NN variety)) (PP (IN of) (NP (NNS applications))))))) (. .))
(S (S (VP (VBG Growing) (NP (NP (DT the) (NN size)) (PP (IN of) (NP (DT the) (JJ neural) (NN network)))))) (ADVP (RB typically)) (VP (VBZ results) (PP (IN in) (NP (VBN improved) (NN accuracy)))) (. .))
(S (SBAR (IN As) (S (NP (NN model) (NNS sizes)) (VP (VBP grow)))) (, ,) (NP (NP (DT the) (NN memory)) (CC and) (NP (NML (S (VP (VB compute) (NP (NNS requirements)) (PP (IN for) (S (VP (VBG training) (NP (DT these)))))))) (NNS models))) (ADVP (RB also)) (VP (VBZ increases)) (. .))
(S (NP (PRP We)) (VP (VBP introduce) (NP (DT a) (NN technique)) (S (VP (TO to) (VP (VB train) (NP (JJ deep) (JJ neural) (NNS networks)) (S (VP (VBG using) (NP (NN half) (NN precision)) (S (VP (VBG floating) (NP (NN point) (NNS numbers)))))))))) (. .))
(S (PP (IN In) (NP (PRP$ our) (NN technique))) (, ,) (NP (NNS weights) (, ,) (NNS activations) (CC and) (NNS gradients)) (VP (VBP are) (VP (VBN stored) (PP (IN in) (NP (NML (NNP IEEE) (NN half)) (HYPH -) (NN precision) (NN format))))) (. .))
(S (NP (ADJP (NP (NN Half) (HYPH -) (NN precision)) (VBG floating)) (NNS numbers)) (VP (VBP have) (VP (VBN limited) (NP (JJ numerical) (NN range)) (PP (VBN compared) (PP (IN to) (NP (NML (JJ single) (HYPH -) (NN precision)) (NNS numbers)))))) (. .))
(S (NP (PRP We)) (VP (VBP propose) (NP (CD two) (NNS techniques)) (S (VP (TO to) (VP (VB handle) (NP (NP (DT this) (NN loss)) (PP (IN of) (NP (NN information)))))))) (. .))
(S (ADVP (RB Firstly)) (, ,) (NP (PRP we)) (VP (VBP recommend) (S (VP (VBG maintaining) (NP (NP (DT a) (JJ single) (HYPH -) (NN precision) (NN copy)) (PP (IN of) (NP (NP (DT the) (NNS weights)) (SBAR (WHNP (WDT that)) (S (VP (VBZ accumulates) (NP (DT the) (NNS gradients)) (PP (IN after) (NP (DT each) (NN optimizer) (NN step)))))))))))) (. .))
(S (NP (DT This) (NML (JJ single) (HYPH -) (NN precision)) (NN copy)) (VP (VBZ is) (VP (VBN rounded) (PP (IN to) (NP (NN half) (HYPH -) (NN precision) (NN format))) (PP (IN during) (NP (NN training))))) (. .))
(S (ADVP (RB Secondly)) (, ,) (NP (PRP we)) (VP (VBP propose) (S (VP (VBG scaling) (NP (DT the) (NN loss)) (ADVP (RB appropriately)) (S (VP (TO to) (VP (VB handle) (NP (NP (DT the) (NN loss)) (PP (IN of) (NP (NML (NML (NN information)) (PP (IN with) (NP (NN half) (HYPH -) (NN precision)))) (NNS gradients)))))))))) (. .))
(S (NP (PRP We)) (VP (VBP demonstrate) (SBAR (IN that) (S (NP (DT this) (NN approach)) (VP (VBZ works) (PP (IN for) (NP (NP (DT a) (JJ wide) (NN variety)) (PP (IN of) (NP (NP (NNS models)) (PP (VBG including) (NP (NP (NN convolution) (JJ neural) (NNS networks)) (, ,) (NP (JJ recurrent) (JJ neural) (NNS networks)) (CC and) (NP (JJ generative) (JJ adversarial) (NNS networks)))))))))))) (. .))
(S (NP (DT This) (NN technique)) (VP (VBZ works) (PP (IN for) (NP (NML (JJ large) (NN scale)) (NNS models))) (PP (IN with) (NP (NP (QP (JJR more) (IN than) (CD 100) (CD million)) (NNS parameters)) (VP (VBN trained) (PP (IN on) (NP (JJ large) (NNS datasets))))))) (. .))
(S (S (VP (VBG Using) (NP (DT this) (NN approach)))) (, ,) (NP (PRP we)) (VP (MD can) (VP (VB reduce) (NP (NP (DT the) (NN memory) (NN consumption)) (PP (IN of) (NP (NML (JJ deep) (NN learning)) (NNS models)))) (PP (IN by) (NP (RB nearly) (NN 2x))))) (. .))
(S (PP (IN In) (NP (JJ future) (NNS processors))) (, ,) (NP (PRP we)) (VP (MD can) (ADVP (RB also)) (VP (VB expect) (NP (DT a) (JJ significant) (NN computation) (NN speedup)) (S (VP (VBG using) (NP (NP (NN half) (HYPH -) (NN precision)) (NP (NN hardware) (NNS units))))))) (. .))
