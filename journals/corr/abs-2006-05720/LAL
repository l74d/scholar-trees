(S (NP (NNP Deep) (NN learning) (NNS networks)) (VP (VBP are) (ADVP (RB typically)) (VP (VBN trained) (PP (IN by) (NP (NP (JJ Stochastic) (NNP Gradient) (NNP Descent) (PRN (-LRB- -LRB-) (NP (NNP SGD)) (-RRB- -RRB-)) (NNS methods)) (SBAR (WHNP (WDT that)) (S (ADVP (RB iteratively)) (VP (VB improve) (NP (DT the) (NN model) (NNS parameters)) (PP (IN by) (S (VP (VBG estimating) (NP (DT a) (NN gradient)) (PP (IN on) (NP (NP (DT a) (ADJP (RB very) (JJ small)) (NN fraction)) (PP (IN of) (NP (DT the) (NN training) (NNS data))))))))))))))) (. .))
(S (NP (NP (DT A) (JJ major) (NN roadblock)) (VP (VBD faced) (SBAR (WHADVP (WRB when)) (S (VP (VBG increasing) (NP (DT the) (NN batch) (NN size)) (PP (TO to) (NP (NP (DT a) (JJ substantial) (NN fraction)) (PP (IN of) (NP (DT the) (NN training) (NNS data))))) (PP (IN for) (S (VP (VBG improving) (NP (NN training) (NN time)))))))))) (VP (VBZ is) (NP (NP (NP (DT the) (JJ persistent) (NN degradation)) (PP (IN in) (NP (NN performance)))) (PRN (-LRB- -LRB-) (NP (NN generalization) (NN gap)) (-RRB- -RRB-)))) (. .))
(S (S (VP (TO To) (VP (VB address) (NP (DT this) (NN issue))))) (, ,) (NP (JJ recent) (NN work)) (VP (VP (NN propose) (S (VP (TO to) (VP (VB add) (NP (JJ small) (NNS perturbations)) (PP (TO to) (NP (DT the) (NN model) (NNS parameters))) (SBAR (WHADVP (WRB when)) (S (VP (VBG computing) (NP (DT the) (JJ stochastic) (NNS gradients))))))))) (CC and) (VP (NN report) (NP (NP (VBD improved) (JJ generalization) (NN performance)) (ADJP (JJ due) (PP (TO to) (NP (VBG smoothing) (NNS effects))))))) (. .))
(S (S (ADVP (RB However)) (, ,) (NP (DT this) (NN approach)) (VP (VBZ is) (ADJP (ADVP (RB poorly)) (JJ understood)))) (: ;) (S (NP (PRP it)) (VP (VBZ requires) (ADVP (RB often)) (NP (JJ model-specific) (NN noise) (CC and) (NN fine-tuning)))) (. .))
(S (S (VP (TO To) (VP (VB alleviate) (NP (DT these) (NNS drawbacks))))) (, ,) (NP (PRP we)) (VP (VBP propose) (S (VP (TO to) (VP (VB use) (ADVP (RB instead)) (NP (NP (ADJP (RB computationally) (JJ efficient)) (NN extrapolation)) (PRN (-LRB- -LRB-) (NP (NN extragradient)) (-RRB- -RRB-))) (S (VP (TO to) (VP (VB stabilize) (NP (DT the) (NN optimization) (NN trajectory)) (SBAR (IN while) (S (ADVP (RB still)) (VP (VBG benefiting) (PP (IN from) (NP (NP (VBG smoothing)) (SBAR (S (VP (TO to) (VP (VB avoid) (NP (JJ sharp) (NN minima)))))))))))))))))) (. .))
(S (S (NP (DT This) (JJ principled) (NN approach)) (VP (VBZ is) (ADJP (RB well) (VBN grounded) (PP (IN from) (NP (DT an) (NN optimization) (NN perspective)))))) (CC and) (S (NP (PRP we)) (VP (VBP show) (SBAR (IN that) (S (NP (NP (DT a) (NN host)) (PP (IN of) (NP (NNS variations)))) (VP (MD can) (VP (VB be) (VP (VBN covered) (PP (IN in) (NP (NP (DT a) (JJ unified) (NN framework)) (SBAR (WHNP (IN that)) (S (NP (PRP we)) (VP (VBP propose))))))))))))) (. .))
(S (NP (PRP We)) (VP (VP (VBP prove) (NP (NP (DT the) (NN convergence)) (PP (IN of) (NP (DT this) (JJ novel) (NN scheme))))) (CC and) (VP (ADVP (RB rigorously)) (VB evaluate) (NP (NP (PRP$ its) (JJ empirical) (NN performance)) (PP (IN on) (NP (NP (NNP ResNet)) (, ,) (NP (NNP LSTM)) (, ,) (CC and) (NP (NNP Transformer))))))) (. .))
(S (NP (PRP We)) (VP (VBP demonstrate) (SBAR (IN that) (S (PP (IN in) (NP (NP (DT a) (NN variety)) (PP (IN of) (NP (NNS experiments))))) (NP (DT the) (NN scheme)) (VP (VBZ allows) (S (VP (NP (VBG scaling)) (PP (TO to) (NP (NP (ADJP (JJ much) (JJR larger)) (NN batch) (NNS sizes)) (PP (IN than) (ADVP (IN before))))))) (SBAR (JJ whilst) (S (VP (NN reaching) (CC or) (VBG surpassing) (NP (NNP SOTA) (NN accuracy))))))))) (. .))
