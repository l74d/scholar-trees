(S (NP (NN Policy) (NN gradient) (NNS methods)) (VP (VBP are) (NP (NP (DT an) (VBG appealing) (NN approach)) (PP (IN in) (NP (NN reinforcement) (NN learning)))) (SBAR (IN because) (S (NP (PRP they)) (VP (VP (ADVP (RB directly)) (VBP optimize) (NP (DT the) (JJ cumulative) (NN reward))) (CC and) (VP (MD can) (ADVP (RB straightforwardly)) (VP (VB be) (VP (VBN used) (PP (IN with) (NP (NP (JJ nonlinear) (NN function) (NNS approximators)) (PP (JJ such) (IN as) (NP (JJ neural) (NNS networks)))))))))))) (. .))
(S (NP (DT The) (CD two) (JJ main) (NNS challenges)) (VP (VBP are) (NP (NP (NP (DT the) (JJ large) (NN number)) (PP (IN of) (NP (NNS samples))) (VP (ADVP (RB typically)) (VBN required))) (, ,) (CC and) (NP (NP (DT the) (NN difficulty)) (PP (IN of) (S (VP (VBG obtaining) (NP (ADJP (JJ stable) (CC and) (JJ steady)) (NN improvement)) (PP (IN despite) (NP (NP (DT the) (NN nonstationarity)) (PP (IN of) (NP (DT the) (NN incoming) (NNS data))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP address) (NP (DT the) (JJ first) (NN challenge)) (PP (IN by) (S (VP (VBG using) (NP (NN value) (NNS functions)) (S (VP (TO to) (VP (ADVP (RB substantially)) (VB reduce) (NP (NP (DT the) (NN variance)) (PP (IN of) (NP (NN policy) (NN gradient) (NNS estimates)))) (PP (IN at) (NP (NP (DT the) (NN cost)) (PP (IN of) (NP (DT some) (NN bias)))))))) (, ,) (PP (IN with) (NP (NP (DT an) (JJ exponentially-weighted) (NN estimator)) (PP (IN of) (NP (DT the) (NN advantage) (NN function))) (SBAR (WHNP (WDT that)) (S (VP (VBZ is) (ADJP (JJ analogous) (PP (TO to) (NP (NP (NNP TD)) (PRN (-LRB- -LRB-) (NP (NN lambda)) (-RRB- -RRB-)))))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP address) (NP (DT the) (JJ second) (NN challenge)) (PP (IN by) (S (VP (VBG using) (NP (JJ trust) (NN region) (NN optimization) (NN procedure)) (PP (IN for) (NP (NP (DT both) (NP (DT the) (NN policy)) (CC and) (NP (DT the) (NN value) (NN function))) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBP are) (VP (VBN represented) (PP (IN by) (NP (JJ neural) (NNS networks))))))))))))) (. .))
(S (NP (PRP$ Our) (NN approach)) (VP (NNS yields) (NP (NP (JJ strong) (JJ empirical) (NNS results)) (PP (IN on) (NP (ADJP (RB highly) (VBG challenging)) (CD 3D) (NN locomotion) (NNS tasks)))) (, ,) (S (VP (VP (VBG learning) (NP (NP (VBG running) (NNS gaits)) (PP (IN for) (NP (ADJP (NN bipedal) (CC and) (NN quadrupedal)) (JJ simulated) (NNS robots))))) (, ,) (CC and) (VP (VBG learning) (NP (NP (DT a) (NN policy)) (PP (IN for) (S (VP (VBG getting) (S (NP (DT the) (VBN biped)) (VP (TO to) (VP (VB stand) (ADVP (RP up))))) (PP (IN from) (S (VP (VBG starting) (PRT (RP out)) (S (VP (VBG lying) (PP (IN on) (NP (DT the) (NN ground)))))))))))))))) (. .))
(S (PP (IN In) (NP (NP (NN contrast)) (PP (TO to) (NP (NP (DT a) (NN body)) (PP (IN of) (NP (JJ prior) (NN work))) (SBAR (WHNP (WDT that)) (S (VP (VBZ uses) (NP (JJ hand-crafted) (NN policy) (NNS representations))))))))) (, ,) (NP (PRP$ our) (JJ neural) (NN network) (NNS policies)) (VP (VBP map) (ADVP (RB directly)) (PP (IN from) (NP (JJ raw) (NNS kinematics))) (PP (TO to) (NP (VB joint) (NNS torques)))) (. .))
(S (S (NP (PRP$ Our) (NN algorithm)) (VP (VBZ is) (ADJP (RB fully) (JJ model-free)))) (, ,) (CC and) (S (NP (NP (DT the) (NN amount)) (PP (IN of) (NP (JJ simulated) (NN experience))) (VP (VBN required) (PP (IN for) (NP (NP (DT the) (NN learning) (NNS tasks)) (PP (IN on) (NP (CD 3D) (NNS bipeds))))))) (VP (NNS corresponds) (PP (TO to) (NP (NP (JJ 1-2) (NNS weeks)) (PP (IN of) (NP (JJ real) (NN time))))))) (. .))
