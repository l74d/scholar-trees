(S (S (VP (VBG Augmenting) (NP (JJ neural) (NNS networks)) (PP (IN with) (NP (NML (S (VP (VB skip)))) (NNS connections))))) (, ,) (SBAR (IN as) (S (VP (VBN introduced) (PP (IN in) (NP (DT the) (ADJP (RB so) (HYPH -) (VBN called)) (NNP ResNet) (NN architecture)))))) (, ,) (VP (VBD surprised) (NP (DT the) (NN community)) (PP (IN by) (S (VP (VBG enabling) (NP (NP (DT the) (NN training)) (PP (IN of) (NP (NP (NNS networks)) (PP (IN of) (NP (QP (JJR more) (IN than) (CD 1,000)) (NNS layers)))))) (PP (IN with) (NP (JJ significant) (NN performance) (NNS gains))))))) (. .))
(S (NP (DT This) (NN paper)) (VP (VP (VBZ deciphers) (NP (NNP ResNet)) (PP (IN by) (S (VP (VBG analyzing) (NP (NP (DT the) (NN effect)) (PP (IN of) (NP (NML (S (VP (VB skip)))) (NNS connections)))))))) (, ,) (CC and) (VP (VBZ puts) (PRT (RP forward)) (NP (NP (JJ new) (JJ theoretical) (NNS results)) (PP (IN on) (NP (DT the) (NNS advantages)))) (PP (IN of) (NP (NN identity))) (S (VP (VB skip) (NP (NNS connections)) (PP (IN in) (NP (JJ neural) (NNS networks))))))) (. .))
(S (S (NP (PRP We)) (VP (VBP prove) (SBAR (IN that) (S (NP (NP (DT the) (NML (S (VP (VB skip)))) (NNS connections)) (PP (IN in) (NP (DT the) (JJ residual) (NNS blocks)))) (VP (VBP facilitate) (S (VP (VBG preserving) (NP (NP (DT the) (NN norm)) (PP (IN of) (NP (DT the) (NN gradient))))))))))) (, ,) (CC and) (SINV (VP (VB lead) (PP (IN to) (ADJP (JJ stable)))) (NP (NP (RB back) (HYPH -) (NN propagation)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ is) (ADJP (JJ desirable) (PP (IN from) (NP (NN optimization) (NN perspective))))))))) (. .))
(S (NP (PRP We)) (ADVP (RB also)) (VP (VBP show) (SBAR (IN that) (, ,) (S (ADVP (ADVP (RB perhaps) (RB surprisingly)) (, ,) (SBAR (IN as) (S (NP (RBR more) (JJ residual) (NNS blocks)) (VP (VBP are) (VP (VBN stacked)))))) (, ,) (NP (NP (DT the) (NN norm) (HYPH -) (NN preservation)) (PP (IN of) (NP (DT the) (NN network)))) (VP (VBZ is) (VP (VBN enhanced)))))) (. .))
(S (NP (PRP$ Our) (JJ theoretical) (NNS arguments)) (VP (VBP are) (VP (VBN supported) (PP (IN by) (NP (JJ extensive) (JJ empirical) (NN evidence))))) (. .))
(SQ (MD Can) (NP (PRP we)) (VP (VB push) (PP (IN for) (NP (JJ extra) (NN norm) (HYPH -) (NN preservation)))) (. ?))
(S (NP (PRP We)) (VP (VBP answer) (NP (DT this) (NN question)) (PP (IN by) (S (VP (VP (VBG proposing) (NP (DT an) (JJ efficient) (NN method)) (S (VP (TO to) (VP (VB regularize) (NP (NP (DT the) (JJ singular) (NNS values)) (PP (IN of) (NP (DT the) (NN convolution) (NN operator)))))))) (CC and) (VP (VBG making) (NP (NP (NP (DT the) (NNP ResNet) (POS 's)) (NN transition) (NNS layers)) (ADJP (NP (JJ extra) (NN norm)) (HYPH -) (VBG preserving)))))))) (. .))
(S (NP (PRP$ Our) (JJ numerical) (NNS investigations)) (VP (VBP demonstrate) (SBAR (IN that) (S (NP (NP (DT the) (NML (NML (NN learning) (NNS dynamics)) (CC and) (NML (DT the) (NN classification))) (NN performance)) (PP (IN of) (NP (NNP ResNet)))) (VP (MD can) (VP (VB be) (VP (VBN improved) (PP (IN by) (S (VP (VBG making) (NP (PRP it)) (NP (ADJP (RB even) (JJR more)) (NN norm)) (S (VP (VBG preserving)))))))))))) (. .))
(S (NP (NP (PRP$ Our) (NNS results)) (CC and) (NP (NP (DT the) (VBN introduced) (NN modification)) (PP (IN for) (NP (NNP ResNet))))) (, ,) (VP (VP (VBD referred) (PP (IN to) (PP (IN as) (NP (NNP Procrustes) (NNPS ResNets))))) (, ,) (VP (MD can) (VP (VB be) (VP (VBN used) (PP (IN as) (NP (NP (DT a) (NN guide)) (PP (IN for) (NP (NN training))) (ADVP (RBR deeper)) (NP (NNS networks))))))) (CC and) (VP (MD can) (ADVP (RB also)) (VP (VB inspire) (NP (JJ new) (JJR deeper) (NNS architectures))))) (. .))
