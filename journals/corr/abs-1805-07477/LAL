(S (S (VP (VBG Augmenting) (NP (JJ neural) (NNS networks)) (PP (IN with) (NP (JJ skip) (NNS connections))) (, ,) (SBAR (IN as) (S (VP (VBN introduced) (PP (IN in) (NP (DT the) (JJ so-called) (NNP ResNet) (NN architecture)))))) (, ,))) (VP (VBD surprised) (NP (DT the) (NN community)) (PP (IN by) (S (VP (VBG enabling) (NP (NP (DT the) (NN training)) (PP (IN of) (NP (NP (NNS networks)) (PP (IN of) (NP (QP (JJR more) (IN than) (CD 1,000)) (NNS layers))))) (PP (IN with) (NP (JJ significant) (NN performance) (NNS gains)))))))) (. .))
(S (NP (DT This) (NN paper)) (VP (VP (NNS deciphers) (NP (NNP ResNet)) (PP (IN by) (S (VP (VBG analyzing) (NP (NP (DT the) (NN effect)) (PP (IN of) (NP (NN skip) (NNS connections)))))))) (, ,) (CC and) (VP (VBZ puts) (PRT (RB forward)) (NP (NP (JJ new) (JJ theoretical) (NNS results)) (PP (IN on) (NP (NP (DT the) (NNS advantages)) (PP (IN of) (NP (NN identity) (NN skip) (NNS connections))) (PP (IN in) (NP (JJ neural) (NNS networks)))))))) (. .))
(S (NP (PRP We)) (VP (VBP prove) (SBAR (IN that) (S (NP (NP (DT the) (JJ skip) (NNS connections)) (PP (IN in) (NP (DT the) (JJ residual) (NNS blocks)))) (VP (VP (VBP facilitate) (S (VP (VBG preserving) (NP (NP (DT the) (NN norm)) (PP (IN of) (NP (DT the) (NN gradient))))))) (, ,) (CC and) (VP (NN lead) (PP (TO to) (NP (NP (JJ stable) (NN back-propagation)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ is) (ADJP (JJ desirable)) (PP (IN from) (NP (NN optimization) (NN perspective))))))))))))) (. .))
(S (NP (PRP We)) (ADVP (RB also)) (VP (VBP show) (SBAR (IN that) (, ,) (ADVP (RB perhaps) (RB surprisingly)) (, ,) (SBAR (IN as) (S (NP (RBR more) (JJ residual) (NNS blocks)) (VP (VBP are) (VP (VBN stacked))))) (, ,) (NP (NP (DT the) (NN norm-preservation)) (PP (IN of) (NP (DT the) (NN network)))) (VP (VBZ is) (VP (VBN enhanced))))) (. .))
(S (NP (PRP$ Our) (JJ theoretical) (NNS arguments)) (VP (VBP are) (VP (VBN supported) (PP (IN by) (NP (JJ extensive) (JJ empirical) (NN evidence))))) (. .))
(SQ (MD Can) (NP (PRP we)) (VP (VB push) (PP (IN for) (NP (JJ extra) (NN norm-preservation)))) (. ?))
(S (NP (PRP We)) (VP (VBP answer) (NP (DT this) (NN question)) (PP (IN by) (S (VP (VP (VBG proposing) (NP (NP (DT an) (JJ efficient) (NN method)) (SBAR (S (VP (TO to) (VP (VB regularize) (NP (NP (DT the) (JJ singular) (NNS values)) (PP (IN of) (NP (DT the) (NN convolution) (NN operator)))))))))) (CC and) (VP (VBG making) (S (NP (NP (DT the) (NNP ResNet) (POS 's)) (NN transition) (NNS layers)) (ADJP (JJ extra) (JJ norm-preserving)))))))) (. .))
(S (NP (PRP$ Our) (JJ numerical) (NNS investigations)) (VP (VBP demonstrate) (SBAR (IN that) (S (NP (NP (NP (DT the) (NN learning) (NNS dynamics)) (CC and) (NP (DT the) (NN classification) (NN performance))) (PP (IN of) (NP (NNP ResNet)))) (VP (MD can) (VP (VB be) (VP (VBN improved) (PP (IN by) (S (VP (VBG making) (S (NP (PRP it)) (ADJP (ADVP (RB even) (RBR more)) (JJ norm) (NN preserving)))))))))))) (. .))
(S (NP (NP (PRP$ Our) (NNS results)) (CC and) (NP (NP (DT the) (JJ introduced) (NN modification)) (PP (IN for) (NP (NNP ResNet))) (, ,) (VP (VBD referred) (PP (TO to)) (PP (IN as) (NP (NNP Procrustes) (NNP ResNets)))) (, ,))) (VP (VP (MD can) (VP (VB be) (VP (VBN used) (PP (IN as) (NP (NP (DT a) (NN guide)) (PP (IN for) (S (VP (VBG training) (NP (JJR deeper) (NNS networks)))))))))) (CC and) (VP (MD can) (ADVP (RB also)) (VP (VB inspire) (NP (JJ new) (JJR deeper) (NNS architectures))))) (. .))
