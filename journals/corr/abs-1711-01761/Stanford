(S (NP (PRP We)) (VP (VBP study) (NP (DT a) (JJ new) (NN aggregation) (NN operator)) (PP (IN for) (NP (NP (NNS gradients)) (VP (VBG coming) (PP (IN from) (NP (NP (NP (DT a) (NN mini-batch)) (PP (IN for) (NP (JJ stochastic) (NML (NN gradient) (PRN (-LRB- -LRB-) (NP (NN SG)) (-RRB- -RRB-))) (NNS methods)))) (SBAR (WHNP (WDT that)) (S (VP (VBZ allows) (NP (NP (DT a) (JJ significant) (NN speed) (HYPH -) (NN up)) (PP (IN in) (NP (NP (DT the) (NN case)) (PP (IN of) (NP (JJ sparse) (NN optimization) (NNS problems))))))))))))))) (. .))
(S (S (NP (PRP We)) (VP (VBP call) (NP (NP (DT this) (NN method)) (NP (NNP AdaBatch))))) (CC and) (S (NP (PRP it)) (ADVP (RB only)) (VP (VBZ requires) (NP (NP (DT a) (JJ few) (NNS lines)) (PP (IN of) (NP (NN code) (NN change)))) (PP (VBN compared) (PP (IN to) (NP (JJ regular) (JJ mini-batch) (NNP SGD) (NNS algorithms)))))) (. .))
(S (NP (PRP We)) (VP (VBP provide) (NP (DT a) (JJ theoretical) (NN insight)) (S (VP (TO to) (VP (VP (VB understand) (SBAR (WHADVP (WRB how)) (S (NP (NP (DT this) (JJ new) (NN class)) (PP (IN of) (NP (NNS algorithms)))) (VP (VBZ is) (VP (VBG performing)))))) (CC and) (VP (VB show) (SBAR (IN that) (S (NP (PRP it)) (VP (VBZ is) (ADJP (JJ equivalent) (PP (IN to) (NP (NP (DT an) (JJ implicit) (NML (PP (IN per) (HYPH -) (NP (NN coordinate)))) (NN rescaling)) (PP (IN of) (NP (DT the) (NNS gradients)))))) (, ,) (PP (ADVP (RB similarly)) (IN to) (SBAR (WHNP (WP what)) (S (NP (NNP Adagrad) (NNS methods)) (VP (MD can) (VP (VB do)))))))))))))) (. .))
(S (PP (PP (IN In) (NP (NN theory))) (CC and) (PP (IN in) (NP (NN practice)))) (, ,) (NP (DT this) (JJ new) (NN aggregation)) (VP (VBZ allows) (S (VP (TO to) (VP (VB keep) (NP (NP (DT the) (JJ same) (NN sample) (NN efficiency)) (PP (IN of) (NP (NN SG) (NNS methods)))) (PP (IN while) (S (VP (VBG increasing) (NP (DT the) (NN batch) (NN size))))))))) (. .))
(S (ADVP (RB Experimentally)) (, ,) (NP (PRP we)) (ADVP (RB also)) (VP (VBP show) (SBAR (IN that) (S (PP (IN in) (NP (NP (DT the) (NN case)) (PP (IN of) (NP (JJ smooth) (NN convex) (NN optimization))))) (, ,) (NP (PRP$ our) (NN procedure)) (VP (MD can) (ADVP (RB even)) (VP (VB obtain) (NP (DT a) (JJR better) (NN loss)) (SBAR (WHADVP (WRB when)) (S (VP (VBG increasing) (NP (DT the) (NN batch) (NN size)) (PP (IN for) (NP (NP (DT a) (VBN fixed) (NN number)) (PP (IN of) (NP (NNS samples))))))))))))) (. .))
(S (NP (PRP We)) (ADVP (RB then)) (VP (VB apply) (NP (DT this) (JJ new) (NN algorithm)) (S (VP (TO to) (VP (VB obtain) (NP (NP (DT a) (JJ parallelizable) (JJ stochastic) (NN gradient) (NN method)) (SBAR (WHNP (WDT that)) (S (VP (VP (VBZ is) (ADJP (JJ synchronous))) (CC but) (VP (VBZ allows) (NP (NP (NN speed) (HYPH -) (NN up)) (PP (IN on) (NP (NN par)))) (PP (IN with) (NP (NNP Hogwild)))))))))))) (. !))
(S (NP (NP (NNS methods)) (PP (IN as) (NP (NN convergence)))) (VP (VBZ does) (RB not) (VP (VB deteriorate) (PP (IN with) (NP (NP (DT the) (NN increase)) (PP (IN of) (NP (DT the) (NN batch) (NN size))))))) (. .))
(S (NP (DT The) (JJ same) (NN approach)) (VP (MD can) (VP (VB be) (VP (VBN used) (S (VP (TO to) (VP (VB make) (NP (NP (JJ mini-batch) (ADJP (NP (NP (ADJP (RB provably) (JJ efficient))) (PP (IN for) (NP (NN variance)))) (HYPH -) (VBN reduced)) (NN SG) (NNS methods)) (PP (JJ such) (IN as) (NP (NN SVRG)))))))))) (. .))
