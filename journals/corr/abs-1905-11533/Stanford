(S (NP-TMP (NN Today)) (NP (DT a) (JJ canonical) (NN approach) (S (VP (TO to) (VP (VB reduce) (NP (NP (DT the) (NN computation) (NN cost)) (PP (IN of) (NP (NP (JJ Deep) (JJ Neural) (NNS Networks)) (-LRB- -LRB-) (NP (NNS DNNs)) (-RRB- -RRB-)))))))) (VP (VBZ is) (S (VP (TO to) (VP (VB pre-define) (NP (DT an) (JJ over-parameterized) (NN model)) (PP (IN before) (NP (NN training) (S (VP (TO to) (VP (VP (VB guarantee) (NP (DT the) (NN learning) (NN capacity))) (, ,) (CC and) (ADVP (RB then)) (VP (VB prune) (NP (NP (ADJP (JJ unimportant)) (NN learning) (NNS units)) (-LRB- -LRB-) (NP (NNS filters) (CC and) (NNS neurons)) (-RRB- -RRB-)) (PP (IN during) (NP (NN training) (S (VP (TO to) (VP (VB improve) (NP (NN model) (NN compactness))))))))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP argue) (SBAR (S (NP (PRP it)) (VP (VBZ is) (ADJP (JJ unnecessary) (S (VP (TO to) (VP (VP (VB introduce) (NP (NN redundancy)) (PP (IN at) (NP (NP (DT the) (NN beginning)) (PP (IN of) (NP (DT the) (NN training)))))) (CC but) (ADVP (RB then)) (VP (VB reduce) (NP (NN redundancy)) (PP (IN for) (NP (DT the) (JJ ultimate) (NN inference) (NN model)))))))))))) (. .))
(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (DT a) (JJ Continuous) (NML (NML (NN Growth)) (CC and) (NML (NN Pruning) (PRN (-LRB- -LRB-) (NP (NN CGaP)) (-RRB- -RRB-)))) (NN scheme)) (S (VP (TO to) (VP (VB minimize) (NP (DT the) (NN redundancy)) (PP (IN from) (NP (DT the) (NN beginning))))))) (. .))
(S (NP (NN CGaP)) (VP (VP (VBZ starts) (NP (DT the) (NN training)) (PP (IN from) (NP (DT a) (JJ small) (NN network) (NN seed)))) (, ,) (VP (ADVP (RB then)) (VBZ expands) (NP (DT the) (NN model)) (ADVP (RB continuously)) (PP (IN by) (S (VP (VBG reinforcing) (NP (JJ important) (NN learning) (NNS units)))))) (, ,) (CC and) (ADVP (RB finally)) (VP (VBZ prunes) (NP (DT the) (NN network)) (S (VP (TO to) (VP (VB obtain) (NP (DT a) (ADJP (JJ compact) (CC and) (JJ accurate)) (NN model))))))) (. .))
(S (SBAR (IN As) (S (NP (DT the) (NN growth) (NN phase)) (VP (VBZ favors) (NP (JJ important) (NN learning) (NNS units))))) (, ,) (NP (NN CGaP)) (VP (VBZ provides) (NP (DT a) (JJ clear) (NN learning) (NN purpose)) (PP (IN to) (NP (DT the) (NN pruning) (NN phase)))) (. .))
(S (NP (NP (JJ Experimental) (NNS results)) (PP (IN on) (NP (NP (JJ representative) (NNS datasets)) (CC and) (NP (NN DNN) (NNS architectures))))) (VP (VBP demonstrate) (SBAR (IN that) (S (NP (NN CGaP)) (VP (VBZ outperforms) (NP (NP (ADJP (NP (JJ previous) (NN pruning)) (HYPH -) (JJ only)) (NNS approaches)) (SBAR (WHNP (WDT that)) (S (VP (VBP deal) (PP (IN with) (NP (JJ pre-defined) (NNS structures))))))))))) (. .))
(S (S (PP (IN For) (NP (NP (NN VGG) (HYPH -) (CD 19)) (PP (IN on) (NP (NML (NML (NN CIFAR) (HYPH -) (CD 100)) (CC and) (NML (NN SVHN))) (NNS datasets))))) (, ,) (NP (NN CGaP)) (VP (VP (VBZ reduces) (NP (NP (DT the) (NN number)) (PP (IN of) (NP (NNS parameters)))) (PP (IN by) (NP (QP (CD 78.9) (NN %) (CC and) (CD 85.8) (NN %))))) (, ,) (VP (NP (NNS FLOPs)) (PP (IN by) (NP (QP (CD 53.2) (NN %) (CC and) (CD 74.2) (NN %))))) (, ,) (ADVP (RB respectively)))) (: ;) (S (PP (IN For) (NP (NNP ResNet) (HYPH -) (CD 110))) (PP (IN On) (NP (NN CIFAR) (HYPH -) (CD 10))) (, ,) (NP (NN CGaP)) (VP (VBZ reduces) (NP (CD 64.0) (NN %)) (NP (NP (NN number)) (PP (IN of) (NP (NP (NNS parameters)) (CC and) (NP (NML (CD 63.3) (NN %)) (NNS FLOPs))))))) (. .))
