(S (NP (ADJP (NP (NN Gradient)) (HYPH -) (VBN based)) (NN hyperparameter) (NN optimization)) (VP (VBZ is) (NP (DT an) (JJ attractive) (NN way) (S (VP (TO to) (VP (VP (VB perform) (NP (NN meta) (HYPH -) (NN learning)) (PP (IN across) (NP (NP (DT a) (NN distribution)) (PP (IN of) (NP (NNS tasks)))))) (, ,) (CC or) (VP (VB improve) (NP (NP (DT the) (NN performance)) (PP (IN of) (NP (NP (DT an) (NN optimizer)) (PP (IN on) (NP (DT a) (JJ single) (NN task)))))))))))) (. .))
(S (ADVP (RB However)) (, ,) (NP (DT this) (NN approach)) (VP (VBZ has) (VP (VBN been) (ADJP (JJ unpopular) (PP (IN for) (NP (NP (NNS tasks)) (VP (VBG requiring) (NP (NP (JJ long) (NNS horizons)) (-LRB- -LRB-) (NP (JJ many) (NN gradient) (NNS steps)) (-RRB- -RRB-)) (, ,) (PP (IN due) (PP (IN to) (NP (NP (NN memory) (NN scaling)) (CC and) (NP (NN gradient) (NN degradation) (NNS issues))))))))))) (. .))
(S (NP (DT A) (JJ common) (NN workaround)) (VP (VBZ is) (S (VP (TO to) (VP (VP (VB learn) (NP (NNS hyperparameters)) (ADVP (RB online))) (CC or) (VP (VB split) (NP (DT the) (NN horizon)) (PP (IN into) (NP (JJR smaller) (NNS chunks)))))))) (. .))
(S (ADVP (RB However)) (, ,) (NP (DT this)) (VP (VBZ introduces) (NP (NP (NN greediness)) (SBAR (WHNP (WDT which)) (S (VP (VBZ comes) (PP (IN with) (NP (DT a) (JJ large) (NN performance) (NN drop))) (, ,) (SBAR (IN since) (S (NP (DT the) (JJS best) (JJ local) (NNS hyperparameters)) (VP (MD can) (VP (VB make) (PP (IN for) (NP (JJ poor) (JJ global) (NNS solutions)))))))))))) (. .))
(S (PP (IN In) (NP (DT this) (NN work))) (, ,) (NP (PRP we)) (VP (VBP enable) (NP (NP (NN non-greediness)) (PP (IN over) (NP (JJ long) (NNS horizons)))) (PP (IN with) (NP (QP (DT a) (JJ two-fold)) (NN solution)))) (. .))
(S (ADVP (RB First)) (, ,) (NP (PRP we)) (VP (VBP share) (NP (NP (NNS hyperparameters)) (SBAR (WHNP (WDT that)) (S (VP (VP (VBP are) (ADJP (JJ contiguous) (PP (IN in) (NP (NN time))))) (, ,) (CC and) (VP (VBP show) (SBAR (IN that) (S (NP (DT this)) (ADVP (RB drastically)) (VP (VBZ mitigates) (NP (NML (NN gradient) (NN degradation)) (NNS issues))))))))))) (. .))
(S (ADVP (RB Then)) (, ,) (NP (PRP we)) (VP (VBP derive) (NP (NP (DT a) (NML (RB forward) (HYPH -) (NN mode)) (NN differentiation) (NN algorithm)) (PP (IN for) (NP (NP (DT the) (ADJP (NP (JJ popular) (NN momentum)) (HYPH -) (VBN based)) (NNP SGD) (NN optimizer)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ allows) (PP (IN for) (NP (NP (DT a) (NN memory) (NN cost)) (SBAR (WHNP (WDT that)) (S (VP (VBZ is) (ADJP (JJ constant) (PP (IN with) (NP (NN horizon) (NN size)))))))))))))))) (. .))
(S (SBAR (WHADVP (WRB When)) (S (VP (VBN put) (ADVP (RB together))))) (, ,) (NP (DT these) (NNS solutions)) (VP (VBP allow) (S (NP (PRP us)) (VP (TO to) (VP (VB learn) (NP (NNS hyperparameters)) (PP (IN without) (NP (DT any) (JJ prior) (NN knowledge))))))) (. .))
(S (PP (VBN Compared) (PP (IN to) (NP (NP (DT the) (NN baseline)) (PP (IN of) (NP (ADJP (ADJP (NN hand) (HYPH -) (VBN tuned)) (PP (IN off) (HYPH -) (NP (DT the) (HYPH -) (NN shelf)))) (NNS hyperparameters)))))) (, ,) (NP (PRP$ our) (NN method)) (VP (VBZ compares) (ADVP (RB favorably) (RB on)) (NP (NP (JJ simple) (NNS datasets)) (PP (IN like) (NP (NNP SVHN))))) (. .))
(S (PP (IN On) (NP (NN CIFAR) (HYPH -) (CD 10))) (NP (PRP we)) (VP (VP (VBP match) (NP (DT the) (NN baseline) (NN performance))) (, ,) (CC and) (VP (VBP demonstrate) (PP (IN for) (NP (NP (DT the) (JJ first) (NN time)) (SBAR (WHNP (WDT that)) (S (VP (VBG learning) (NP (NP (NN rate) (, ,) (NN momentum) (CC and) (NN weight)) (SBAR (S (NP (NN decay) (NNS schedules)) (VP (MD can) (VP (VB be) (VP (VBN learned) (PP (IN with) (NP (NP (NNS gradients)) (PP (IN on) (NP (NP (DT a) (NN dataset)) (PP (IN of) (NP (DT this) (NN size)))))))))))))))))))) (. .))
(S (NP (NN Code)) (VP (VBZ is) (ADJP (JJ available) (PP (IN at) (NP (NP (DT this)) (SBAR (S (VP (VBZ https) (NP (NN URL))))))))))
