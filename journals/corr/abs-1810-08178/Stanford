(S (NP (DT This) (NN paper)) (VP (VBZ presents) (NP (DT a) (JJ novel) (NN optimization) (NN method)) (PP (IN for) (S (VP (VBG maximizing) (NP (NN generalization)) (PP (IN over) (NP (NP (NNS tasks)) (PP (IN in) (NP (NN meta) (HYPH -) (NN learning))))))))) (. .))
(S (NP (NP (DT The) (NN goal)) (PP (IN of) (NP (NN meta) (HYPH -) (NN learning)))) (VP (VBZ is) (S (VP (TO to) (VP (VB learn) (NP (DT a) (NN model)) (PP (IN for) (NP (NP (DT an) (NN agent)) (VP (VBG adapting) (ADVP (RB rapidly)) (SBAR (WHADVP (WRB when)) (S (VP (VBN presented) (PP (IN with) (NP (ADJP (RB previously) (JJ unseen)) (NNS tasks))))))))))))) (. .))
(S (NP (NNS Tasks)) (VP (VBP are) (VP (VBN sampled) (PP (IN from) (NP (NP (DT a) (JJ specific) (NN distribution)) (SBAR (WHNP (WDT which)) (S (VP (VBZ is) (VP (VBN assumed) (S (VP (TO to) (VP (VB be) (ADJP (JJ similar) (PP (IN for) (NP (DT both) (ADJP (VBN seen) (CC and) (JJ unseen)) (NNS tasks))))))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP focus) (PP (IN on) (NP (NP (DT a) (NN family)) (PP (IN of) (NP (NML (NN meta) (HYPH -) (NN learning)) (NNS methods))))) (S (VP (VBG learning) (NP (NP (JJ initial) (NNS parameters)) (PP (IN of) (NP (NP (DT a) (NN base) (NN model)) (SBAR (WHNP (WDT which)) (S (VP (MD can) (VP (VB be) (ADJP (JJ fine) (HYPH -) (VBN tuned)) (ADVP (RB quickly) (PP (IN on) (NP (DT a) (JJ new) (NN task)))) (, ,) (PP (IN by) (NP (JJ few) (NN gradient) (NNS steps)))))))))))) (PRN (-LRB- -LRB-) (NP (NN MAML)) (-RRB- -RRB-))) (. .))
(S (NP (PRP$ Our) (NN approach)) (VP (VBZ is) (VP (VBN based) (PP (IN on) (S (VP (VBG pushing) (NP (NP (DT the) (NNS parameters)) (PP (IN of) (NP (DT the) (NN model)))) (PP (IN to) (NP (NP (DT a) (NN direction)) (SBAR (WHPP (IN in) (WHNP (WDT which))) (S (NP (NNS tasks)) (VP (VBP have) (NP (JJR more) (NN agreement)) (PP (IN upon)))))))))))) (. .))
(S (SBAR (IN If) (S (NP (NP (DT the) (NNS gradients)) (PP (IN of) (NP (DT a) (NN task)))) (VP (VBP agree) (PP (IN with) (NP (DT the) (NNS parameters) (NN update) (NN vector)))))) (, ,) (ADVP (RB then)) (NP (PRP$ their) (JJ inner) (NN product)) (VP (MD will) (VP (VB be) (NP (DT a) (JJ large) (JJ positive) (NN value)))) (. .))
(S (PP (IN As) (NP (DT a) (NN result))) (, ,) (PP (VBN given) (NP (NP (DT a) (NN batch)) (PP (IN of) (NP (NNS tasks) (S (VP (TO to) (VP (VB be) (VP (VBN optimized) (PP (IN for)))))))))) (, ,) (NP (PRP we)) (VP (VBP associate) (NP (DT a) (ADJP (JJ positive) (PRN (-LRB- -LRB-) (ADJP (JJ negative)) (-RRB- -RRB-))) (NN weight)) (PP (IN to) (NP (NP (DT the) (NN loss) (NN function)) (PP (IN of) (NP (DT a) (NN task))))) (, ,) (SBAR (IN if) (S (NP (NP (NP (DT the) (JJ inner) (NN product)) (PP (IN between) (NP (PRP$ its) (NNS gradients)))) (CC and) (NP (NP (DT the) (NN average)) (PP (IN of) (NP (NP (DT the) (NNS gradients)) (PP (IN of) (NP (NP (DT all) (NNS tasks)) (PP (IN in) (NP (DT the) (NN batch))))))))) (VP (VBZ is) (NP (DT a) (ADJP (JJ positive) (PRN (-LRB- -LRB-) (ADJP (JJ negative)) (-RRB- -RRB-))) (NN value)))))) (. .))
(S (ADVP (RB Therefore)) (, ,) (NP (NP (DT the) (NN degree)) (PP (IN of) (NP (NP (DT the) (NN contribution)) (PP (IN of) (NP (NP (DT a) (NN task)) (PP (IN to) (NP (DT the) (NN parameter) (NNS updates)))))))) (VP (VBZ is) (VP (VBN controlled) (PP (IN by) (S (VP (VBG introducing) (NP (NP (DT a) (NN set)) (PP (IN of) (NP (NP (NNS weights)) (PP (IN on) (NP (NP (DT the) (NN loss) (NN function)) (PP (IN of) (NP (DT the) (NNS tasks))))))))))))) (. .))
(S (NP (PRP$ Our) (NN method)) (VP (MD can) (VP (VB be) (ADVP (RB easily)) (VP (VBN integrated) (PP (IN with) (NP (NP (DT the) (JJ current) (NML (NN meta) (HYPH -) (NN learning)) (NNS algorithms)) (PP (IN for) (NP (JJ neural) (NNS networks)))))))) (. .))
(S (NP (PRP$ Our) (NNS experiments)) (VP (VBP demonstrate) (SBAR (IN that) (S (NP (PRP it)) (VP (VBZ yields) (NP (NNS models)) (PP (IN with) (NP (NP (JJR better) (NN generalization)) (PP (VBN compared) (PP (IN to) (NP (NNP MAML) (CC and) (NNP Reptile)))))))))) (. .))
