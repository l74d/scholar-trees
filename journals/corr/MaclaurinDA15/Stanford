(S (S (VP (VBG Tuning) (NP (NP (NNS hyperparameters)) (PP (IN of) (NP (VBG learning) (NNS algorithms)))))) (VP (VBZ is) (ADJP (JJ hard)) (SBAR (IN because) (S (NP (NNS gradients)) (VP (VBP are) (ADVP (RB usually)) (ADJP (JJ unavailable)))))) (. .))
(S (NP (PRP We)) (VP (VBP compute) (NP (NP (JJ exact) (NNS gradients)) (PP (IN of) (NP (NN cross-validation) (NN performance)))) (PP (IN with) (NP (NN respect))) (PP (IN to) (NP (DT all) (NNS hyperparameters))) (PP (IN by) (S (VP (VBG chaining) (NP (NNS derivatives)) (ADVP (RB backwards)) (PP (IN through) (NP (DT the) (JJ entire) (NN training) (NN procedure))))))) (. .))
(S (NP (DT These) (NNS gradients)) (VP (VBP allow) (S (NP (PRP us)) (VP (TO to) (VP (VB optimize) (NP (NP (NNS thousands)) (PP (IN of) (NP (NNS hyperparameters))) (, ,) (PP (VBG including) (NP (NP (NML (NN step) (HYPH -) (NN size)) (CC and) (NN momentum) (NNS schedules)) (, ,) (NP (NML (NN weight) (NN initialization)) (NNS distributions)) (, ,) (NP (ADJP (RB richly) (JJ parameterized)) (NN regularization) (NNS schemes)) (, ,) (CC and) (NP (NML (JJ neural) (NN network)) (NNS architectures))))))))) (. .))
(S (NP (PRP We)) (VP (VBP compute) (NP (NN hyperparameter) (NNS gradients)) (PP (IN by) (S (ADVP (RB exactly)) (VP (VBG reversing) (NP (NP (DT the) (NNS dynamics)) (PP (IN of) (NP (JJ stochastic) (NN gradient) (NN descent)))) (PP (IN with) (NP (NN momentum))))))) (. .))
