(S (NP (PRP We)) (VP (VBP introduce) (NP (DT a) (JJ model-based) (NN image) (NN reconstruction) (NN framework)) (PP (IN with) (NP (DT a) (ADJP (NN convolution) (JJ neural) (NN network) (PRN (-LRB- -LRB-) (NP (NNP CNN)) (-RRB- -RRB-)) (VBN based)) (NN regularization) (RB prior)))) (. .))
(S (NP (DT The) (VBN proposed) (NN formulation)) (VP (VBZ provides) (NP (NP (DT a) (JJ systematic) (NN approach)) (PP (IN for) (S (VP (VBG deriving) (NP (NP (JJ deep) (NNS architectures)) (PP (IN for) (NP (NP (JJ inverse) (NNS problems)) (PP (IN with) (NP (DT the) (JJ arbitrary) (NN structure))))))))))) (. .))
(S (SBAR (IN Since) (S (NP (DT the) (NN forward) (NN model)) (VP (VBZ is) (VP (ADVP (RB explicitly)) (VBN accounted) (PP (IN for)))))) (, ,) (NP (NP (DT a) (JJR smaller) (NN network)) (PP (IN with) (NP (JJR fewer) (NNS parameters)))) (VP (VBZ is) (ADJP (JJ sufficient) (S (VP (TO to) (VP (VB capture) (NP (DT the) (NN image) (NN information)))))) (PP (VBN compared) (PP (TO to) (NP (JJ black-box) (JJ deep) (NN learning) (NNS approaches)))) (, ,) (S (ADVP (RB thus)) (VP (VBG reducing) (NP (NP (DT the) (NN demand)) (PP (IN for) (NP (NP (VBG training) (NNS data)) (CC and) (NP (NN training) (NN time)))))))) (. .))
(S (SBAR (IN Since) (S (NP (PRP we)) (VP (VBP rely) (PP (IN on) (NP (JJ end-to-end) (NN training)))))) (, ,) (NP (DT the) (NNP CNN) (NNS weights)) (VP (VBP are) (VP (VBN customized) (PP (TO to) (NP (DT the) (NN forward) (NN model))) (, ,) (S (ADVP (RB thus)) (VP (VBG offering) (NP (NP (VBN improved) (NN performance)) (PP (IN over) (NP (NP (NNS approaches)) (SBAR (WHNP (WDT that)) (S (VP (VBP rely) (PP (IN on) (NP (JJ pre-trained) (NNS denoisers))))))))))))) (. .))
(S (NP (NP (DT The) (JJ main) (NN difference)) (PP (IN of) (NP (DT the) (NN framework))) (PP (IN from) (NP (VBG existing) (JJ end-to-end) (NN training) (NNS strategies)))) (VP (VBZ is) (NP (NP (DT the) (NN sharing)) (PP (IN of) (NP (DT the) (NN network) (NNS weights))) (PP (IN across) (NP (NNS iterations) (CC and) (NNS channels))))) (. .))
(S (NP (PRP$ Our) (NNS experiments)) (VP (VBP show) (SBAR (IN that) (S (NP (NP (DT the) (NN decoupling)) (PP (IN of) (NP (NP (DT the) (NN number)) (PP (IN of) (NP (NNS iterations))))) (PP (IN from) (NP (NP (DT the) (NN network) (NN complexity)) (VP (VBN offered) (PP (IN by) (NP (DT this) (NN approach))))))) (VP (VBZ provides) (NP (NP (NNS benefits)) (PP (VBG including) (NP (NP (NP (JJR lower) (NN demand)) (PP (IN for) (NP (VBG training) (NNS data)))) (, ,) (NP (NP (VBN reduced) (NN risk)) (PP (IN of) (NP (VBG overfitting)))) (, ,) (CC and) (NP (NP (NNS implementations)) (PP (IN with) (NP (ADJP (RB significantly) (VBN reduced)) (NN memory) (NN footprint))))))))))) (. .))
(S (S (NP (PRP We)) (VP (VBP propose) (S (VP (TO to) (VP (VB enforce) (NP (NN data-consistency)) (PP (IN by) (S (VP (VBG using) (NP (NP (JJ numerical) (NN optimization) (NNS blocks)) (PP (JJ such) (IN as) (NP (NN conjugate) (NNS gradients) (VBP algorithm))) (PP (IN within) (NP (DT the) (NN network)))))))))))) (: ;) (S (NP (DT this) (NN approach)) (VP (VBZ offers) (NP (NP (JJR faster) (NN convergence)) (PP (IN per) (NP (NN iteration)))) (, ,) (PP (VBN compared) (PP (TO to) (NP (NP (NNS methods)) (SBAR (WHNP (WDT that)) (S (VP (VBP rely) (PP (IN on) (NP (JJ proximal) (NNS gradients) (NNS steps))) (S (VP (TO to) (VP (VB enforce) (NP (NNS data) (NN consistency))))))))))))) (. .))
(S (NP (PRP$ Our) (NNS experiments)) (VP (VBP show) (SBAR (IN that) (S (NP (DT the) (JJR faster) (NN convergence)) (VP (NNS translates) (PP (TO to) (NP (JJ improved) (NN performance))) (, ,) (SBAR (WHADVP (ADVP (RB especially)) (WRB when)) (S (NP (DT the) (JJ available) (NNP GPU) (NN memory)) (VP (VBZ restricts) (NP (NP (DT the) (NN number)) (PP (IN of) (NP (NNS iterations))))))))))) (. .))
