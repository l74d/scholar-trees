(S (NP (NN Error) (NN backpropagation)) (VP (VBZ is) (NP (NP (DT a) (ADJP (RB highly) (JJ effective)) (NN mechanism)) (PP (IN for) (S (VP (VBG learning) (NP (NP (NML (JJ high) (HYPH -) (NN quality)) (JJ hierarchical) (NNS features)) (PP (IN in) (NP (JJ deep) (NNS networks))))))))) (. .))
(S (S (VP (VBG Updating) (NP (NP (DT the) (NNS features) (CC or) (NNS weights)) (PP (IN in) (NP (CD one) (NN layer)))))) (, ,) (ADVP (RB however)) (, ,) (VP (VBZ requires) (S (VP (VBG waiting) (PP (IN for) (NP (NP (DT the) (NN propagation)) (PP (IN of) (NP (NN error) (NNS signals))))) (PP (IN from) (NP (JJR higher) (NNS layers)))))) (. .))
(S (S (VP (VBG Learning) (S (VP (VBG using) (NP (ADJP (VBN delayed) (CC and) (JJ non-local)) (NNS errors)))))) (VP (VBZ makes) (S (NP (PRP it)) (ADJP (JJ hard) (S (VP (TO to) (VP (VB reconcile) (NP (NN backpropagation)) (PP (IN with) (NP (NP (DT the) (NN learning) (NNS mechanisms)) (VP (VBN observed) (PP (IN in) (NP (JJ biological) (JJ neural) (NNS networks))) (SBAR (IN as) (S (NP (PRP it)) (VP (VBZ requires) (NP (DT the) (NNS neurons)) (S (VP (TO to) (VP (VB maintain) (NP (NP (DT a) (NN memory)) (PP (IN of) (NP (DT the) (NN input))))))))))))) (ADVP (ADVP (RB long) (RB enough)) (SBAR (IN until) (S (NP (DT the) (NML (JJR higher) (HYPH -) (NN layer)) (NNS errors)) (VP (VBP arrive))))))))))) (. .))
(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (DT an) (JJ alternative) (NN learning) (NN mechanism)) (SBAR (WHADVP (WRB where)) (S (NP (NNS errors)) (VP (VBP are) (VP (VBN generated) (ADVP (RB locally)) (PP (IN in) (NP (NP (DT each) (NN layer)) (VP (VBG using) (NP (NP (VBN fixed)) (, ,) (NP (JJ random) (JJ auxiliary) (NNS classifiers))))))))))) (. .))
(S (S (NP (JJR Lower) (NNS layers)) (VP (MD could) (ADVP (RB thus)) (VP (VB be) (VP (VBN trained) (ADVP (RB independently)) (PP (IN of) (NP (JJR higher) (NNS layers))))))) (CC and) (S (NP (NN training)) (VP (MD could) (ADVP (CC either)) (VP (VB proceed) (NP (NN layer)) (PP (PP (IN by) (NP (NN layer))) (, ,) (CC or) (ADVP (RB simultaneously)) (PP (IN in) (NP (DT all) (NNS layers)))) (S (VP (VBG using) (NP (JJ local) (NN error) (NN information))))))) (. .))
(S (NP (PRP We)) (VP (VP (VBP address) (NP (NP (JJ biological) (NN plausibility) (NNS concerns)) (PP (JJ such) (IN as) (NP (NML (NN weight) (NN symmetry)) (NNS requirements))))) (CC and) (VP (VBP show) (SBAR (IN that) (S (NP (NP (DT the) (VBN proposed) (NN learning) (NN mechanism)) (PP (VBN based) (PP (IN on) (NP (NP (ADJP (VBN fixed) (, ,) (JJ broad) (, ,) (CC and) (JJ random)) (NN tuning)) (PP (IN of) (NP (NP (DT each) (NN neuron)) (PP (IN to) (NP (DT the) (NN classification) (NNS categories))))))))) (VP (VBZ outperforms) (NP (DT the) (ADJP (RB biologically) (HYPH -) (VBN motivated)) (NN feedback) (NN alignment)) (S (VP (VBG learning) (NP (NN technique)) (PP (IN on) (NP (DT the) (NML (NN MNIST) (, ,) (NN CIFAR10) (, ,) (CC and) (NN SVHN)) (NNS datasets))))) (, ,) (S (VP (VBG approaching) (NP (NP (DT the) (NN performance)) (PP (IN of) (NP (JJ standard) (NN backpropagation))))))))))) (. .))
(S (NP (PRP$ Our) (NN approach)) (VP (VBZ highlights) (NP (NP (DT a) (JJ potential) (JJ biological) (NN mechanism)) (PP (IN for) (NP (NP (DT the) (ADJP (ADJP (JJ supervised)) (, ,) (CC or) (ADJP (NN task) (HYPH -) (JJ dependent)) (, ,)) (NN learning)) (PP (IN of) (NP (NN feature) (NNS hierarchies))))))) (. .))
(S (PP (IN In) (NP (NN addition))) (, ,) (NP (PRP we)) (VP (VBP show) (SBAR (IN that) (S (NP (PRP it)) (VP (VBZ is) (ADJP (RB well) (JJ suited) (PP (IN for) (S (VP (VBG learning) (NP (JJ deep) (NNS networks)) (PP (IN in) (NP (NN custom) (NN hardware))))))) (SBAR (WHADVP (WRB where)) (S (NP (PRP it)) (VP (MD can) (ADVP (RB drastically)) (VP (VB reduce) (NP (NML (NML (NN memory) (NN traffic)) (CC and) (NML (NNS data))) (NN communication) (NNS overheads)))))))))) (. .))
