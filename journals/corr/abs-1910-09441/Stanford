(S (NP (PRP We)) (VP (VBP present) (NP (NP (NP (DT a) (JJ novel) (NN algorithm)) (-LRB- -LRB-) (NP (NNP DeepMNavigate)) (-RRB- -RRB-)) (PP (IN for) (NP (NP (JJ global) (JJ multi-agent) (NN navigation)) (PP (IN in) (NP (NP (JJ dense) (NNS scenarios)) (VP (VBG using) (NP (JJ deep) (NN reinforcement) (NN learning)))))))) (PRN (-LRB- -LRB-) (NP (NN DRL)) (-RRB- -RRB-))) (. .))
(S (S (NP (PRP$ Our) (NN approach)) (VP (VBZ uses) (ADJP (JJ local)))) (CC and) (S (NP (NP (JJ global) (NN information)) (PP (IN for) (NP (NP (DT each) (NN robot)) (PP (IN from) (NP (NN motion) (NN information)))))) (VP (VBZ maps))) (. .))
(S (NP (PRP We)) (VP (VBP use) (NP (NP (DT a) (NML (CD three) (HYPH -) (NN layer)) (NNP CNN)) (SBAR (IN that) (S (VP (VBZ takes) (NP (DT these) (NNS maps)) (PP (IN as) (NP (NN input) (S (VP (TO to) (VP (VB generate) (NP (DT a) (JJ suitable) (NN action)) (S (VP (TO to) (VP (VB drive) (NP (DT each) (NN robot)) (PP (IN to) (NP (PRP$ its) (NN goal) (NN position)))))))))))))))) (. .))
(S (NP (PRP$ Our) (NN approach)) (VP (VP (VBZ is) (ADJP (JJ general))) (, ,) (VP (VBZ learns) (NP (DT an) (JJ optimal) (NN policy)) (S (VP (VBG using) (NP (DT a) (JJ multi-scenario) (, ,) (JJ multi-state) (NN training) (NN algorithm))))) (, ,) (CC and) (VP (MD can) (ADVP (RB directly)) (VP (VB handle) (NP (JJ raw) (NN sensor) (NNS measurements)) (PP (IN for) (NP (JJ local) (NNS observations)))))) (. .))
(S (NP (PRP We)) (VP (VBP demonstrate) (NP (NP (DT the) (NN performance)) (PP (IN on) (NP (NP (JJ dense) (, ,) (JJ complex) (NNS benchmarks)) (PP (IN with) (NP (JJ narrow) (NNS passages) (CC and) (NNS environments)))))) (PP (IN with) (NP (NP (NNS tens)) (PP (IN of) (NP (NNS agents)))))) (. .))
(S (NP (PRP We)) (VP (VBP highlight) (NP (NP (NP (NP (DT the) (NN algorithm) (POS 's)) (NNS benefits)) (PP (IN over) (NP (JJ prior) (NN learning) (NNS methods)))) (CC and) (NP (NP (JJ geometric) (JJ decentralized) (NNS algorithms)) (PP (IN in) (NP (JJ complex) (NNS scenarios)))))) (. .))
