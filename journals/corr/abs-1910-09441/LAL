(S (NP (PRP We)) (VP (VBP present) (NP (NP (DT a) (JJ novel) (NN algorithm)) (PRN (-LRB- -LRB-) (NP (NNP DeepMNavigate)) (-RRB- -RRB-)) (PP (IN for) (NP (NP (JJ global) (JJ multi-agent) (NN navigation)) (PP (IN in) (NP (JJ dense) (NNS scenarios))))) (VP (VBG using) (NP (NP (JJ deep) (NN reinforcement) (NN learning)) (PRN (-LRB- -LRB-) (NP (NNP DRL)) (-RRB- -RRB-)))))) (. .))
(S (NP (PRP$ Our) (NN approach)) (VP (VBZ uses) (NP (NP (ADJP (JJ local) (CC and) (JJ global)) (NN information)) (PP (IN for) (NP (DT each) (NN robot))) (PP (IN from) (NP (NN motion) (NN information) (NNS maps))))) (. .))
(S (NP (PRP We)) (VP (VBP use) (NP (NP (DT a) (JJ three-layer) (NNP CNN)) (SBAR (WHNP (WDT that)) (S (VP (VBZ takes) (NP (DT these) (NNS maps)) (PP (IN as) (NP (NN input))))))) (S (VP (TO to) (VP (VB generate) (NP (NP (DT a) (JJ suitable) (NN action)) (SBAR (S (VP (TO to) (VP (VB drive) (NP (DT each) (NN robot)) (PP (TO to) (NP (PRP$ its) (NN goal) (NN position)))))))))))) (. .))
(S (NP (PRP$ Our) (NN approach)) (VP (VP (VBZ is) (ADJP (JJ general))) (, ,) (VP (VBZ learns) (NP (DT an) (JJ optimal) (NN policy)) (S (VP (VBG using) (NP (DT a) (NN multi-scenario) (, ,) (JJ multi-state) (NN training) (NN algorithm))))) (, ,) (CC and) (VP (MD can) (VP (ADVP (RB directly)) (VB handle) (NP (NP (JJ raw) (NN sensor) (NNS measurements)) (PP (IN for) (NP (JJ local) (NNS observations))))))) (. .))
(S (NP (PRP We)) (VP (VBP demonstrate) (NP (DT the) (NN performance)) (PP (IN on) (NP (NP (NN dense) (, ,) (JJ complex) (NNS benchmarks)) (PP (IN with) (NP (NP (JJ narrow) (NNS passages)) (CC and) (NP (NP (NNS environments)) (PP (IN with) (NP (NP (NNS tens)) (PP (IN of) (NP (NNS agents))))))))))) (. .))
(S (NP (PRP We)) (VP (VBD highlight) (NP (NP (NP (DT the) (NN algorithm) (POS 's)) (NNS benefits)) (PP (IN over) (NP (NP (JJ prior) (VBG learning) (NNS methods)) (CC and) (NP (JJ geometric) (VBN decentralized) (NN algorithms)))) (PP (IN in) (NP (JJ complex) (NNS scenarios))))) (. .))
