(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (DT a) (JJ distributed) (NN zeroth-order) (NN policy) (NN optimization) (NN method)) (PP (IN for) (NP (NP (NNP Multi-Agent) (NNP Reinforcement) (NNP Learning)) (PRN (-LRB- -LRB-) (NP (NNP MARL)) (-RRB- -RRB-)))))) (. .))
(S (NP (VBG Existing) (NNP MARL) (RB algorithms)) (ADVP (RB often)) (VP (VBP assume) (SBAR (IN that) (S (NP (DT every) (NN agent)) (VP (MD can) (VP (VB observe) (NP (NP (DT the) (NNS states) (CC and) (NNS actions)) (PP (IN of) (NP (NP (PDT all) (DT the) (JJ other) (NNS agents)) (PP (IN in) (NP (DT the) (NN network))))))))))) (. .))
(S (NP (DT This)) (VP (MD can) (VP (VB be) (ADJP (JJ impractical)) (PP (IN in) (NP (NP (JJ large-scale) (NNS problems)) (, ,) (SBAR (WHADVP (WRB where)) (S (S (VP (VBG sharing) (NP (DT the) (NP (NP (NP (NN state) (CC and)))) (NN action) (NN information)) (PP (IN with) (NP (JJ multi-hop) (NNS neighbors))))) (VP (MD may) (VP (VB incur) (NP (JJ significant) (NN communication) (NN overhead)))))))))) (. .))
(S (NP (NP (DT The) (NN advantage)) (PP (IN of) (NP (DT the) (VBN proposed) (NN zeroth-order) (NN policy) (NN optimization) (NN method)))) (VP (VBZ is) (SBAR (IN that) (S (NP (PRP it)) (VP (VBZ allows) (S (NP (DT the) (NNS agents)) (VP (TO to) (VP (VB compute) (NP (NP (DT the) (JJ local) (NN policy) (NNS gradients)) (VP (VBN needed) (S (VP (TO to) (VP (VB update) (NP (PRP$ their) (JJ local) (NN policy) (NNS functions))))))) (S (VP (VBG using) (NP (NP (JJ local) (NNS estimates)) (PP (IN of) (NP (DT the) (JJ global) (JJ accumulated) (NNS rewards))) (SBAR (WHNP (WDT that)) (S (VP (VP (VBP depend) (PP (IN on) (NP (JJ partial) (NN state) (CC and) (NN action) (NN information))) (ADVP (RB only))) (CC and) (VP (MD can) (VP (VB be) (VP (VBN obtained) (S (VP (VBG using) (NP (NN consensus)))))))))))))))))))) (. .))
(S (ADVP (RB Specifically)) (, ,) (S (VP (TO to) (VP (VB calculate) (NP (DT the) (JJ local) (NN policy) (NNS gradients))))) (, ,) (NP (PRP we)) (VP (VBP develop) (NP (NP (DT a) (JJ new) (VBN distributed) (NN zeroth-order) (NN policy) (NN gradient) (NN estimator)) (SBAR (WHNP (WDT that)) (S (VP (VBZ relies) (PP (IN on) (NP (NP (NN one-point) (NN residual-feedback)) (SBAR (WHNP (WDT which)) (, ,) (S (PP (VBN compared) (PP (TO to) (NP (NP (VBG existing) (NN zeroth-order) (NNS estimators)) (SBAR (WHNP (WDT that)) (S (ADVP (RB also)) (VP (VBP rely) (PP (IN on) (NP (JJ one-point) (NN feedback))))))))) (, ,) (VP (ADVP (RB significantly)) (VBZ reduces) (NP (NP (DT the) (NN variance)) (PP (IN of) (NP (DT the) (NN policy) (NN gradient) (NNS estimates)))) (S (VP (VBG improving) (, ,) (PP (IN in) (NP (DT this) (NN way))) (, ,) (NP (DT the) (JJ learning) (NN performance)))))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP show) (SBAR (IN that) (S (NP (NP (DT the) (VBN proposed) (JJ distributed) (NN zeroth-order) (NN policy) (NN optimization) (NN method)) (PP (IN with) (NP (JJ constant) (NN stepsize)))) (VP (NNS converges) (PP (TO to) (NP (NP (DT a) (NN neighborhood)) (PP (IN of) (NP (DT the) (JJ global) (NN optimal) (NN policy))) (SBAR (WHNP (WDT that)) (S (VP (VBZ depends) (PP (IN on) (NP (NP (DT the) (NN number)) (PP (IN of) (NP (NP (NN consensus) (NNS steps)) (VP (VBN used) (S (VP (TO to) (VP (VB calculate) (NP (NP (DT the) (JJ local) (NNS estimates)) (PP (IN of) (NP (DT the) (JJ global) (JJ accumulated) (NNS rewards))))))))))))))))))))) (. .))
(S (ADVP (RB Moreover)) (, ,) (NP (PRP we)) (VP (VBP provide) (NP (NP (JJ numerical) (NNS experiments)) (SBAR (WHNP (WDT that)) (S (VP (VBP demonstrate) (SBAR (IN that) (S (NP (PRP$ our) (JJ new) (NN zeroth-order) (NN policy) (NN gradient) (NN estimator)) (VP (VBZ is) (ADJP (RBR more) (JJ sample-efficient)) (PP (VBN compared) (PP (TO to) (NP (JJ other) (VBG existing) (JJ one-point) (NNS estimators)))))))))))) (. .))
