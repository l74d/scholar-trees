(S (NP (PRP We)) (VP (VBP develop) (NP (NP (DT a) (ADJP (JJ scalable) (CC and) (JJ extendable)) (NN training) (NN framework)) (SBAR (WHNP (WDT that)) (S (VP (MD can) (VP (VP (VB utilize) (NP (NNP GPUs)) (PP (IN across) (NP (NP (NNS nodes)) (PP (IN in) (NP (DT a) (NN cluster)))))) (CC and) (VP (VB accelerate) (NP (NP (DT the) (NN training)) (PP (IN of) (NP (NP (JJ deep) (NN learning) (NNS models)) (VP (VBN based) (PP (IN on) (NP (NNS data) (NN parallelism)))))))))))))) (. .))
(S (NP (DT Both) (JJ synchronous) (CC and) (JJ asynchronous) (NN training)) (VP (VBP are) (VP (VBN implemented) (PP (IN in) (NP (NP (PRP$ our) (NN framework)) (, ,) (SBAR (WHADVP (WRB where)) (S (NP (NP (JJ parameter) (NN exchange)) (PP (IN among) (NP (NNP GPUs)))) (VP (VBZ is) (VP (VBN based) (PP (IN on) (NP (NNP CUDA-aware) (NNP MPI))))))))))) (. .))
(S (PP (IN In) (NP (DT this) (NN report))) (, ,) (NP (PRP we)) (VP (VBP analyze) (NP (NP (DT the) (NN convergence) (CC and) (NN capability)) (PP (IN of) (NP (DT the) (NN framework))) (SBAR (S (VP (TO to) (VP (VB reduce) (NP (NN training) (NN time)) (SBAR (WHADVP (WRB when)) (S (VP (VBG scaling) (NP (NP (DT the) (JJ synchronous) (NN training)) (PP (IN of) (NP (NNP AlexNet) (CC and) (NNP GoogLeNet)))) (PP (IN from) (NP (CD 2) (NNP GPUs))) (PP (TO to) (NP (CD 8) (NNP GPUs)))))))))))) (. .))
(S (PP (IN In) (NP (NN addition))) (, ,) (NP (PRP we)) (VP (VBP explore) (NP (NP (JJ novel) (NNS ways)) (SBAR (S (VP (TO to) (VP (VB reduce) (NP (NP (DT the) (NN communication) (NN overhead)) (VP (VBN caused) (PP (IN by) (S (VP (VBG exchanging) (NP (NNS parameters))))))))))))) (. .))
(S (ADVP (RB Finally)) (, ,) (NP (PRP we)) (VP (VBP release) (NP (DT the) (NN framework)) (PP (IN as) (ADJP (NN open-source))) (PP (IN for) (NP (NP (JJ further) (NN research)) (PP (IN on) (NP (JJ distributed) (JJ deep) (NN learning)))))))
