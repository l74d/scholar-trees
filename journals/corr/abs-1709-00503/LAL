(S (NP (PRP We)) (VP (VBP propose) (NP (NP (DT a) (JJ new) (NN algorithm)) (, ,) (NP (NP (NNP Mean) (NNP Actor-Critic)) (PRN (-LRB- -LRB-) (NP (NNP MAC)) (-RRB- -RRB-))) (, ,) (PP (IN for) (NP (JJ discrete-action) (JJ continuous-state) (NN reinforcement) (NN learning))))) (. .))
(S (NP (NNP MAC)) (VP (VBZ is) (NP (NP (DT a) (NN policy) (NN gradient) (NN algorithm)) (SBAR (WHNP (WDT that)) (S (VP (VBZ uses) (NP (NP (NP (DT the) (NN agent) (POS 's)) (JJ explicit) (NN representation)) (PP (IN of) (NP (DT all) (NN action) (NNS values)))) (S (VP (TO to) (VP (VB estimate) (NP (NP (DT the) (NN gradient)) (PP (IN of) (NP (DT the) (NN policy))))))) (, ,) (PP (RB rather) (IN than) (S (VP (VBG using) (NP (NP (RB only) (DT the) (NNS actions)) (SBAR (WHNP (WDT that)) (S (VP (VBD were) (ADVP (RB actually)) (VP (VBN executed)))))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP prove) (SBAR (IN that) (S (NP (DT this) (NN approach)) (VP (VBZ reduces) (NP (NP (NN variance)) (PP (IN in) (NP (DT the) (NN policy) (NN gradient) (NN estimate)))) (ADVP (NN relative) (PP (TO to) (NP (JJ traditional) (JJ actor-critic) (NNS methods)))))))) (. .))
(S (NP (PRP We)) (VP (VBP show) (NP (NP (JJ empirical) (NNS results)) (PP (PP (IN on) (NP (CD two) (NN control) (NNS domains))) (CC and) (PP (IN on) (NP (NP (CD six) (NNP Atari) (NNS games)) (, ,) (SBAR (WHADVP (WRB where)) (S (NP (NNP MAC)) (VP (VBZ is) (ADJP (JJ competitive) (PP (IN with) (NP (JJ state-of-the-art) (NN policy) (NN search) (NN algorithms)))))))))))) (. .))
