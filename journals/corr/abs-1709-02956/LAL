(S (NP (NP (JJ Residual) (NNP Network)) (PRN (-LRB- -LRB-) (NP (NNP ResNet)) (-RRB- -RRB-))) (VP (VBZ is) (NP (NP (DT the) (JJ state-of-the-art) (NN architecture)) (SBAR (WHNP (WDT that)) (S (VP (VBZ realizes) (NP (NP (JJ successful) (NN training)) (PP (IN of) (NP (ADJP (RB really) (JJ deep)) (JJ neural) (NN network))))))))) (. .))
(S (NP (PRP It)) (VP (VBZ is) (ADVP (RB also)) (VP (VBN known) (SBAR (IN that) (S (NP (NP (JJ good) (NN weight) (NN initialization)) (PP (IN of) (NP (JJ neural) (NN network)))) (VP (NNS avoids) (NP (NP (NN problem)) (PP (IN of) (NP (VBG vanishing/exploding) (NNS gradients))))))))) (. .))
(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (NP (NP (VBD simplified) (NNS models)) (PP (IN of) (NP (NNS ResNets)))) (VP (VBP are) (VP (VBN analyzed))) (. .))
(S (NP (PRP We)) (VP (VBP argue) (SBAR (IN that) (S (NP (NP (NN goodness)) (PP (IN of) (NP (NNP ResNet)))) (VP (VBZ is) (VP (VBN correlated) (PP (IN with) (NP (DT the) (NN fact) (SBAR (IN that) (S (NP (NNS ResNets)) (VP (VBP are) (ADJP (RB relatively) (JJ insensitive) (PP (TO to) (NP (NP (NN choice)) (PP (IN of) (NP (JJ initial) (NNS weights)))))))))))))))) (. .))
(S (NP (PRP We)) (ADVP (RB also)) (VP (VB demonstrate) (SBAR (WHADVP (WRB how)) (S (NP (JJ batch) (NN normalization)) (VP (VBZ improves) (NP (NP (NN backpropagation)) (PP (IN of) (NP (JJ deep) (NNS ResNets)))) (PP (IN without) (S (VP (VBG tuning) (NP (NP (JJ initial) (NNS values)) (PP (IN of) (NP (NNS weights))))))))))) (. .))
