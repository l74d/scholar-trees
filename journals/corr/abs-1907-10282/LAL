(S (NP (DT The) (JJ extreme) (NN learning) (NN machine)) (VP (VBZ needs) (NP (NP (DT a) (JJ large) (NN number)) (PP (IN of) (NP (JJ hidden) (NNS nodes)))) (S (VP (TO to) (VP (VB generalize) (NP (DT a) (JJ single) (NN hidden) (NN layer) (JJ neural) (NN network)) (PP (IN for) (NP (DT a) (VBN given) (NN training) (NN data-set))))))) (. .))
(S (NP (NP (DT The) (NN need)) (PP (IN for) (NP (NP (JJR more) (NN number)) (PP (IN of) (NP (JJ hidden) (NNS nodes)))))) (VP (VBZ suggests) (SBAR (IN that) (S (NP (DT the) (NN neural-network)) (VP (VBZ is) (VP (VP (VBG memorizing)) (CONJP (RB rather) (IN than)) (VBG generalizing) (NP (DT the) (NN model))))))) (. .))
(S (ADVP (NNP Hence)) (, ,) (NP (NP (DT a) (JJ supervised) (NN learning) (NN method))) (VP (VBZ is) (VP (VBN described) (ADVP (RB here)) (SBAR (WHNP (DT that)) (S (VP (VBZ uses) (NP (JJ Moore-Penrose) (NN approximation)) (S (VP (TO to) (VP (VB determine) (NP (DT both) (JJ input-weight) (CC and) (JJ output-weight)) (PP (IN in) (NP (NP (CD two) (NNS epochs)) (, ,) (ADVP (RB namely)) (, ,) (NP (NN backward-pass) (CC and) (NN forward-pass)))))))))))) (. .))
(S (NP (DT The) (VBN proposed) (NN technique)) (VP (VP (VBZ has) (NP (NP (DT an) (NN advantage)) (PP (IN over) (NP (DT the) (NN back-propagation) (NN method)))) (PP (IN in) (NP (NP (NNS terms)) (PP (IN of) (NP (NP (NNS iterations)) (VP (VBN required))))))) (CC and) (VP (VBZ is) (ADJP (JJ superior) (PP (TO to) (NP (DT the) (JJ extreme) (NN learning) (NN machine)))) (PP (IN in) (NP (NP (NNS terms)) (PP (IN of) (NP (NP (DT the) (NN number)) (PP (IN of) (NP (JJ hidden) (NNS units))) (ADJP (JJ necessary) (PP (IN for) (NP (NN generalization)))))))))) (. .))
