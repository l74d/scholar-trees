(S (PP (RB Rather) (IN than) (S (VP (VBG learning) (NP (JJ new) (NN control) (NNS policies)) (PP (IN for) (NP (DT each) (JJ new) (NN task)))))) (, ,) (NP (NP (PRP it))) (VP (VBZ is) (ADJP (JJ possible)) (, ,) (SBAR (WHADVP (WRB when)) (S (NP (NNS tasks)) (VP (NN share) (NP (DT some) (NN structure))))) (, ,) (S (VP (TO to) (VP (VB compose) (NP (DT a) (`` ``) (NN meta-policy) ('' '')) (PP (IN from) (NP (ADJP (RB previously) (VBN learned)) (NNS policies))))))) (. .))
(S (NP (DT This) (NN paper)) (VP (VBZ reports) (NP (NP (NNS results)) (PP (IN from) (NP (NP (NNS experiments)) (VP (VBG using) (NP (NNP Deep) (NNP Reinforcement) (NNP Learning)) (PP (IN on) (NP (DT a) (JJ continuous-state) (, ,) (JJ discrete-action) (JJ autonomous) (VBG driving) (NN simulator)))))))) (. .))
(S (NP (PRP We)) (VP (VBP explore) (SBAR (SBAR (WHADVP (WRB how)) (S (NP (JJ Deep) (JJ Neural) (NNS Networks)) (VP (MD can) (VP (VB represent) (NP (NP (NNS meta-policies)) (SBAR (WHNP (IN that)) (S (VP (NN switch) (PP (IN among) (NP (NP (DT a) (NN set)) (PP (IN of) (NP (ADJP (RB previously) (VBN learned)) (NNS policies))))))))))))) (, ,) (PP (ADVP (RB specifically)) (IN in) (NP (NP (NNS settings)) (SBAR (SBAR (WHADVP (WRB where)) (S (NP (NP (DT the) (NNS dynamics)) (PP (IN of) (NP (DT a) (JJ new) (NN scenario)))) (VP (VBP are) (VP (VBN composed) (PP (IN of) (NP (NP (DT a) (NN mixture)) (PP (IN of) (NP (ADJP (RB previously) (VBN learned)) (NNS dynamics))))))))) (CC and) (SBAR (WHADVP (WRB where)) (S (NP (DT the) (NN state) (NN observation)) (VP (VBZ is) (ADVP (RB possibly)) (VP (VBN corrupted) (PP (IN by) (S (VP (VBG sensing) (NP (NN noise)))))))))))))) (. .))
(S (NP (PRP We)) (ADVP (RB also)) (VP (VBP report) (NP (NP (DT the) (NNS results)) (PP (IN of) (NP (NP (NNS experiments)) (VP (VBG varying) (NP (NP (NNS dynamics) (NNS mixes)) (, ,) (NP (NN distractor) (NNS policies)) (, ,) (NP (NP (NNS magnitudes/distributions)) (PP (IN of) (S (VP (VBG sensing) (NP (NN noise)))))) (, ,) (CC and) (NP (NNS obstacles)))))))) (. .))
(S (PP (IN In) (NP (DT a) (ADJP (RB fully) (JJ observed)) (NN experiment))) (, ,) (NP (DT the) (NN meta-policy) (NN learning) (JJ algorithm)) (VP (VBZ achieves) (NP (NP (CD 2.6x) (DT the) (NN reward)) (VP (VBN achieved) (PP (IN by) (NP (DT the) (ADJP (JJ next) (NN best)) (NN policy) (NN composition) (NN technique))))) (PP (IN with) (NP (ADJP (NP (CD 80) (NN %)) (JJR less)) (NN exploration)))) (. .))
(S (PP (IN In) (NP (DT a) (ADJP (RB partially) (JJ observed)) (NN experiment))) (, ,) (NP (DT the) (NN meta-policy) (NN learning) (NN algorithm)) (VP (NNS converges) (PP (IN after) (NP (CD 50) (NNS iterations))) (SBAR (IN while) (S (NP (NP (DT a) (JJ direct) (NN application)) (PP (IN of) (NP (NNP RL)))) (VP (VBZ fails) (S (VP (TO to) (VP (VB converge) (PP (ADVP (RB even)) (IN after) (NP (CD 200) (NNS iterations)))))))))) (. .))
