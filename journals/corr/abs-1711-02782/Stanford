(S (NP (NP (JJ Recurrent) (JJ Neural) (NNS Networks)) (-LRB- -LRB-) (NP (NNS RNNs)) (-RRB- -RRB-)) (VP (VBP are) (VP (VBN used) (PP (IN in) (NP (NML (NML (NN state)) (HYPH -) (PP (IN of) (HYPH -) (NP (DT the) (HYPH -) (NN art)))) (NNS models))) (PP (IN in) (NP (NP (NNS domains)) (PP (JJ such) (IN as) (NP (NML (NML (NN speech) (NN recognition)) (, ,) (NML (NN machine) (NN translation)) (, ,) (CC and) (NML (NN language))) (NN modelling))))))) (. .))
(S (NP (NN Sparsity)) (VP (VBZ is) (NP (NP (DT a) (NN technique) (S (VP (TO to) (VP (VB reduce) (S (VP (VB compute))))))) (CC and) (NP (NP (NN memory) (NNS requirements)) (PP (IN of) (NP (NML (JJ deep) (NN learning)) (NNS models)))))) (. .))
(S (NP (JJ Sparse) (NNS RNNs)) (VP (VBP are) (ADJP (ADJP (JJR easier)) (S (VP (TO to) (VP (VB deploy) (PP (IN on) (NP (NP (NNS devices)) (CC and) (NP (NML (JJ high) (HYPH -) (NN end)) (NN server) (NNS processors))))))))) (. .))
(S (SBAR (RB Even) (IN though) (S (S (NP (JJ sparse) (NNS operations)) (VP (VBP need) (ADVP (JJR less)) (VP (VB compute)))) (CC and) (S (NP (NN memory)) (ADJP (JJ relative) (PP (IN to) (NP (PRP$ their) (JJ dense) (NNS counterparts))))))) (, ,) (NP (NP (DT the) (NN speed) (HYPH -) (NN up)) (VP (VBN observed) (PP (IN by) (S (VP (VBG using) (NP (JJ sparse) (NNS operations))))))) (VP (VBZ is) (ADVP (JJR less) (IN than)) (VP (VBN expected) (PP (IN on) (NP (JJ different) (NN hardware) (NNS platforms))))) (. .))
(S (PP (IN In) (NP (NN order) (S (VP (TO to) (VP (VB address) (NP (DT this) (NN issue))))))) (, ,) (NP (PRP we)) (VP (VP (VBP investigate) (NP (CD two) (JJ different) (NNS approaches)) (S (VP (TO to) (VP (VB induce) (NP (NP (NP (NN block) (NN sparsity)) (PP (IN in) (NP (NNS RNNs)))) (: :) (NP (NP (NN pruning) (NNS blocks)) (PP (IN of) (NP (NP (NNS weights)) (PP (IN in) (NP (DT a) (NN layer))))))))))) (CC and) (VP (VBG using) (NP (NN group) (NN lasso) (NN regularization)) (S (VP (TO to) (VP (VB create) (NP (NP (NNS blocks)) (PP (IN of) (NP (NNS weights)))) (PP (IN with) (NP (NNS zeros)))))))) (. .))
(S (S (VP (VBG Using) (NP (DT these) (NNS techniques)))) (, ,) (NP (PRP we)) (VP (VBP demonstrate) (SBAR (IN that) (S (NP (PRP we)) (VP (MD can) (VP (VB create) (NP (NP (NN block) (HYPH -) (JJ sparse) (NNS RNNs)) (PP (IN with) (NP (NP (NN sparsity)) (VP (VBG ranging) (PP (IN from) (NP (QP (CD 80) (NN %) (IN to) (CD 90) (NN %)))) (PP (IN with) (NP (NP (JJ small) (NN loss)) (PP (IN in) (NP (NN accuracy)))))))))))))) (. .))
(S (NP (DT This)) (VP (VBZ allows) (S (NP (PRP us)) (VP (TO to) (VP (VB reduce) (NP (DT the) (NN model) (NN size)) (PP (IN by) (NP (QP (RB roughly) (CD 10x)))))))) (. .))
(S (ADVP (RB Additionally)) (, ,) (NP (PRP we)) (VP (MD can) (VP (VB prune) (NP (DT a) (JJR larger) (JJ dense) (NN network)) (S (VP (TO to) (VP (VB recover) (NP (DT this) (NN loss)) (PP (IN in) (NP (NN accuracy))) (SBAR (IN while) (S (S (VP (VBG maintaining) (NP (NML (JJ high) (NN block)) (NN sparsity)))) (CC and) (S (VP (VBG reducing) (NP (DT the) (JJ overall) (NN parameter) (NN count))))))))))) (. .))
(S (NP (PRP$ Our) (NN technique)) (VP (VBZ works) (SBAR (IN with) (S (NP (NP (DT a) (NN variety)) (PP (IN of) (NP (NN block)))) (VP (VBZ sizes) (PRT (RP up)) (PP (IN to) (NP (CD 32x32))))))) (. .))
(S (NP (NP (NNP Block)) (HYPH -) (NP (JJ sparse) (NNS RNNs))) (VP (VBP eliminate) (SBAR (S (NP (NP (NNS overheads)) (VP (VBN related) (PP (IN to) (NP (NP (NNS data) (NN storage)) (CC and) (NP (JJ irregular) (NN memory)))))) (VP (VBZ accesses) (SBAR (IN while) (S (VP (VBG increasing) (NP (NN hardware) (NN efficiency)) (PP (VBN compared) (PP (IN to) (NP (JJ unstructured) (NN sparsity))))))))))) (. .))
