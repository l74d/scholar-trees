(S (NP (NP (JJS Most) (NNS investigations)) (PP (IN into) (NP (NML (JJ near) (HYPH -) (NN memory)) (NN hardware) (NNS accelerators))) (PP (IN for) (NP (JJ deep) (JJ neural) (NNS networks)))) (VP (VBP have) (ADVP (RB primarily)) (VP (VBN focused) (PP (IN on) (NP (NN inference))) (, ,) (SBAR (IN while) (S (NP (NP (DT the) (NN potential)) (PP (IN of) (NP (VBG accelerating) (NN training)))) (VP (VBZ has) (VP (VBN received) (NP (ADJP (RB relatively) (JJ little)) (NN attention)) (ADVP (RB so) (RB far)))))))) (. .))
(S (PP (VBN Based) (PP (IN on) (NP (ADJP (NP (NP (DT an) (NML (PP (IN in) (HYPH -) (NP (NN depth)))) (NN analysis)) (PP (IN of) (NP (DT the) (JJ key) (NML (NML (NML (JJ computational) (NNS patterns)) (PP (IN in) (NP (NN state)))) (HYPH -) (PP (IN of) (HYPH -) (NP (DT the) (HYPH -) (NN art)))) (NN gradient)))) (HYPH -) (VBN based)) (NN training) (NNS methods)))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (DT an) (JJ efficient) (NML (JJ near) (HYPH -) (NN memory)) (NN acceleration) (NN engine)) (VP (VBN called) (NP (NP (NNP NTX)) (SBAR (WHNP (WDT that)) (S (VP (MD can) (VP (VB be) (VP (VBN used) (S (VP (TO to) (VP (VB train) (NP (ADJP (NN state) (HYPH -) (IN of) (HYPH -) (DT the) (HYPH -) (NN art)) (JJ deep) (JJ convolutional) (JJ neural) (NNS networks)) (PP (IN at) (NP (NN scale))))))))))))))) (. .))
(S (NP (PRP$ Our) (JJ main) (NNS contributions)) (VP (VBP are) (: :) (NP (NP (LST (-LRB- -LRB-) (LS i) (-RRB- -RRB-)) (NP (NP (DT a) (JJ loose) (NN coupling)) (PP (IN of) (NP (NML (NNP RISC) (HYPH -) (NNP V)) (NNS cores)))) (CC and) (NP (NP (NNP NTX) (NNS co-processors)) (VP (VBG reducing) (S (VP (VBG offloading) (ADVP (RB overhead)) (PP (IN by) (NP (NP (NN 7x)) (PP (IN over) (NP (ADJP (RB previously) (VBN published)) (NNS results)))))))))) (: ;) (NP (LST (-LRB- -LRB-) (LS ii) (-RRB- -RRB-)) (NP (NP (NP (DT an) (VBN optimized) (NN IEEE754)) (ADJP (JJ compliant) (NP-TMP (NP (NNS data) (NN path)) (PP (IN for) (NP (NP (ADVP (RB fast)) (NML (JJ high) (HYPH -) (NN precision)) (NNS convolutions)) (CC and) (NP (NN gradient) (NN propagation) (: ;) (LST (-LRB- -LRB-) (LS iii) (-RRB- -RRB-)) (NN evaluation))))))) (PP (IN of) (NP (NP (NML (JJ near) (HYPH -) (NN memory)) (NN computing)) (PP (IN with) (NP (NN NTX)))))) (S (VP (VBN embedded) (PP (IN into) (NP (NP (JJ residual) (NN area)) (PP (IN on) (NP (DT the) (NN Logic) (NML (NML (NN Base) (NN die)) (PP (IN of) (NP (DT a) (NN Hybrid) (NN Memory)))) (NN Cube)))))))) (: ;) (CC and) (NP (NP (-LRB- -LRB-) (NN iv) (-RRB- -RRB-)) (NP (NP (DT a) (NN scaling) (NN analysis)) (PP (IN to) (NP (NP (NNS meshes)) (PP (IN of) (NP (NP (NNS HMCs)) (PP (IN in) (NP (DT a) (NML (NN data) (NN center)) (NN scenario))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP demonstrate) (NP (NP (DT a) (NML (QP (CD 2.7) (SYM x))) (NML (NN energy) (NN efficiency)) (NN improvement)) (PP (IN of) (NP (NN NTX))) (PP (IN over) (NP (NP (JJ contemporary) (NNS GPUs)) (PP (IN at) (NP (ADJP (NP (CD 4.4) (SYM x)) (JJR less)) (NN silicon) (NN area))))) (, ,) (CC and) (NP (NP (DT a) (NML (S (VP (VB compute) (NP (NP (NN performance)) (PP (IN of) (NP (CD 1.2))))))) (NN Tflop)) (PP (SYM /) (NP (NN s))))) (PP (IN for) (S (VP (VBG training) (NP (NML (NML (JJ large) (NN state)) (HYPH -) (PP (IN of) (HYPH -) (NP (DT the) (HYPH -) (NN art)))) (NNS networks)) (PP (IN with) (NP (JJ full) (NML (JJ floating) (HYPH -) (NN point)) (NN precision))))))) (. .))
(S (PP (IN At) (NP (DT the) (NML (NNS data) (NN center)) (NN scale))) (, ,) (NP (NP (DT a) (NN mesh)) (PP (IN of) (NP (NNP NTX)))) (VP (VBZ achieves) (PP (IN above) (NP (NP (NML (CD 95) (NN %)) (NN parallel)) (CC and) (NP (NN energy) (NN efficiency)))) (, ,) (SBAR (IN while) (S (VP (VBG providing) (NP (NP (QP (CD 2.1) (SYM x)) (NML (NML (NN energy) (NNS savings)) (CC or) (NML (NML (CD 3.1)) (PP (SYM x) (NP (NN performance))))) (NN improvement)) (PP (IN over) (NP (ADJP (NP (DT a) (NN GPU)) (HYPH -) (VBN based)) (NN system)))))))) (. .))
