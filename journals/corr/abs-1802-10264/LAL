(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (NP (PRP we)) (VP (VBP explore) (NP (NP (JJ deep) (NN reinforcement) (VBG learning) (NNS algorithms)) (PP (IN for) (NP (JJ vision-based) (JJ robotic) (NN grasping))))) (. .))
(S (S (NP (NP (JJ Model-free) (JJ deep) (NN reinforcement) (NN learning)) (PRN (-LRB- -LRB-) (NP (NNP RL)) (-RRB- -RRB-))) (VP (VBZ has) (VP (VBN been) (VP (ADVP (RB successfully)) (VBN applied) (PP (TO to) (NP (NP (DT a) (NN range)) (PP (IN of) (NP (VBG challenging) (NNS environments))))))))) (, ,) (CC but) (S (NP (NP (DT the) (NN proliferation)) (PP (IN of) (NP (NN algorithms)))) (VP (VBZ makes) (S (NP (NP (PRP it))) (ADJP (JJ difficult)) (S (VP (TO to) (VP (VB discern) (SBAR (WHNP (WDT which) (JJ particular) (NN approach)) (S (VP (MD would) (VP (VB be) (VP (ADVP (RB best)) (VBN suited) (PP (IN for) (NP (NP (DT a) (JJ rich) (, ,) (JJ diverse) (NN task)) (PP (IN like) (NP (VBG grasping)))))))))))))))) (. .))
(S (S (VP (TO To) (VP (VB answer) (NP (DT this) (NN question))))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (DT a) (JJ simulated) (NN benchmark)) (PP (IN for) (NP (JJ robotic) (NN grasping))) (SBAR (WHNP (WDT that)) (S (VP (VBZ emphasizes) (NP (NP (NN off-policy) (NN learning)) (CC and) (NP (NP (NN generalization)) (PP (TO to) (NP (JJ unseen) (NNS objects)))))))))) (. .))
(S (S (NP (NN Off-policy) (NN learning)) (VP (VBZ enables) (NP (NP (NN utilization)) (PP (IN of) (NP (VBG grasping) (NNS data))) (PP (IN over) (NP (NP (DT a) (JJ wide) (NN variety)) (PP (IN of) (NP (NNS objects)))))))) (, ,) (CC and) (S (NP (NN diversity)) (VP (VBZ is) (JJ important) (S (VP (TO to) (VP (VB enable) (S (NP (DT the) (NN method)) (VP (TO to) (VP (VB generalize) (PP (TO to) (NP (NP (JJ new) (NNS objects)) (SBAR (WHNP (WDT that)) (S (VP (VBD were) (RB not) (VP (VBN seen) (PP (IN during) (NP (NN training))))))))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP evaluate) (NP (DT the) (NN benchmark) (NNS tasks)) (PP (IN against) (NP (NP (DT a) (NN variety)) (PP (IN of) (NP (NP (NNP Q-function) (NN estimation) (NNS methods)) (, ,) (NP (NP (DT a) (NN method)) (VP (ADVP (RB previously)) (VBN proposed) (PP (IN for) (NP (NP (JJ robotic) (NN grasping)) (PP (IN with) (NP (JJ deep) (JJ neural) (NN network) (NNS models))))))) (, ,) (CC and) (NP (NP (DT a) (JJ novel) (NN approach)) (VP (VBN based) (PP (IN on) (NP (NP (DT a) (NN combination)) (PP (IN of) (NP (NP (NNP Monte) (NNP Carlo) (NN return) (NN estimation)) (CC and) (NP (DT an) (JJ off-policy) (NN correction))))))))))))) (. .))
(S (S (NP (PRP$ Our) (NNS results)) (VP (VBP indicate) (SBAR (IN that) (S (NP (JJ several) (JJ simple) (NNS methods)) (VP (VBP provide) (NP (NP (DT a) (ADJP (RB surprisingly) (JJ strong)) (NN competitor)) (PP (TO to) (NP (NP (JJ popular) (NNS algorithms)) (PP (JJ such) (IN as) (NP (JJ double) (NN Q-learning))))))))))) (, ,) (CC and) (S (NP (NP (PRP$ our) (NN analysis)) (PP (IN of) (NP (NN stability)))) (VP (NNS sheds) (NP (VBD light)) (PP (IN on) (NP (NP (DT the) (JJ relative) (NNS tradeoffs)) (PP (IN between) (NP (DT the) (NN algorithms))))))) (. .))
