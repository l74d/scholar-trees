(S (NP (NN Off-policy) (NN learning)) (VP (VBZ is) (ADJP (RBR more) (JJ unstable)) (PP (VBN compared) (PP (TO to) (NP (NP (NN on-policy) (NN learning)) (PP (IN in) (NP (NP (NN reinforcement) (NN learning)) (PRN (-LRB- -LRB-) (NP (NNP RL)) (-RRB- -RRB-)))))))) (. .))
(S (S (VP (TO To) (VP (VB cope) (PP (IN with) (NP (NN instability)))))) (, ,) (NP (PRP we)) (VP (VBP present) (NP (NP (DT the) (JJ first) (JJ relative) (NN importance) (NN sampling-off-policy) (JJ actor-critic) (PRN (-LRB- -LRB-) (NNP RIS-Off-PAC) (-RRB- -RRB-)) (CD model-free) (NN algorithms)) (PP (IN in) (NP (NNP RL))))) (. .))
(S (NP (PRP We)) (VP (VBP use) (NP (NP (NN action) (NN value)) (VP (VBN generated) (PP (IN from) (NP (NP (DT the) (NN behavior) (NN policy)) (PP (IN in) (NP (NN reward) (NN function))))))) (S (VP (S (VP (TO to) (VP (VB train) (NP (PRP$ our) (NN algorithm))))) (PP (RB rather) (IN than) (PP (IN from) (NP (DT the) (NN target) (NN policy))))))) (. .))
(S (NP (PRP We)) (ADVP (RB also)) (VP (VBP use) (NP (JJ deep) (JJ neural) (NNS networks)) (S (VP (TO to) (VP (VB train) (NP (DT both) (NN actor) (CC and) (NN critic)))))) (. .))
(S (NP (PRP We)) (VP (VP (VBD evaluated) (NP (PRP$ our) (NN algorithm)) (PP (IN on) (NP (NP (DT a) (NN number)) (PP (IN of) (NP (NNP Open) (NNP AI) (NNP Gym) (NN benchmark) (NNS problems)))))) (CC and) (VP (VB demonstrate) (NP (NP (ADJP (JJR better) (CC or) (JJ comparable)) (NN performance)) (PP (TO to) (NP (JJ several) (JJ state-of-the-art) (NNP RL) (NNS baselines)))))) (. .))
