(S (NP (NP (JJ Modern) (JJ deep) (JJ neural) (NNS networks)) (PRN (-LRB- -LRB-) (NP (NNP DNNs)) (-RRB- -RRB-))) (ADVP (RB often)) (VP (VBP require) (NP (NP (JJ high) (NN memory) (NN consumption)) (CC and) (NP (JJ large) (JJ computational) (NNS loads)))) (. .))
(S (SBAR (IN In) (NN order) (S (VP (TO to) (VP (VB deploy) (NP (NNP DNN) (FW algorithms)) (ADVP (RB efficiently)) (PP (IN on) (NP (ADJP (NN edge) (CC or) (JJ mobile)) (NNS devices))))))) (, ,) (NP (NP (DT a) (NN series)) (PP (IN of) (NP (NNP DNN) (NN compression) (NNS algorithms)))) (VP (VBP have) (VP (VBN been) (VP (VBN explored) (, ,) (PP (VBG including) (NP (NN factorization) (NNS methods)))))) (. .))
(S (NP (NNP Factorization) (NNS methods)) (VP (VBP approximate) (NP (NP (DT the) (NN weight) (NN matrix)) (PP (IN of) (NP (DT a) (NNP DNN) (NN layer)))) (PP (IN with) (NP (NP (DT the) (NN multiplication)) (PP (IN of) (NP (CD two) (CC or) (JJ multiple) (JJ low-rank) (NNS matrices)))))) (. .))
(S (ADVP (RB However)) (, ,) (NP (NP (PRP it))) (VP (VBZ is) (ADJP (JJ hard)) (S (VP (TO to) (VP (VB measure) (NP (NP (DT the) (NNS ranks)) (PP (IN of) (NP (NNP DNN) (NNS layers)))) (PP (IN during) (NP (DT the) (NN training) (NN process))))))) (. .))
(S (NP (JJ Previous) (VBZ works)) (ADVP (RB mainly)) (VP (VB induce) (NP (JJ low-rank)) (PP (PP (IN through) (NP (JJ implicit) (NNS approximations))) (CC or) (PP (IN via) (NP (NP (JJ costly) (JJ singular) (NN value) (NN decomposition) (PRN (-LRB- -LRB-) (NNP SVD) (-RRB- -RRB-)) (NN process)) (PP (IN on) (NP (DT every) (NN training) (NN step))))))) (. .))
(S (NP (DT The) (JJ former) (NN approach)) (ADVP (RB usually)) (VP (VBZ induces) (NP (DT a) (JJ high) (NN accuracy) (NN loss)) (SBAR (IN while) (S (NP (DT the) (NN latter)) (VP (VBZ has) (NP (DT a) (JJ low) (NN efficiency)))))) (. .))
(S (PP (IN In) (NP (DT this) (NN work))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (JJ SVD) (NN training)) (, ,) (NP (NP (DT the) (JJ first) (NN method)) (SBAR (S (VP (TO to) (VP (ADVP (RB explicitly)) (VB achieve) (NP (JJ low-rank) (NNP DNNs)) (PP (IN during) (NP (VBG training))) (PP (IN without) (S (VP (VBG applying) (NP (NNP SVD)) (PP (IN on) (NP (DT every) (NN step))))))))))))) (. .))
(S (NP (NNP SVD) (NN training)) (VP (VP (ADVP (JJ first)) (VBZ decomposes) (NP (DT each) (NN layer)) (PP (IN into) (NP (NP (DT the) (NN form)) (PP (IN of) (NP (PRP$ its) (JJ full-rank) (NNP SVD)))))) (, ,) (VP (ADVP (RB then)) (VBZ performs) (NP (VBG training)) (ADVP (RB directly)) (PP (IN on) (NP (DT the) (JJ decomposed) (NNS weights))))) (. .))
(S (NP (PRP We)) (VP (VBP add) (NP (JJ orthogonality) (NN regularization)) (PP (TO to) (NP (DT the) (JJ singular) (NNS vectors))) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VP (VB ensure) (NP (NP (DT the) (JJ valid) (NN form)) (PP (IN of) (NP (NNP SVD))))) (CC and) (VP (VB avoid) (NP (NN gradient) (NN vanishing/exploding))))))) (. .))
(S (NP (NNP Low-rank)) (VP (VBZ is) (VP (VBN encouraged) (PP (IN by) (S (VP (VBG applying) (NP (JJ sparsity-inducing) (NNS regularizers)) (PP (IN on) (NP (NP (DT the) (JJ singular) (NNS values)) (PP (IN of) (NP (DT each) (NN layer)))))))))) (. .))
(S (NP (JJ Singular) (NN value) (NN pruning)) (VP (VBZ is) (VP (VBN applied) (PP (IN at) (NP (DT the) (NN end))) (S (VP (TO to) (VP (ADVP (RB explicitly)) (VB reach) (NP (DT a) (JJ low-rank) (NN model))))))) (. .))
(S (NP (PRP We)) (VP (ADVP (RB empirically)) (VBP show) (SBAR (IN that) (S (NP (NNP SVD) (NN training)) (VP (MD can) (VP (VP (ADVP (RB significantly)) (VB reduce) (NP (NP (DT the) (NN rank)) (PP (IN of) (NP (NNP DNN) (NNS layers))))) (CC and) (VP (VB achieve) (NP (NP (JJR higher) (NN reduction)) (PP (IN on) (NP (NN computation) (NN load)))) (PP (IN under) (NP (DT the) (JJ same) (NN accuracy)))) (, ,) (S (VP (VBG comparing) (PP (TO to) (NP (CONJP (RB not) (RB only)) (NP (JJ previous) (NN factorization) (NNS methods)) (CONJP (CC but) (RB also)) (NP (JJ state-of-the-art) (NN filter) (VBG pruning) (NNS methods))))))))))) (. .))
