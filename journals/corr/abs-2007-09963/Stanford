(S (SBAR (IN While) (S (NP (NP (DT the) (NN accuracy)) (PP (IN of) (NP (JJ convolutional) (JJ neural) (NNS networks)))) (VP (VBZ has) (VP (VBN achieved) (NP (JJ vast) (NNS improvements)) (PP (IN by) (S (VP (VBG introducing) (NP (ADJP (JJR larger) (CC and) (JJR deeper)) (NN network) (NNS architectures))))))))) (, ,) (ADVP (RB also)) (NP (NP (DT the) (NN memory) (NN footprint)) (PP (IN for) (S (VP (VBG storing) (NP (PRP$ their) (NNS parameters) (CC and) (NNS activations)))))) (VP (VBZ has) (VP (VBN increased))) (. .))
(S (NP (DT This) (NN trend)) (ADVP (RB especially)) (VP (VBZ challenges) (NP (NP (ADJP (NP (NN power) (HYPH -) (CC and) (NN resource)) (HYPH -) (VBN limited)) (NN accelerator) (NNS designs)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBP are) (ADVP (RB often)) (VP (VBN restricted) (S (VP (TO to) (VP (VB store) (NP (NP (DT all) (NN network) (NNS data)) (PP (IN in) (NP (NML (IN on) (HYPH -) (NN chip)) (NN memory)))) (S (VP (TO to) (VP (VB avoid) (S (VP (VBG interfacing) (NP (ADJP (NN energy) (HYPH -) (JJ hungry)) (JJ external) (NNS memories)))))))))))))))) (. .))
(S (S (VP (VBG Maximizing) (NP (NP (DT the) (NN network) (NN size)) (SBAR (WHNP (WDT that)) (S (VP (VBZ fits) (PP (IN on) (NP (DT a) (VBN given) (NN accelerator))))))))) (ADVP (RB thus)) (VP (VBZ requires) (S (VP (TO to) (VP (VB maximize) (NP (PRP$ its) (NN memory) (NN utilization)))))) (. .))
(S (SBAR (IN While) (S (NP (NP (DT the) (ADJP (RB traditionally) (VBN used)) (NML (NN ping) (HYPH -) (NN pong))) (VP (VBG buffering) (NP (NN technique)))) (VP (VBZ is) (VP (VBG mapping) (NP (JJ subsequent) (NN activation) (NNS layers)) (PP (IN to) (NP (JJ disjunctive) (NN memory) (NNS regions))))))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (DT a) (NN mapping) (NN method)) (SBAR (WHNP (WDT that)) (S (VP (VP (VBZ allows) (NP (DT these) (NNS regions)) (PP (IN to) (NP (NN overlap)))) (CC and) (ADVP (RB thus)) (VP (VB utilize) (NP (DT the) (NN memory)) (ADVP (RBR more) (RB efficiently)))))))) (. .))
(S (NP (DT This) (NN work)) (VP (VP (VBZ presents) (NP (DT the) (JJ mathematical) (NN model)) (PP (IN to) (NP (NML (S (VP (VB compute) (NP (DT the) (JJ maximum) (NNS activations))))) (NN memory) (NP (NN overlap))))) (CC and) (ADVP (RB thus)) (ADVP (DT the) (JJR lower)) (VP (VBN bound) (PP (IN of) (NP (NP (NML (IN on) (HYPH -) (NN chip)) (NN memory)) (VP (VBN needed) (S (VP (TO to) (VP (VB perform) (NP (NP (NML (NML (NN layer)) (HYPH -) (PP (IN by) (HYPH -) (NP (NN layer)))) (NN processing)) (PP (IN of) (NP (JJ convolutional) (JJ neural) (NNS networks)))) (PP (IN on) (NP (ADJP (NN memory) (HYPH -) (VBN limited)) (NNS accelerators))))))))))) (. .))
(S (NP (NP (PRP$ Our) (NNS experiments)) (PP (IN with) (NP (JJ various) (NML (JJ real) (HYPH -) (NN world)) (NN object) (NN detector) (NNS networks)))) (VP (VBP show) (SBAR (IN that) (S (NP (DT the) (JJ proposed) (NN mapping) (NN technique)) (VP (MD can) (VP (VB decrease) (NP (DT the) (NNS activations) (NN memory)) (PP (IN by) (NP (NP (QP (RB up) (IN to) (CD 32.9)) (NN %)) (, ,) (VP (VBG reducing) (NP (NP (DT the) (JJ overall) (NN memory)) (PP (IN for) (NP (DT the) (JJ entire) (NN network)))) (PP (IN by) (NP (NP (QP (RB up) (IN to) (CD 23.9)) (NN %)) (PP (VBN compared) (PP (IN to) (NP (JJ traditional) (NML (NN ping) (HYPH -) (NN pong)) (NN buffering)))))))))))))) (. .))
(S (PP (IN For) (NP (JJR higher) (NN resolution) (NN de-noising) (NNS networks))) (, ,) (NP (PRP we)) (VP (VBP achieve) (NP (NP (NML (NN activation) (NN memory)) (NNS savings)) (PP (IN of) (NP (CD 48.8) (NN %))))) (. .))
(S (ADVP (RB Additionally)) (, ,) (NP (PRP we)) (VP (VBP implement) (NP (DT a) (NN face) (NN detector) (NN network)) (PP (IN on) (NP (ADJP (NP (DT an) (NN FPGA)) (HYPH -) (VBN based)) (NN camera))) (S (VP (TO to) (VP (VB validate) (NP (DT these) (NML (NML (NML (NN memory) (NNS savings)) (PP (IN on) (NP (DT a) (JJ complete) (NN end)))) (HYPH -) (PP (IN to) (HYPH -) (NP (NN end)))) (NN system)))))) (. .))
