(S (NP (NNP Data) (NN augmentation)) (VP (VBZ is) (NP (NP (DT a) (JJ popular) (NN technique)) (VP (ADVP (RB largely)) (VBD used) (S (VP (TO to) (VP (VB enhance) (NP (NP (DT the) (NN training)) (PP (IN of) (NP (JJ convolutional) (JJ neural) (NNS networks)))))))))) (. .))
(S (SBAR (IN Although) (S (NP (NP (JJ many)) (PP (IN of) (NP (PRP$ its) (NNS benefits)))) (VP (VBP are) (ADJP (RB well) (VBN known) (PP (IN by) (NP (JJ deep) (NN learning) (NNS researchers) (CC and) (NNS practitioners))))))) (, ,) (NP (NP (PRP$ its) (JJ implicit) (NN regularization) (NNS effects)) (, ,) (PP (IN as) (PP (VBN compared) (PP (TO to) (NP (NP (JJ popular) (JJ explicit) (NN regularization) (NNS techniques)) (, ,) (PP (JJ such) (IN as) (NP (NP (NN weight) (NN decay)) (CC and) (NP (NN dropout)))))))) (, ,)) (VP (VBP remain) (ADJP (RB largely) (JJ unstudied))) (. .))
(S (PP (IN As) (NP (NP (DT a) (NN matter)) (PP (IN of) (NP (NN fact))))) (, ,) (NP (NP (JJ convolutional) (JJ neural) (NNS networks)) (PP (IN for) (NP (NN image) (JJ object) (NN classification)))) (VP (VBP are) (ADVP (RB typically)) (VP (VBN trained) (PP (IN with) (NP (DT both) (NP (NNS data) (NN augmentation)) (CC and) (NP (JJ explicit) (NN regularization)))) (, ,) (S (VP (VBG assuming) (SBAR (S (NP (NP (DT the) (NNS benefits)) (PP (IN of) (NP (DT all) (NNS techniques)))) (VP (VBP are) (ADJP (JJ complementary))))))))) (. .))
(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (NP (PRP we)) (VP (ADVP (RB systematically)) (VBP analyze) (NP (DT these) (NNS techniques)) (PP (IN through) (NP (NP (NN ablation) (NNS studies)) (PP (IN of) (NP (NP (JJ different) (NN network) (NNS architectures)) (VP (VBN trained) (PP (IN with) (NP (NP (JJ different) (NNS amounts)) (PP (IN of) (NP (NN training) (NNS data))))))))))) (. .))
(S (NP (PRP$ Our) (NNS results)) (VP (VBP unveil) (NP (NP (NP (DT a) (ADJP (RB largely) (JJ ignored)) (NN advantage)) (PP (IN of) (NP (NNS data) (NN augmentation)))) (: :) (S (NP (NP (NNS networks)) (VP (VBD trained) (PP (IN with) (NP (RB just) (NNS data) (NN augmentation))))) (VP (ADVP (RBR more) (RB easily)) (RB adapt) (PP (TO to) (NP (JJ different) (NX (NNS architectures)) (CC and) (NX (NX (NN amount)) (PP (IN of) (NP (NN training) (NNS data)))))) (, ,) (SBAR (IN as) (S (VP (VBN opposed) (PP (TO to) (NP (NP (NP (VB weight) (NN decay)) (CC and) (NP (NN dropout))) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBP require) (NP (NP (JJ specific) (NN fine-tuning)) (PP (IN of) (NP (PRP$ their) (NNS hyperparameters)))))))))))))))) (. .))
