(S (NP (NNS Data) (NN augmentation)) (VP (VBZ is) (NP (NP (DT a) (JJ popular) (NN technique)) (VP (ADVP (RB largely)) (VBN used) (S (VP (TO to) (VP (VB enhance) (NP (NP (DT the) (NN training)) (PP (IN of) (NP (JJ convolutional) (JJ neural) (NNS networks)))))))))) (. .))
(S (SBAR (IN Although) (S (NP (NP (JJ many)) (PP (IN of) (NP (PRP$ its) (NNS benefits)))) (VP (VBP are) (ADVP (RB well)) (VP (VBN known) (PP (IN by) (NP (NML (JJ deep) (NN learning)) (NNS researchers) (CC and) (NNS practitioners))))))) (, ,) (NP (PRP$ its) (JJ implicit) (NN regularization) (NNS effects)) (, ,) (PP (IN as) (PP (VBN compared) (PP (IN to) (NP (JJ popular) (JJ explicit) (NN regularization) (NNS techniques))))) (, ,) (PP (JJ such) (PP (IN as) (NP (NN weight) (NN decay) (CC and) (NN dropout)))) (, ,) (VP (VBP remain) (ADJP (RB largely) (JJ unstudied))) (. .))
(S (PP (IN As) (NP (NP (DT a) (NN matter)) (PP (IN of) (NP (NN fact))))) (, ,) (NP (NP (JJ convolutional) (JJ neural) (NNS networks)) (PP (IN for) (NP (NN image) (NN object) (NN classification)))) (VP (VBP are) (ADVP (RB typically)) (VP (VBN trained) (PP (IN with) (NP (NP (DT both) (NNS data) (NN augmentation)) (CC and) (NP (JJ explicit) (NN regularization)))) (, ,) (S (VP (VBG assuming) (SBAR (S (NP (NP (DT the) (NNS benefits)) (PP (IN of) (NP (DT all) (NNS techniques)))) (VP (VBP are) (ADJP (JJ complementary))))))))) (. .))
(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (NP (PRP we)) (ADVP (RB systematically)) (VP (VB analyze) (NP (DT these) (NNS techniques)) (PP (IN through) (NP (NP (NN ablation) (NNS studies)) (PP (IN of) (NP (NP (JJ different) (NN network) (NNS architectures)) (VP (VBN trained) (PP (IN with) (NP (NP (JJ different) (NNS amounts)) (PP (IN of) (NP (NN training) (NNS data))))))))))) (. .))
(S (NP (PRP$ Our) (NNS results)) (VP (VBP unveil) (NP (NP (NP (NP (DT a) (ADJP (RB largely) (VBN ignored)) (NN advantage)) (PP (IN of) (NP (NNS data) (NN augmentation)))) (: :) (NP (NP (NNS networks)) (VP (VBN trained) (PP (IN with) (NP (NP (RB just) (NNS data)) (SBAR (S (NP (NN augmentation)) (ADVP (RBR more) (RB easily)) (VP (VB adapt) (PP (IN to) (NP (NP (JJ different) (NNS architectures) (CC and) (NN amount)) (PP (IN of) (NP (NN training) (NNS data))))) (, ,) (SBAR (IN as) (S (VP (VBN opposed) (PP (IN to) (NP (NN weight) (NN decay) (CC and) (NN dropout)))))))))))))) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBP require) (NP (NP (JJ specific) (JJ fine) (HYPH -) (NN tuning)) (PP (IN of) (NP (PRP$ their) (NNS hyperparameters))))))))) (. .))
