(S (PP (IN In) (NP (DT this) (NN paper))) (NP (PRP we)) (VP (VBP present) (NP (NP (DT a) (JJ deep) (JJ neural) (NN network) (NN topology)) (SBAR (WHNP (WDT that)) (S (VP (VBZ incorporates) (NP (DT a) (JJ simple)) (S (VP (TO to) (VP (VB implement) (S (NP (NN transformation)) (ADJP (JJ invariant)) (S (VP (VBG pooling) (NP (NN operator)))))))))))) (PRN (-LRB- -LRB-) (NP (NNP TI) (HYPH -) (NNP POOLING)) (-RRB- -RRB-))) (. .))
(S (NP (DT This) (NN operator)) (VP (VBZ is) (ADJP (JJ able) (S (VP (TO to) (ADVP (RB efficiently)) (VP (VB handle) (NP (JJ prior) (NN knowledge)) (PP (IN on) (NP (NP (JJ nuisance) (NNS variations)) (PP (IN in) (NP (DT the) (NNS data))) (, ,) (PP (JJ such) (IN as) (NP (NP (NN rotation)) (CC or) (NP (NN scale) (NNS changes))))))))))) (. .))
(S (S (NP (JJS Most) (JJ current) (NNS methods)) (ADVP (RB usually)) (VP (VBP make) (NP (NP (NN use)) (PP (IN of) (NP (NN dataset) (NN augmentation)))) (S (VP (TO to) (VP (VB address) (NP (DT this) (NN issue))))))) (, ,) (CC but) (S (NP (DT this)) (VP (VBZ requires) (NP (NP (NP (JJR larger) (NN number)) (PP (IN of) (NP (NP (NN model) (NNS parameters)) (CC and) (NP (JJR more) (NN training) (NNS data))))) (, ,) (CC and) (NP (NP (NNS results)) (PP (IN in) (NP (NP (ADJP (RB significantly) (VBN increased)) (NN training) (NN time)) (CC and) (NP (JJR larger) (NML (NML (NP (NN chance)) (PP (IN of) (PP (IN under)))) (HYPH -) (CC or) (NML (NN overfitting)))))))))) (. .))
(S (NP (NP (DT The) (JJ main) (NN reason)) (PP (IN for) (NP (DT these) (NNS drawbacks)))) (VP (VBZ is) (SBAR (IN that) (S (NP (DT the) (NML (S (VP (VBN learned)))) (NN model)) (VP (VBZ needs) (S (VP (TO to) (VP (VB capture) (NP (JJ adequate) (NNS features)) (PP (IN for) (NP (NP (PDT all) (DT the) (JJ possible) (NNS transformations)) (PP (IN of) (NP (DT the) (NN input)))))))))))) (. .))
(S (PP (IN On) (NP (DT the) (JJ other) (NN hand))) (, ,) (NP (PRP we)) (VP (VBP formulate) (NP (NP (NNS features)) (PP (IN in) (NP (JJ convolutional) (JJ neural) (NNS networks)))) (S (VP (TO to) (VP (VB be) (ADJP (NN transformation) (HYPH -) (JJ invariant)))))) (. .))
(S (NP (PRP We)) (VP (VBP achieve) (SBAR (IN that) (S (S (VP (VBG using) (NP (NP (JJ parallel) (JJ siamese) (NNS architectures)) (PP (IN for) (NP (DT the) (VBN considered) (NN transformation) (NN set)))))) (CC and) (S (VP (VBG applying) (NP (NP (DT the) (NNP TI)) (HYPH -) (SBAR (S (VP (VBG POOLING) (NP (NN operator)) (PP (IN on) (NP (PRP$ their) (NNS outputs))))))) (PP (IN before) (NP (DT the) (ADJP (RB fully) (HYPH -) (JJ connected)) (NNS layers)))))))) (. .))
(S (NP (PRP We)) (VP (VBP show) (SBAR (IN that) (S (NP (DT this) (NN topology)) (ADVP (RB internally)) (VP (VP (VBZ finds) (NP (DT the) (RBS most) (JJ optimal)) (S ('' ") (ADJP (ADJP (JJ canonical)) (SBAR (S (NP (`` ") (NN instance) (PP (IN of) (NP (DT the) (NN input) (NN image))))))) (PP (IN for) (NP (NN training))))) (CC and) (VP (ADVP (RB therefore)) (VBZ limits) (NP (DT the) (NN redundancy)) (PP (IN in) (S (VP (VBN learned) (NP (NNS features)))))))))) (. .))
(S (NP (NP (DT This) (ADJP (RBR more) (JJ efficient)) (NN use)) (PP (IN of) (NP (NN training) (NNS data)))) (VP (VBZ results) (PP (IN in) (NP (NP (JJR better) (NN performance)) (PP (IN on) (NP (NP (JJ popular) (NN benchmark) (NNS datasets)) (PP (IN with) (NP (NP (JJR smaller) (NN number)) (PP (IN of) (NP (NNS parameters))))))))) (SBAR (WHADVP (WRB when)) (S (VP (VBG comparing) (PP (PP (IN to) (NP (NP (JJ standard) (JJ convolutional) (JJ neural) (NNS networks)) (PP (IN with) (NP (NN dataset) (NN augmentation))))) (CC and) (PP (IN to) (NP (JJ other) (NNS baselines)))))))) (. .))
