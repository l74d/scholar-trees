(S (NP (NML (NN Policy) (NN gradient)) (NNS algorithms)) (ADVP (RB typically)) (VP (VBP combine) (NP (NP (VBN discounted) (JJ future) (NNS rewards)) (PP (IN with) (NP (DT an) (VBN estimated) (NN value) (NN function)))) (, ,) (PP (IN to) (NP (NML (S (VP (VB compute) (NP (NP (DT the) (NN direction) (CC and) (NN magnitude)) (PP (IN of) (NP (NN parameter))))))) (NNS updates)))) (. .))
(S (ADVP (RB However)) (, ,) (PP (IN for) (NP (ADJP (JJS most) (NN Reinforcement)) (NN Learning) (NNS tasks))) (, ,) (NP (NNS humans)) (VP (MD can) (VP (VB provide) (NP (JJ additional) (NN insight)) (S (VP (TO to) (VP (VB constrain) (NP (DT the) (NN policy) (NN learning))))))) (. .))
(S (NP (PRP We)) (VP (VBP introduce) (NP (DT a) (JJ general) (NN method)) (S (VP (TO to) (VP (VB incorporate) (NP (JJ multiple) (JJ different) (NN feedback) (NNS channels)) (PP (IN into) (NP (DT a) (NML (JJ single) (NN policy)) (NN gradient) (NN loss))))))) (. .))
(S (PP (IN In) (NP (NP (PRP$ our) (NN formulation)) (, ,) (NP (DT the) (NNP Multi-Preference) (NN Actor) (NN Critic) (PRN (-LRB- -LRB-) (NP (NN M) (HYPH -) (NN PAC)) (-RRB- -RRB-))))) (, ,) (NP (NP (DT these) (JJ different) (NNS types)) (PP (IN of) (NP (NN feedback)))) (VP (VBP are) (VP (VBN implemented) (PP (IN as) (NP (NP (NNS constraints)) (PP (IN on) (NP (DT the) (NN policy))))))) (. .))
(S (NP (PRP We)) (VP (VBP use) (NP (DT a) (JJ Lagrangian) (NN relaxation)) (S (VP (TO to) (VP (VB satisfy) (NP (DT these) (NNS constraints)) (S (VP (VBG using) (NP (NN gradient) (NN descent)) (PP (IN while) (S (VP (VBG learning) (NP (NP (DT a) (NN policy)) (SBAR (WHNP (WDT that)) (S (VP (VBZ maximizes) (NP (NNS rewards))))))))))))))) (. .))
(S (NP (NP (NNS Experiments)) (PP (IN in) (NP (NP (NNP Atari)) (CC and) (NP (NN Pendulum))))) (VP (VP (VBP verify) (SBAR (IN that) (S (NP (NNS constraints)) (VP (VBP are) (VP (VBG being) (VP (VBN respected))))))) (CC and) (VP (MD can) (VP (VB accelerate) (NP (DT the) (NN learning) (NN process))))) (. .))
