(S (NP (NP (NP (JJ Stochastic) (NN gradient) (NN descent)) (PRN (-LRB- -LRB-) (NP (NNP SGD)) (-RRB- -RRB-)) (PP (IN with) (NP (JJ constant) (NN momentum)))) (CC and) (NP (NP (PRP$ its) (NNS variants)) (PP (JJ such) (IN as) (NP (NNP Adam))))) (VP (VBP are) (NP (NP (DT the) (NN optimization) (NN algorithms)) (PP (IN of) (NP (NN choice))) (PP (IN for) (S (VP (VBG training) (NP (NP (JJ deep) (JJ neural) (NNS networks)) (PRN (-LRB- -LRB-) (NP (NNP DNNs)) (-RRB- -RRB-)))))))) (. .))
(S (SBAR (IN Since) (S (NP (NNP DNN) (NN training)) (VP (VBZ is) (ADJP (RB incredibly) (RB computationally) (JJ expensive))))) (, ,) (NP (EX there)) (VP (VBZ is) (NP (NP (JJ great) (NN interest)) (PP (IN in) (S (VP (VBG speeding) (PRT (RP up)) (NP (DT the) (NN convergence))))))) (. .))
(S (S (NP (NP (NNP Nesterov) (VBD accelerated) (NN gradient)) (PRN (-LRB- -LRB-) (NP (NNP NAG)) (-RRB- -RRB-))) (VP (VBZ improves) (NP (NP (DT the) (NN convergence) (NN rate)) (PP (IN of) (NP (NP (NN gradient) (NN descent)) (PRN (-LRB- -LRB-) (NP (NNP GD)) (-RRB- -RRB-))))) (PP (IN for) (NP (NN convex) (NN optimization))) (S (VP (VBG using) (NP (DT a) (ADJP (RB specially) (VBN designed)) (NN momentum)))))) (: ;) (S (ADVP (RB however)) (, ,) (NP (PRP it)) (VP (VBZ accumulates) (NP (NN error)) (SBAR (WHADVP (WRB when)) (S (NP (DT an) (JJ inexact) (NN gradient)) (VP (VBZ is) (VP (VBN used) (PRN (-LRB- -LRB-) (PP (JJ such) (IN as) (PP (IN in) (NP (NNP SGD)))) (-RRB- -RRB-)))))) (, ,) (S (VP (VP (VBG slowing) (NP (NN convergence)) (ADVP (IN at) (JJS best))) (CC and) (VP (VBG diverging) (PP (IN at) (NP (JJS worst)))))))) (. .))
(S (PP (IN In) (NP (DT this) (NN paper))) (, ,) (NP (PRP we)) (VP (VBP propose) (NP (NP (NP (VBN Scheduled) (NNP Restart) (NNP SGD)) (PRN (-LRB- -LRB-) (NP (NNP SRSGD)) (-RRB- -RRB-))) (, ,) (NP (NP (DT a) (JJ new) (JJ NAG-style) (NN scheme)) (PP (IN for) (S (VP (VBG training) (NP (NNP DNNs)))))))) (. .))
(S (NP (NNP SRSGD)) (VP (VP (VBZ replaces) (NP (NP (DT the) (JJ constant) (NN momentum)) (PP (IN in) (NP (NNP SGD)))) (PP (IN by) (NP (NP (DT the) (VBG increasing) (NN momentum)) (PP (IN in) (NP (NNP NAG)))))) (CC but) (VP (VBZ stabilizes) (NP (DT the) (NNS iterations)) (PP (IN by) (S (VP (VBG resetting) (NP (DT the) (NN momentum)) (PP (TO to) (NP (CD zero))) (PP (VBG according) (PP (TO to) (NP (DT a) (NN schedule))))))))) (. .))
(S (S (S (VP (VBG Using) (NP (NP (DT a) (NN variety)) (PP (IN of) (NP (NP (NNS models) (CC and) (NNS benchmarks)) (PP (IN for) (NP (NN image) (NN classification)))))))) (, ,) (NP (PRP we)) (VP (VBP demonstrate) (SBAR (DT that) (, ,) (S (PP (IN in) (S (VP (VBG training) (NP (NNP DNNs))))) (, ,) (NP (NNP SRSGD)) (VP (ADVP (RB significantly)) (VBZ improves) (NP (NN convergence) (CC and) (NN generalization))))))) (: ;) (S (PP (IN for) (NP (NN instance))) (PP (IN in) (S (VP (NN training) (NP (NNP ResNet200)) (PP (IN for) (NP (NNP ImageNet) (NN classification)))))) (, ,) (NP (NNP SRSGD)) (VP (VBZ achieves) (NP (NP (DT an) (NN error) (NN rate)) (PP (IN of) (NP (CD 20.93) (NN %))) (PP (IN vs.) (NP (NP (DT the) (NN benchmark)) (PP (IN of) (NP (CD 22.13) (NN %)))))))) (. .))
(S (NP (DT These) (NNS improvements)) (VP (VBP become) (ADJP (RBR more) (JJ significant)) (SBAR (IN as) (S (NP (DT the) (NN network)) (VP (VBZ grows) (ADVP (JJR deeper)))))) (. .))
(S (ADVP (RB Furthermore)) (, ,) (PP (IN on) (NP (DT both) (NNP CIFAR) (CC and) (NNP ImageNet))) (, ,) (NP (NNP SRSGD)) (VP (VBZ reaches) (NP (ADJP (ADJP (JJ similar)) (CC or) (ADJP (RB even) (JJR better))) (NN error) (NNS rates)) (PP (IN with) (NP (NP (ADJP (RB significantly) (JJR fewer)) (NN training) (NNS epochs)) (PP (VBN compared) (PP (TO to) (NP (DT the) (NNP SGD) (NN baseline))))))) (. .))
