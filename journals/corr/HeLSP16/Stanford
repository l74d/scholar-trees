(S (NP (PRP We)) (VP (VP (VBP propose) (NP (NP (NP (DT a) (JJ novel) (NN training) (NN algorithm)) (PP (IN for) (NP (NN reinforcement) (NN learning)))) (SBAR (WHNP (WDT which)) (S (VP (VBZ combines) (NP (NP (DT the) (NN strength)) (PP (IN of) (NP (JJ deep) (NML (NN Q) (HYPH -) (NN learning))))) (PP (IN with) (NP (DT a) (ADJP (VBN constrained)) (NN optimization) (NN approach))))))) (S (VP (TO to) (VP (VB tighten) (NP (NN optimality)))))) (CC and) (VP (VBP encourage) (ADVP (RBR faster)) (VP (VB reward) (NP (NN propagation))))) (. .))
(S (NP (PRP$ Our) (JJ novel) (NN technique)) (VP (VBZ makes) (NP (NP (JJ deep) (NN reinforcement)) (VP (VBG learning) (S (ADJP (RBR more) (JJ practical) (PP (IN by) (S (ADVP (RB drastically)) (VP (VBG reducing) (NP (DT the) (NN training) (NN time)))))))))) (. .))
(S (NP (PRP We)) (VP (VP (VBP evaluate) (NP (NP (DT the) (NN performance)) (PP (IN of) (NP (PRP$ our) (NN approach)))) (PP (IN on) (NP (NP (DT the) (CD 49) (NNS games)) (PP (IN of) (NP (NP (DT the) (JJ challenging) (NN Arcade)) (VP (VBG Learning) (NP (NNP Environment)))))))) (, ,) (CC and) (VP (VB report) (NP (JJ significant) (NNS improvements)) (PP (IN in) (NP (DT both) (NN training) (NN time) (CC and) (NN accuracy))))) (. .))
