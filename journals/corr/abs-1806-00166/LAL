(S (PP (IN In) (NP (PRP$ our) (JJ previous) (NN work))) (NP (PRP we)) (VP (VBP have) (VP (VBN shown) (SBAR (IN that) (S (NP (NP (JJ resistive) (NN cross) (NN point) (NNS devices)) (, ,) (NP (ADJP (RB so) (VBD called)) (JJ Resistive) (NNP Processing) (NNP Unit) (PRN (-LRB- -LRB-) (NNP RPU) (-RRB- -RRB-)) (NNS devices)) (, ,)) (VP (MD can) (VP (VB provide) (NP (JJ significant) (NN power) (CC and) (NN speed) (NNS benefits)) (SBAR (WHADVP (WRB when)) (S (VP (VBG training) (NP (NP (RB deep) (ADJP (RB fully) (VBN connected)) (NNS networks)) (CONJP (RB as) (RB well) (IN as)) (NP (JJ convolutional) (JJ neural) (NNS networks)))))))))))) (. .))
(S (PP (IN In) (NP (DT this) (NN work))) (, ,) (NP (PRP we)) (VP (ADVP (VBP further)) (VB extend) (NP (NP (DT the) (NNP RPU) (NN concept)) (PP (IN for) (S (VP (VBG training) (NP (NP (JJ recurrent) (JJ neural) (NNS networks)) (PRN (-LRB- -LRB-) (NP (NNP RNNs)) (-RRB- -RRB-)) (RB namely) (NP (NNP LSTMs)))))))) (. .))
(S (NP (PRP We)) (VP (VBP show) (SBAR (SBAR (IN that) (S (NP (NP (DT the) (NN mapping)) (PP (IN of) (NP (NN recurrent) (NNS layers)))) (VP (VBZ is) (ADJP (RB very) (JJ similar) (PP (TO to) (NP (NP (DT the) (NN mapping)) (PP (IN of) (NP (ADJP (RB fully) (VBN connected)) (NNS layers))))))))) (CC and) (S (ADVP (VB therefore)) (NP (DT the) (NNP RPU) (NN concept)) (VP (MD can) (ADVP (RB potentially)) (VP (VB provide) (NP (JJ large) (NN acceleration) (NNS factors)) (PP (IN for) (NP (NNP RNNs))) (ADVP (RB as) (RB well))))))) (. .))
(S (PP (IN In) (NP (NN addition))) (, ,) (NP (PRP we)) (VP (VBP study) (NP (NP (DT the) (NN effect)) (PP (IN of) (NP (JJ various) (NX (NX (NN device) (NNS imperfections)) (CC and) (NX (NN system) (NNS parameters))))) (PP (IN on) (NP (NN training) (NN performance))))) (. .))
(S (S (NP (NP (NN Symmetry)) (PP (IN of) (NP (NNS updates)))) (VP (NNS becomes) (ADJP (ADVP (RB even) (RBR more)) (JJ crucial)) (PP (IN for) (NP (NNP RNNs))))) (: ;) (S (ADVP (RB already)) (NP (DT a) (ADJP (JJ few) (NN percent)) (NN asymmetry)) (VP (NNS results) (PP (IN in) (NP (NP (DT an) (NN increase)) (PP (IN in) (NP (DT the) (NN test) (NN error))))) (PP (VBN compared) (PP (TO to) (NP (NP (DT the) (JJ ideal) (NN case)) (VP (VBN trained) (PP (IN with) (NP (VBG floating) (NN point) (NNS numbers))))))))) (. .))
(S (ADVP (RB Furthermore)) (, ,) (NP (NP (DT the) (NN input) (JJ signal) (NN resolution)) (PP (TO to) (NP (VB device) (NNS arrays)))) (VP (VBZ needs) (S (VP (TO to) (VP (VB be) (NP (QP (IN at) (JJS least) (CD 7)) (NNS bits))))) (PP (IN for) (NP (JJ successful) (NN training)))) (. .))
(S (ADVP (RB However)) (, ,) (NP (PRP we)) (VP (VBP show) (SBAR (IN that) (S (NP (DT a) (JJ stochastic) (NN rounding) (NN scheme)) (VP (MD can) (VP (VB reduce) (NP (DT the) (NN input) (JJ signal) (NN resolution)) (ADVP (RB back) (PP (TO to) (NP (CD 5) (NNS bits))))))))) (. .))
(S (ADVP (RB Further)) (, ,) (NP (PRP we)) (VP (VBP find) (SBAR (IN that) (S (NP (NP (NNP RPU) (NN device) (NNS variations)) (CC and) (NP (NN hardware) (NN noise))) (VP (VBP are) (ADJP (JJ enough) (S (VP (TO to) (VP (VB mitigate) (NP (NN overfitting)) (, ,) (SBAR (IN so) (IN that) (S (NP (EX there)) (VP (VBZ is) (NP (NP (JJR less) (NN need)) (PP (IN for) (S (VP (VBG using) (NP (NN dropout))))))))))))))))) (. .))
(S (NP (PRP We)) (VP (VBP note) (SBAR (IN that) (S (NP (NP (DT the) (NNS models)) (VP (VBN trained) (ADVP (RB here)))) (VP (VBP are) (ADJP (ADJP (QP (RB roughly) (CD 1500) (NNS times)) (JJR larger)) (PP (IN than) (NP (NP (DT the) (ADJP (RB fully) (VBN connected)) (NN network)) (VP (VBN trained) (PP (IN on) (NP (NNP MNIST) (NN dataset))))))) (PP (IN in) (NP (NP (NNS terms)) (PP (IN of) (NP (NP (DT the) (JJ total) (NN number)) (PP (IN of) (NP (NN multiplication) (CC and) (NN summation) (NNS operations))) (VP (VBD performed) (PP (IN per) (NP (NN epoch)))))))))))) (. .))
(S (ADVP (RB Thus)) (, ,) (ADVP (RB here)) (NP (PRP we)) (VP (VBP attempt) (S (VP (TO to) (VP (VB study) (NP (NP (DT the) (NN validity)) (PP (IN of) (NP (DT the) (NNP RPU) (NN approach))) (PP (IN for) (NP (JJ large) (NN scale) (NNS networks)))))))) (. .))
